{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ITQ0ls-AeXCH",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1765566460120,
     "user": {
      "displayName": "Forwood Cloud Wiser",
      "userId": "16746988716585544349"
     },
     "user_tz": 300
    },
    "id": "ITQ0ls-AeXCH"
   },
   "outputs": [],
   "source": [
    "# Inputs for module\n",
    "# Which optimization method to use (GD default)\n",
    "# Learning rate (0.015 default)\n",
    "# Adam hyperprameters (0.9 default for both)\n",
    "# Reduced mechanism txt file\n",
    "# full_eqn.txt\n",
    "# full_spc.txt\n",
    "# Rates at which coefficients shouldn't be optimized (1 default)\n",
    "# Which coefficients contribute to the score\n",
    "# Ranges for coefficients (Lower limit 0 default)\n",
    "# Input conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5tp4p7QFUreB",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1765566460346,
     "user": {
      "displayName": "Forwood Cloud Wiser",
      "userId": "16746988716585544349"
     },
     "user_tz": 300
    },
    "id": "5tp4p7QFUreB"
   },
   "outputs": [],
   "source": [
    "reduced_mech = './AMORE_v2_isoprene_42_sp.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rL3pbzlCEDGU",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1765566460647,
     "user": {
      "displayName": "Forwood Cloud Wiser",
      "userId": "16746988716585544349"
     },
     "user_tz": 300
    },
    "id": "rL3pbzlCEDGU"
   },
   "outputs": [],
   "source": [
    "# this setup allows for the user to choose to not optimize coefficients if they are isop or =1, and it also has compare species groups\n",
    "# config = {'mech_file': './reduced_mech.txt', 'full_eqn': './caltech_amore_isoprene_full_eqn.txt', 'full_spc': './caltech_amore_isoprene_full_spc.txt', 'input_conditions': 1, 'individual_params': 2, 'ignore': \"ones\"}\n",
    "config = {'mech_file': './AMORE_v2_isoprene_42_sp.txt', 'full_eqn': './caltech_amore_isoprene_full_eqn.txt', 'full_spc': './caltech_amore_isoprene_full_spc.txt', 'input_conditions': 1, 'individual_params': 2, 'ignore': \"ones\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GDiVxv48D7Tz",
   "metadata": {
    "id": "GDiVxv48D7Tz"
   },
   "source": [
    "#Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "XXOskYuZDXQJ",
   "metadata": {
    "executionInfo": {
     "elapsed": 6202,
     "status": "ok",
     "timestamp": 1765566469184,
     "user": {
      "displayName": "Forwood Cloud Wiser",
      "userId": "16746988716585544349"
     },
     "user_tz": 300
    },
    "id": "XXOskYuZDXQJ"
   },
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "#import isoprene_rates as rate\n",
    "from math import exp as EXP\n",
    "from copy import deepcopy\n",
    "import sympy as sym\n",
    "#import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "#import graphviz\n",
    "#import pygraphviz as pgv\n",
    "#import to_precision\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "#from sklearn.preprocessing import normalize\n",
    "\n",
    "if len(config)>0:\n",
    "  input_conditions = config['input_conditions']\n",
    "  mech_file = config['mech_file']\n",
    "  full_eqn = config['full_eqn']\n",
    "  full_spc = config['full_spc']\n",
    "  individual_params = config['individual_params']\n",
    "  ignore_coeffs = config['ignore']\n",
    "else:\n",
    "  print(\"Please configure your variables in variables.py\")\n",
    "\n",
    "\n",
    "\"\"\"Delete the blocks of text containing J22, J34, J41, and TROE2 in the mechanism\n",
    "\n",
    "### 1. Set input conditions\n",
    "\n",
    "\"\"\"### 2. Inital graph (Imports 1, 2, and 3 needed)\"\"\"\n",
    "\n",
    "# create a copy of the sample coefficient list\n",
    "def copy_prod(prod_coeff_list_r):\n",
    "  list_copy = [row[:] for row in prod_coeff_list_r]\n",
    "  return list_copy\n",
    "\n",
    "\n",
    "\n",
    "#KPP eqns to python (rxn,rate) format\n",
    "#as an input and outputs the reactions as a list in the format of [[reaction, rate],[r2],[r3]...]\n",
    "def read_eqns(full_eqn):\n",
    "    '''Read .eqn files\n",
    "    Parameters\n",
    "    ----------\n",
    "    eqn_file: .eqn file\n",
    "      The .eqn file to read\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    species: list\n",
    "      A list of tuples. The first element in the tuple is an equation.\n",
    "      The second element in the tuple is reaction rate.\n",
    "    '''\n",
    "\n",
    "    equations = None\n",
    "    with open(full_eqn,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        equantions = lines[:]\n",
    "    equations = [i.strip() for i in equantions[1:]]\n",
    "    equations = [tuple(i.split(':')) for i in equations if len(i)>0]\n",
    "    equations = [[i[0].strip(),i[1].split(';')[0].strip()] for i in equations if len(i)>1]\n",
    "    return(equations)\n",
    "\n",
    "#KPP spc to python\n",
    "#as an input and outputs the reactions as a list in the format of [species1,species2,...]\n",
    "def read_spc(spc_file):\n",
    "    '''Read .spc files and process the raw input into species\n",
    "    Parameters\n",
    "    ----------\n",
    "    spc_file: .eqn file\n",
    "        The .spc file to read\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    species: list\n",
    "        List of species.\n",
    "    '''\n",
    "    species = None\n",
    "    with open(spc_file,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        species = lines[:]\n",
    "    species = [s.split('=')[0].strip() for s in species]\n",
    "    species = [s for s in species if s and s[0]!='#']\n",
    "    return(species)\n",
    "\n",
    "#python functions for all of the rate constant functions given in the mechanism file.\n",
    "#These were copied from isoprene_rates.py, made by the DSI team\n",
    "def ISO1(A0, B0, C0, D0, E0, F0, G0):\n",
    "    K0 = D0 * EXP(E0/TEMP) * EXP(1.E8/TEMP**3)\n",
    "    K1 = F0 * EXP(G0/TEMP)\n",
    "    K2 = C0 * K0/(K0+K1)\n",
    "    ISO1 = A0 * EXP(B0/TEMP) * (-(K2-1))\n",
    "    return ISO1\n",
    "def EXP(x):\n",
    "    return(math.exp(x))\n",
    "def LOG10(x):\n",
    "    return(math.log10(x))\n",
    "def TUN(A0, B0, C0):\n",
    "    return(A0 * EXP(-B0/TEMP) * EXP(C0/TEMP**3))\n",
    "def ALK(A0, B0, C0, n, X0, Y0):\n",
    "    K0 = 2.0E-22 * EXP(n)\n",
    "    K1 = 4.3E-1 * (TEMP/298.0) ** (-8)\n",
    "    K0 = K0 * CFACTOR\n",
    "    K1 = K0/K1\n",
    "    K2 = (K0/(1.0 + K1)) * 4.1E-1 ** (1.0/(1.0 + (LOG10(K1)) ** 2))\n",
    "    K3 = C0/(K2 + C0)\n",
    "    K4 = A0 * (X0 - TEMP * Y0)\n",
    "    ALK = K4 * EXP(B0/TEMP) * K3\n",
    "    return(ALK)\n",
    "def NIT(A0, B0, C0, n, X0, Y0):\n",
    "    K0 = 2.0E-22 * EXP(n)\n",
    "    K1 = 4.3E-1 *(TEMP/298.0) ** (-8)\n",
    "    K0 = K0 * CFACTOR\n",
    "    K1 = K0/K1\n",
    "    K2 = (K0/(1.0 + K1)) * 4.1E-1 ** (1.0 /(1.0 +(LOG10(K1)) ** 2))\n",
    "    K3 = K2/(K2 + C0)\n",
    "    K4 = A0*(X0 - TEMP * Y0)\n",
    "    NIT = K4 * EXP(B0/TEMP) * K3\n",
    "    return(NIT)\n",
    "def ISO2(A0, B0, C0, D0, E0, F0, G0):\n",
    "    K0 = D0 * EXP(E0/TEMP) * EXP(1.E8/TEMP**3)\n",
    "    K1 = F0 * EXP(G0/TEMP)\n",
    "    K2 = C0 * K0/(K0+K1)\n",
    "    ISO2 = A0 * EXP(B0/TEMP) * K2\n",
    "    return ISO2\n",
    "def EPO(A1, E1, M1):\n",
    "    K1 = 1 / (M1 * CFACTOR + 1)\n",
    "    EPO = A1 * EXP(E1/TEMP) * K1\n",
    "    return EPO\n",
    "def KCO(A1, M1):\n",
    "    KCO = A1 * (1 + (CFACTOR/M1))\n",
    "    return KCO\n",
    "def FALL(A0,B0,C0,A1,B1,C1,CF):\n",
    "    K0 = A0 * EXP(B0/TEMP) * (TEMP/300)**C0\n",
    "    K1 = A1 * EXP(B1/TEMP) * (TEMP/300)**(C1)\n",
    "    K0 = K0*CFACTOR\n",
    "    K1 = K0/K1\n",
    "    FALL = (K0 / (1.00+K1) * CF**(1 / (1 + LOG10(K1)) **2))\n",
    "    return FALL\n",
    "def TROE(A0, B0, C0, A1, B1, C1, CF):\n",
    "    K0 = A0 * EXP(B0/TEMP) * (TEMP/300) ** C0\n",
    "    K1 = A1 * EXP(B1/TEMP) * (TEMP/300) ** C1\n",
    "    K0 = K0 * CFACTOR\n",
    "    KR = K0/K1\n",
    "    NC = 0.75 - 1.27 * LOG10(CF)\n",
    "    F = 10 ** ((LOG10(CF)) / (1+(LOG10(KR)/NC)**2))\n",
    "    TROE = K0*K1*F / (K0+K1)\n",
    "    return TROE\n",
    "def ARR(A0, B0, C0):\n",
    "    return(A0 * EXP(B0/TEMP) * EXP(C0/TEMP**3))\n",
    "def K_OH_CO(T,M):\n",
    "    T3I = 1/T\n",
    "    KLO1=5.9e-33*(300*T3I)**(1.4)\n",
    "    KHI1=1.1E-12*(300.*T3I)**(-1.3)\n",
    "    XYRAT1=(KLO1*M)/KHI1\n",
    "    BLOG1= np.log10(XYRAT1)\n",
    "    FEXP1=1.0/(1.0+BLOG1*BLOG1)\n",
    "    KCO1=KLO1*M*0.6**(FEXP1)/(1.0+XYRAT1)\n",
    "    KLO2=1.5E-13*(300*T3I)**(-0.6)\n",
    "    KHI2=2.1E9*(300*T3I)**(-6.1)\n",
    "    XYRAT2 = KLO2*M/KHI2\n",
    "    BLOG2=LOG10(XYRAT2)\n",
    "    FEXP2=1.0/(1.0+ BLOG2*BLOG2)\n",
    "    KCO2=KLO2*0.6**(FEXP2/(1.0+XYRAT2))\n",
    "    KCO=KCO1+KCO2\n",
    "    return KCO\n",
    "def KRO2NO3():\n",
    "    return 2.3e-12\n",
    "def KAPHO2(T):\n",
    "    return 5.2e-13*np.exp(980/T)\n",
    "def KAPNO(T):\n",
    "    return 7.5e-12*np.exp(290/T)\n",
    "def KNO3AL(T):\n",
    "    return 1.44e-12*np.exp(-1862/T) ;\n",
    "def KCH3O2(T):\n",
    "    return 1.03e-13*np.exp(365/T)\n",
    "def KBPAN(T,M):\n",
    "    KD0 = 1.10e-05*M*np.exp(-10100/T)\n",
    "    KDI = 1.90e17*np.exp(-14100/T)\n",
    "    KRD = KD0/KDI\n",
    "    FCD = 0.30\n",
    "    NCD = 0.75-1.27*(np.log10(FCD)) ;\n",
    "    FD = 10**(np.log10(FCD)/(1+(np.log10(KRD)/NCD)**2)) ;\n",
    "    return (KD0*KDI)*FD/(KD0+KDI) ;\n",
    "def KFPAN(T,M):\n",
    "    KC0 = 3.28e-28*M*(T/300)**(-6.87)\n",
    "    KCI = 1.125e-11*(T/300)**(-1.105)\n",
    "    KRC = KC0/KCI\n",
    "    FCC = 0.30\n",
    "    NC = 0.75-1.27*(np.log10(FCC))\n",
    "    FC = 10**(np.log10(FCC)/(1+(np.log10(KRC)/NC)**2)) ;\n",
    "    return (KC0*KCI)*FC/(KC0+KCI) ;\n",
    "\n",
    "def cycle_simulator_3_out(cycle_species, in_spec, graph, out_graph, in_graph, iteration_set, cutoff, all_spec, out_spec):\n",
    "    data_set = []\n",
    "    search = {in_spec:1}\n",
    "    counter = 0\n",
    "    data = {j:0 for j in all_spec}\n",
    "    out_dat = {j:0 for j in cycle_species}\n",
    "    while counter<iteration_set[1]:\n",
    "        new_search = {}\n",
    "        if counter == iteration_set[0]:\n",
    "            data_1 = {p:data[p] for p in data}\n",
    "            search_1 = {p:search[p] for p in search}\n",
    "        for k in search:\n",
    "            #data[k] = data[k]-search[k]\n",
    "            return_amount = 0\n",
    "            for x in graph[k]:\n",
    "                data[x] = data[x] + graph[k][x]*search[k]\n",
    "\n",
    "                if x in cycle_species:\n",
    "                    return_amount = return_amount + search[k]*graph[k][x]\n",
    "                    if graph[k][x]*search[k]>cutoff:\n",
    "                        if x in new_search:\n",
    "                            new_search[x] = new_search[x] + graph[k][x]*search[k]\n",
    "                        else:\n",
    "                            new_search[x] = graph[k][x]*search[k]\n",
    "            out_dat[k] = out_dat[k] + search[k]-return_amount\n",
    "        search = {p:new_search[p] for p in new_search}\n",
    "        counter = counter + 1\n",
    "    frac1 = 0\n",
    "    frac2 = 0\n",
    "    for n in cycle_species:\n",
    "        if n in search_1:\n",
    "            frac1 = frac1 + search_1[n]\n",
    "        if n in search:\n",
    "            frac2 = frac2 + search[n]\n",
    "    for i in out_dat:\n",
    "        out_dat[i] = max(0,out_dat[i]/max(1-frac2,1e-20))\n",
    "    final_data = {}\n",
    "    within_data = {}\n",
    "    denom = frac2-frac1\n",
    "    if denom ==0:\n",
    "        denom = 1e-20\n",
    "    for n in out_spec:\n",
    "        slope = (data[n]-data_1[n])/denom\n",
    "        dat = data[n] - slope*frac2\n",
    "        final_data[n] = dat\n",
    "    for n in cycle_species:\n",
    "        slope = (data[n]-data_1[n])/denom\n",
    "        dat = data[n] - slope*frac2\n",
    "        within_data[n] = dat\n",
    "    within_data_2 = {}\n",
    "    within_data_sum = 0\n",
    "    for i in within_data:\n",
    "        within_data_sum = within_data_sum + within_data[i]\n",
    "    for i in within_data:\n",
    "        within_data_2[i] = abs(within_data[i]/max(within_data_sum,1e-20))\n",
    "\n",
    "    final_in_data = {}\n",
    "    for i in within_data_2:\n",
    "        final_in_data[i] = np.mean([within_data_2[i],out_dat[i]])\n",
    "    return final_data, final_in_data, out_dat #, data\n",
    "\n",
    "def cycle_simulator_2(cycle_species, in_spec, graph, out_graph, in_graph, iteration_set, cutoff):\n",
    "    all_spec = {i for i in cycle_species}\n",
    "    out_spec = set()\n",
    "    for i in cycle_species:\n",
    "        for j in out_graph[i]:\n",
    "            if j not in cycle_species:\n",
    "                out_spec.add(j)\n",
    "                all_spec.add(j)\n",
    "    data_set = []\n",
    "    search = {in_spec:1}\n",
    "    counter = 0\n",
    "    data = {j:0 for j in out_spec}\n",
    "    while counter<iteration_set[1]:\n",
    "        new_search = {}\n",
    "        if counter == iteration_set[0]:\n",
    "            data_1 = {p:data[p] for p in data}\n",
    "            search_1 = {p:search[p] for p in search}\n",
    "        for k in search:\n",
    "            for x in graph[k]:\n",
    "                if x in out_spec:\n",
    "                    data[x] = data[x] + graph[k][x]*search[k]\n",
    "                if x in cycle_species:\n",
    "                    if graph[k][x]*search[k]>cutoff:\n",
    "                        if x in new_search:\n",
    "                            new_search[x] = new_search[x] + graph[k][x]*search[k]\n",
    "                        else:\n",
    "                            new_search[x] = graph[k][x]*search[k]\n",
    "        search = {p:new_search[p] for p in new_search}\n",
    "        counter = counter + 1\n",
    "    frac1 = 0\n",
    "    frac2 = 0\n",
    "    for n in cycle_species:\n",
    "        if n in search_1:\n",
    "            frac1 = frac1 + search_1[n]\n",
    "        if n in search:\n",
    "            frac2 = frac2 + search[n]\n",
    "\n",
    "    final_data = {}\n",
    "    denom = frac2-frac1\n",
    "    if denom ==0:\n",
    "        denom = 1e-20\n",
    "    for n in out_spec:\n",
    "        slope = (data[n]-data_1[n])/denom\n",
    "        dat = data[n] - slope*frac2\n",
    "        final_data[n] = dat\n",
    "    return final_data#, data\n",
    "\n",
    "def solve_j_rate(i, sza,SUN):\n",
    "    if i == 'J(22)' or i== 'J22':\n",
    "        I = 5.804e-6;\n",
    "        m = 1.092;\n",
    "        n = 0.377;\n",
    "        k = j_func(sza,I,m,n)*SUN\n",
    "    elif i == 'J(34)' or i== 'J34':\n",
    "        I = 1.537e-4;\n",
    "        m = 0.170;\n",
    "        n = 0.208;\n",
    "        k = j_func(sza,I,m,n)*SUN\n",
    "    elif i == 'J(41)' or i== 'J41':\n",
    "        I = 7.649e-6;\n",
    "        m = 0.682;\n",
    "        n = 0.279;\n",
    "        k = j_func(sza,I,m,n)*SUN\n",
    "    elif i == 'J(31)' or i== 'J31':\n",
    "        I = 6.845e-5\n",
    "        m = 0.130\n",
    "        n = 0.201\n",
    "        k = j_func(sza,I,m,n)*SUN\n",
    "    elif i == 'J(32)' or i== 'J32':\n",
    "        I = 1.032e-5\n",
    "        m = 0.130\n",
    "        n = 0.201\n",
    "        k = j_func(sza,I,m,n)*SUN\n",
    "    elif i == 'J(33)' or i== 'J33':\n",
    "        I = 3.802e-5\n",
    "        m = 0.644\n",
    "        n = 0.312\n",
    "        k = j_func(sza,I,m,n)*SUN\n",
    "    elif i == 'J(11)' or i== 'J11':\n",
    "        I = 4.642e-5\n",
    "        m = 0.762\n",
    "        n = 0.353\n",
    "        k = j_func(sza,I,m,n)*SUN\n",
    "    elif i == 'J(12)' or i== 'J12':\n",
    "        I = 6.853e-5\n",
    "        m = 0.477\n",
    "        n = 0.323\n",
    "        k = j_func(sza,I,m,n)*SUN\n",
    "    elif i == 'J(15)' or i== 'J15':\n",
    "        I = 2.792e-5\n",
    "        m = 0.805\n",
    "        n = 0.338\n",
    "        k = j_func(sza,I,m,n)*SUN\n",
    "    elif i == 'J(51)' or i== 'J51':\n",
    "        I = 1.588e-6;\n",
    "        m = 1.154;\n",
    "        n = 0.318;\n",
    "        k = j_func(sza,I,m,n)*SUN\n",
    "    return k\n",
    "\"\"\"PROD 3: Find yields of species with new reaction dictionary based on test prod coefficients\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f0am_to_python(mech):\n",
    "  species_list_0, reaction_list_0 = mech.split('AddSpecies')\n",
    "  species_list_1 = species_list_0.split(';')\n",
    "  species_list_2 = []\n",
    "  for i in species_list_1[:-2]:\n",
    "      species_list_2.append(i.split(\"'\")[1])\n",
    "  species_list_2 = deepcopy(species_list_2)\n",
    "  reaction_list_1 = reaction_list_0.split('i=i+1')[1:]\n",
    "  rnames_0 = []\n",
    "  rates_0 = []\n",
    "  specs_0 = []\n",
    "  for i in reaction_list_1:\n",
    "      pt1,pt2 = i.split('k(:,i)')\n",
    "      rnames_0.append(pt1)\n",
    "      r1, sp = pt2.split('Gstr',1)\n",
    "      rates_0.append(r1)\n",
    "      specs_0.append(sp)\n",
    "  rnames = []\n",
    "  for i in rnames_0:\n",
    "      rnames.append(i.split(\"'\")[1])\n",
    "  rates_1 = []\n",
    "  for i in rates_0:\n",
    "      rates_1.append(i.split(' ')[2])\n",
    "  rates_2 = []\n",
    "  for i in rates_1:\n",
    "      rates_2.append(i.split(';')[0])\n",
    "\n",
    "  T = 298\n",
    "  JAFGS = 1\n",
    "  rates_3 = []\n",
    "  for i in rates_2:\n",
    "      r1 = i.replace('./','/')\n",
    "      r2 = r1.replace('.*','*')\n",
    "      r3 = r2.replace('.^','**')\n",
    "      r4 = r3.replace('exp','math.exp')\n",
    "      r5 = r4.replace('KAPHO2', 'KAPHO2(TEMP)')\n",
    "      r6 = r5.replace('KAPNO', 'KAPNO(TEMP)')\n",
    "      r7 = r6.replace('KFPAN', 'KFPAN(TEMP,M)')\n",
    "      r8 = r7.replace('KRO2NO3', 'KRO2NO3()')\n",
    "      r9 = r8.replace('KNO3AL', 'KNO3AL(TEMP)')\n",
    "      r10 = r9.replace('KBPAN', 'KBPAN(TEMP,M)')\n",
    "      r11 = r10.replace('/T)', '/TEMP)')\n",
    "      r12 = r11.replace('F0AM_isop_NIT', 'NIT')\n",
    "      r13 = r12.replace('F0AM_isop_ALK(T,M,', 'ALK(')\n",
    "      #r14 = r13.replace('T,M,', '')\n",
    "      r15 = r13.replace('F0AM_isop_TUN(T,M,', 'TUN(')\n",
    "      r16 = r15.replace('F0AM_isop_EPO(T,M,', 'EPO(')\n",
    "      r16 = r16.replace('F0AM_isop_TROE2(T,M,','TROE(')\n",
    "      r16 = r16.replace('F0AM_isop_ISO1(T,','ISO1(')\n",
    "      r16 = r16.replace('F0AM_isop_ISO2(T,','ISO2(')\n",
    "      r16 = r16.replace('F0AM_isop_KCO(T,M,','KCO(')\n",
    "      r16 = r16.replace('F0AM_isop_FALL(T,M,','FALL(')\n",
    "      r16 = r16.replace('F0AM_isop_ALK(T,', 'ALK(')\n",
    "      rates_3.append(r16)\n",
    "  rates = deepcopy(rates_3)\n",
    "  rates_eval = []\n",
    "  for i in rates_3:\n",
    "      if 'J' in i:\n",
    "          rates_eval.append(1e-5)\n",
    "      else:\n",
    "          rates_eval.append(eval(i))\n",
    "  specs_1 = []\n",
    "  losses = []\n",
    "  reac_list = []\n",
    "  prod_list = []\n",
    "  prod_coeff_list = []\n",
    "  reac_coeff_list = []\n",
    "  for i in specs_0:\n",
    "      x = i.split('(i)=')[1:]\n",
    "      s1 = []\n",
    "      for k in x:\n",
    "          s1.append(k.split(';')[0])\n",
    "      pr = []\n",
    "      re = []\n",
    "      pr_co = []\n",
    "      re_co = []\n",
    "      l1 = 'none'\n",
    "      l2 = 0\n",
    "      for j in s1:\n",
    "          pt1, pt2 = j.split(')')\n",
    "          r1 = pt1.split('f')[1]\n",
    "          r2 = r1.split('(')[0]\n",
    "          if pt2[0] == '+':\n",
    "              pr.append(r2)\n",
    "              pr_co.append(float(pt2[1:]))\n",
    "          elif r2!= 'XC' and r2!='XN':\n",
    "              re.append(r2)\n",
    "              re_co.append(float(pt2[1:]))\n",
    "          else:\n",
    "              l1 = r2\n",
    "              l2 = float(pt2[1:])\n",
    "      losses.append([l1,l2])\n",
    "      prod_list.append(pr)\n",
    "      reac_list.append(re)\n",
    "      prod_coeff_list.append(pr_co)\n",
    "      reac_coeff_list.append(re_co)\n",
    "  mech_0 = [species_list_2,reac_list,reac_coeff_list,prod_list,prod_coeff_list,rates]\n",
    "  return species_list_2,reac_list,reac_coeff_list,prod_list,prod_coeff_list,rates, rates_eval, rates_2\n",
    "\n",
    "def get_yields_from_inputs_full(inputs):\n",
    "    t1 = time.time()\n",
    "    # create numerical species list\n",
    "    species_list = list(range(len(inputs['spc'])))\n",
    "    spec_len = len(species_list)\n",
    "    dic = {}\n",
    "    dic = {inputs['spc'][i]:i for i in range(spec_len)}\n",
    "    reac_len = len(inputs['rxn']['reac'])\n",
    "    #numerical background species list\n",
    "    background_spc = []\n",
    "    background_spc_n = []\n",
    "    for i in inputs['bck']:\n",
    "        if i in inputs['spc']:\n",
    "            background_spc.append(inputs['spc'].index(i))\n",
    "            background_spc_n.append(i)\n",
    "    back_set = set(background_spc)\n",
    "    # root node\n",
    "    root = inputs['spc'].index(inputs['settings']['root'])\n",
    "    t2 = time.time()\n",
    "    #print('Species list created. Root node = ' + str(root) + '.' + ' Step time = '+ str(t2-t1) + '.')\n",
    "    #change species to numerical values\n",
    "    reac_list, prod_list, reac_no_back, prod_no_back, rxns_reac, rxns_prod = rxn_index_convert(inputs['rxn']['reac'],inputs['rxn']['prod'],dic, background_spc, background_spc_n)\n",
    "    rxns_reac = [set(i) for i in rxns_reac]\n",
    "    prod_coeff_list = inputs['rxn']['prod_coeff'] # this is our prod coeff list\n",
    "    reac_coeff_list = inputs['rxn']['reac_coeff']\n",
    "    for i in background_spc:\n",
    "        rxns_reac[i] = set()\n",
    "    t3 = time.time()\n",
    "    #get rates of rxn\n",
    "    p_fac = pressure_to_m(inputs['atm cond']['pres'],inputs['atm cond']['temp'])/1000000000\n",
    "    rate_list = []\n",
    "    TEMP = inputs['atm cond']['temp']\n",
    "    M = p_fac\n",
    "    k_list = get_k_list(inputs['rxn']['k'],inputs['atm cond']['sza'], TEMP)\n",
    "    for i in range(len(reac_list)):\n",
    "        if len(reac_list[i])==1:\n",
    "            rate_list.append(k_list[i])\n",
    "        else:\n",
    "            mark = True\n",
    "            for j in reac_list[i]:\n",
    "                if j in background_spc:\n",
    "                    mark = False\n",
    "                    conc = inputs['bck'][inputs['spc'][j]]\n",
    "            if mark:\n",
    "                conc = inputs['scnd'][inputs['rxn']['reac'][i][1]]\n",
    "            rate_list.append(k_list[i]*conc*p_fac)\n",
    "    t4 = time.time()\n",
    "    #print('Reaction rates calculated.' + ' Step time = '+ str(t4-t3) + '.')\n",
    "    # remove less important reactions of a species\n",
    "    #create prod_dict\n",
    "    rxn_dict = [{} for i in range(reac_len)]\n",
    "    prod_dict = []\n",
    "    for i in range(reac_len):\n",
    "        prods = {}\n",
    "        for j in reac_list[i]:\n",
    "            if j in back_set:\n",
    "                prods[j] = -1\n",
    "        for k in range(len(prod_list[i])):\n",
    "            spec = prod_list[i][k]\n",
    "            if spec in prods:\n",
    "                prods[spec] = prods[spec] + prod_coeff_list[i][k]\n",
    "            else:\n",
    "                prods[spec] = prod_coeff_list[i][k]\n",
    "        prod_dict.append(prods)\n",
    "        rxn_dict[i]['prod'] = prods\n",
    "        reacys = set()\n",
    "        for j in reac_list[i]:\n",
    "            reacys.add(j)\n",
    "        rxn_dict[i]['reac'] = reacys\n",
    "        rxn_dict[i]['k'] = k_list[i]\n",
    "        rxn_dict[i]['r'] = inputs['rxn']['rates'][i]\n",
    "    #create graph\n",
    "    graph = []\n",
    "    in_graph = [set() for i in range(spec_len)]\n",
    "    out_graph = [set() for i in range(spec_len)]\n",
    "    out_graph_type = [{} for i in range(spec_len)]\n",
    "    in_graph_type  = [{} for i in range(spec_len)]\n",
    "    for i in range(spec_len):\n",
    "        if i not in back_set:\n",
    "            edges = {}\n",
    "            rate_sum = 0\n",
    "            for j in rxns_reac[i]:\n",
    "                rate_sum = rate_sum + rate_list[j]\n",
    "            for j in rxns_reac[i]:\n",
    "                mult = rate_list[j]/max(1e-20,rate_sum)\n",
    "                if len(reac_list[j]) == 1:\n",
    "                    type_r = 'solo'\n",
    "                else:\n",
    "                    mark = True\n",
    "                    for p in reac_list[j]:\n",
    "                        if p in background_spc:\n",
    "                            type_r = p\n",
    "                            mark = False\n",
    "                    if mark:\n",
    "                        type_r = 'double'\n",
    "                for k in prod_dict[j]:\n",
    "                    if i in in_graph_type[k]:\n",
    "                        in_graph_type[k][i].add(type_r)\n",
    "                    else:\n",
    "                        in_graph_type[k][i] = set([type_r])\n",
    "                    if k in edges:\n",
    "                        edges[k] = edges[k] + prod_dict[j][k]*mult\n",
    "                        out_graph[i].add(k)\n",
    "                        in_graph[k].add(i)\n",
    "                        out_graph_type[i][k].add(type_r)\n",
    "\n",
    "                    else:\n",
    "                        edges[k] = prod_dict[j][k]*mult\n",
    "                        out_graph[i].add(k)\n",
    "                        in_graph[k].add(i)\n",
    "                        out_graph_type[i][k] = set([type_r])\n",
    "            graph.append(edges)\n",
    "        else:\n",
    "            graph.append({})\n",
    "    # strongly connected component identification, code source: http://www.logarithmic.net/pfh/blog/01208083168\n",
    "    test_graph = {}\n",
    "    for i in range(len(out_graph)):\n",
    "        test_graph[i] = list(out_graph[i])\n",
    "    scc = strongly_connected_components(test_graph)\n",
    "    scc_2 = []\n",
    "    for i in scc:\n",
    "        if len(i) >1:\n",
    "            scc_2.append(i)\n",
    "    scc_lens = [len(i) for i in scc_2]\n",
    "    t7 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    remove_specs = set()\n",
    "    protected = [inputs['spc'].index(p) for p in inputs['settings']['protected']]\n",
    "    for i in range(spec_len):\n",
    "        if i not in protected:\n",
    "            if len(in_graph[i])==0:\n",
    "                remove_specs.add(i)\n",
    "\n",
    "    old_list = set()\n",
    "    while len(remove_specs)>0:\n",
    "        new_specs = set()\n",
    "        for i in remove_specs:\n",
    "            old_list.add(i)\n",
    "            for j in out_graph[i]:\n",
    "                in_graph[j].discard(i)\n",
    "        for i in range(spec_len):\n",
    "            if i not in protected:\n",
    "                if i not in old_list:\n",
    "                    if len(in_graph[i])==0:\n",
    "                        new_specs.add(i)\n",
    "        remove_specs = {i for i in new_specs}\n",
    "    old_list.discard(root)\n",
    "    graph_2 = []\n",
    "    for i in graph:\n",
    "        new_edge = {}\n",
    "        for j in i:\n",
    "            if j not in old_list:\n",
    "                new_edge[j] = i[j]\n",
    "        graph_2.append(new_edge)\n",
    "\n",
    "\n",
    "    new_graph = []\n",
    "    for i in graph_2:\n",
    "        dicy = {}\n",
    "        for j in i:\n",
    "            dicy[j] = i[j]\n",
    "        new_graph.append(dicy)\n",
    "\n",
    "    new_in = []\n",
    "    for i in in_graph:\n",
    "        sety = set()\n",
    "        for j in i:\n",
    "            sety.add(j)\n",
    "        new_in.append(sety)\n",
    "\n",
    "    new_out = []\n",
    "    for i in out_graph:\n",
    "        sety = set()\n",
    "        for j in i:\n",
    "            sety.add(j)\n",
    "        new_out.append(sety)\n",
    "\n",
    "    scc_set = set()\n",
    "    for i in scc_2:\n",
    "        for j in i:\n",
    "            scc_set.add(j)\n",
    "    scc_dict = {}\n",
    "    for j in range(len(scc_2)):\n",
    "        for k in scc_2[j]:\n",
    "            scc_dict.update({k:j})\n",
    "    #new_graph = [i for i in graph_2]\n",
    "    #new_in = [i for i in in_graph]\n",
    "    #new_out = [i for i in out_graph]\n",
    "    count = 0\n",
    "    for i in scc_2:\n",
    "        count = count + 1\n",
    "        #if count%100 == 0:\n",
    "        #   print(count)\n",
    "        leny = int(np.sqrt(len(i)))\n",
    "        in_cycle_spec = set()\n",
    "        out_spec = set()\n",
    "        for p in i:\n",
    "            for k in in_graph[p]:\n",
    "                if k not in i:\n",
    "                    in_cycle_spec.add(p)\n",
    "            for k in out_graph[p]:\n",
    "                if k not in i:\n",
    "                    out_spec.add(k)\n",
    "\n",
    "        count_3 = 0\n",
    "\n",
    "        for x in in_cycle_spec:\n",
    "            new_out[x] = out_spec\n",
    "\n",
    "            data = cycle_simulator_2(i, x, graph_2,out_graph,in_graph, [leny+20,2*(leny+40)],1e-6) # faster parameters\n",
    "                                       #(, graph, out_graph, in_graph, iteration_set, cutoff, all_spec, out_spec)\n",
    "\n",
    "            # data = cycle_simulator_2(i, x, graph_2,out_graph,in_graph, [leny+200,2*(leny+400)],0) # starter slower parameters\n",
    "            new_graph[x] = data\n",
    "            #for p in out_data:\n",
    "\n",
    "            #    if p in new_graph[x]:\n",
    "\n",
    "            #        new_graph[x][p] += out_data[p]\n",
    "            #    else:\n",
    "            #        new_graph[x][p] = out_data[p]\n",
    "        for y in i:\n",
    "            if y not in in_cycle_spec:\n",
    "                for s in out_graph[y]:\n",
    "                    if y in new_in[s]:\n",
    "                        new_in[s].remove(y)\n",
    "                new_in[y] = set()\n",
    "                new_out[y] = set()\n",
    "                new_graph[y] = set()\n",
    "    t9 = time.time()\n",
    "    yi = get_yields(root, 1e-14, new_graph, new_in, new_out, back_set, spec_len) # getting yields (the test prod coefficients will be in the graph)\n",
    "    yi2 = {inputs['spc'][i]:yi[i] for i in range(spec_len)} # putting yields in dictionary\n",
    "    # print('Yields Calculated')\n",
    "    return yi2, new_graph, new_in, scc_2 #, out_graph_type, in_graph_type, r_g_clean\n",
    "    # new_graph[i] (same indexing as species list) is a dictionary {j:0.8, k:0.5, l:0.1} where j, k, and l are species that i goes to\n",
    "    # new_in[j] is a set with species {i}, lists incoming species\n",
    "    # compare species will be formatted as a list of species\n",
    "      # most species in reduced mechanism compare 1 to 1 with reference, but some compare to a group of species\n",
    "    # check if species are in full species list and if not then take out of category\n",
    "\n",
    "\n",
    "\n",
    "def get_yields_from_inputs(inputs):\n",
    "    t1 = time.time()\n",
    "    # create numerical species list\n",
    "    species_list = list(range(len(inputs['spc'])))\n",
    "    spec_len = len(species_list)\n",
    "    dic = {}\n",
    "    dic = {inputs['spc'][i]:i for i in range(spec_len)}\n",
    "    reac_len = len(inputs['rxn']['reac'])\n",
    "    #numerical background species list\n",
    "    background_spc = []\n",
    "    background_spc_n = []\n",
    "    for i in inputs['bck']:\n",
    "        if i in inputs['spc']:\n",
    "            background_spc.append(inputs['spc'].index(i))\n",
    "            background_spc_n.append(i)\n",
    "    back_set = set(background_spc)\n",
    "    # root node\n",
    "    root = inputs['spc'].index(inputs['settings']['root'])\n",
    "    t2 = time.time()\n",
    "    #print('Species list created. Root node = ' + str(root) + '.' + ' Step time = '+ str(t2-t1) + '.')\n",
    "    #change species to numerical values\n",
    "    reac_list, prod_list, reac_no_back, prod_no_back, rxns_reac, rxns_prod = rxn_index_convert(inputs['rxn']['reac'],inputs['rxn']['prod'],dic, background_spc, background_spc_n)\n",
    "    rxns_reac = [set(i) for i in rxns_reac]\n",
    "    prod_coeff_list = inputs['rxn']['prod_coeff'] # this is our prod coeff list\n",
    "    reac_coeff_list = inputs['rxn']['reac_coeff']\n",
    "    for i in background_spc:\n",
    "        rxns_reac[i] = set()\n",
    "    t3 = time.time()\n",
    "    #get rates of rxn\n",
    "    p_fac = pressure_to_m(inputs['atm cond']['pres'],inputs['atm cond']['temp'])/1000000000\n",
    "    rate_list = []\n",
    "    TEMP = inputs['atm cond']['temp']\n",
    "    M = p_fac\n",
    "    k_list = get_k_list(inputs['rxn']['k'],inputs['atm cond']['sza'], TEMP)\n",
    "    for i in range(len(reac_list)):\n",
    "        if len(reac_list[i])==1:\n",
    "            rate_list.append(k_list[i])\n",
    "        else:\n",
    "            mark = True\n",
    "            for j in reac_list[i]:\n",
    "                if j in background_spc:\n",
    "                    mark = False\n",
    "                    conc = inputs['bck'][inputs['spc'][j]]\n",
    "            if mark:\n",
    "                conc = inputs['scnd'][inputs['rxn']['reac'][i][1]]\n",
    "            rate_list.append(k_list[i]*conc*p_fac)\n",
    "    t4 = time.time()\n",
    "    #print('Reaction rates calculated.' + ' Step time = '+ str(t4-t3) + '.')\n",
    "    # remove less important reactions of a species\n",
    "    #create prod_dict\n",
    "    rxn_dict = [{} for i in range(reac_len)]\n",
    "    prod_dict = []\n",
    "    for i in range(reac_len):\n",
    "        prods = {}\n",
    "        for j in reac_list[i]:\n",
    "            if j in back_set:\n",
    "                prods[j] = -1\n",
    "        for k in range(len(prod_list[i])):\n",
    "            spec = prod_list[i][k]\n",
    "            if spec in prods:\n",
    "                prods[spec] = prods[spec] + prod_coeff_list[i][k]\n",
    "            else:\n",
    "                prods[spec] = prod_coeff_list[i][k]\n",
    "        prod_dict.append(prods)\n",
    "        rxn_dict[i]['prod'] = prods\n",
    "        reacys = set()\n",
    "        for j in reac_list[i]:\n",
    "            reacys.add(j)\n",
    "        rxn_dict[i]['reac'] = reacys\n",
    "        rxn_dict[i]['k'] = k_list[i]\n",
    "        rxn_dict[i]['r'] = inputs['rxn']['rates'][i]\n",
    "    #create graph\n",
    "    graph = []\n",
    "    in_graph = [set() for i in range(spec_len)]\n",
    "    out_graph = [set() for i in range(spec_len)]\n",
    "    out_graph_type = [{} for i in range(spec_len)]\n",
    "    in_graph_type  = [{} for i in range(spec_len)]\n",
    "    for i in range(spec_len):\n",
    "        if i not in back_set:\n",
    "            edges = {}\n",
    "            rate_sum = 0\n",
    "            for j in rxns_reac[i]:\n",
    "                rate_sum = rate_sum + rate_list[j]\n",
    "            for j in rxns_reac[i]:\n",
    "                mult = rate_list[j]/max(1e-20,rate_sum)\n",
    "                if len(reac_list[j]) == 1:\n",
    "                    type_r = 'solo'\n",
    "                else:\n",
    "                    mark = True\n",
    "                    for p in reac_list[j]:\n",
    "                        if p in background_spc:\n",
    "                            type_r = p\n",
    "                            mark = False\n",
    "                    if mark:\n",
    "                        type_r = 'double'\n",
    "                for k in prod_dict[j]:\n",
    "                    if i in in_graph_type[k]:\n",
    "                        in_graph_type[k][i].add(type_r)\n",
    "                    else:\n",
    "                        in_graph_type[k][i] = set([type_r])\n",
    "                    if k in edges:\n",
    "                        edges[k] = edges[k] + prod_dict[j][k]*mult\n",
    "                        out_graph[i].add(k)\n",
    "                        in_graph[k].add(i)\n",
    "                        out_graph_type[i][k].add(type_r)\n",
    "\n",
    "                    else:\n",
    "                        edges[k] = prod_dict[j][k]*mult\n",
    "                        out_graph[i].add(k)\n",
    "                        in_graph[k].add(i)\n",
    "                        out_graph_type[i][k] = set([type_r])\n",
    "            graph.append(edges)\n",
    "        else:\n",
    "            graph.append({})\n",
    "    # strongly connected component identification, code source: http://www.logarithmic.net/pfh/blog/01208083168\n",
    "    scc_2 = inputs['scc']\n",
    "    scc_lens = [len(i) for i in scc_2]\n",
    "    t7 = time.time()\n",
    "\n",
    "    remove_specs = set()\n",
    "    protected = [inputs['spc'].index(p) for p in inputs['settings']['protected']]\n",
    "    for i in range(spec_len):\n",
    "        if i not in protected:\n",
    "            if len(in_graph[i])==0:\n",
    "                remove_specs.add(i)\n",
    "\n",
    "    old_list = set()\n",
    "    while len(remove_specs)>0:\n",
    "        new_specs = set()\n",
    "        for i in remove_specs:\n",
    "            old_list.add(i)\n",
    "            for j in out_graph[i]:\n",
    "                in_graph[j].discard(i)\n",
    "        for i in range(spec_len):\n",
    "            if i not in protected:\n",
    "                if i not in old_list:\n",
    "                    if len(in_graph[i])==0:\n",
    "                        new_specs.add(i)\n",
    "        remove_specs = {i for i in new_specs}\n",
    "    old_list.discard(root)\n",
    "    graph_2 = []\n",
    "    for i in graph:\n",
    "        new_edge = {}\n",
    "        for j in i:\n",
    "            if j not in old_list:\n",
    "                new_edge[j] = i[j]\n",
    "        graph_2.append(new_edge)\n",
    "\n",
    "\n",
    "    new_graph = []\n",
    "    for i in graph_2:\n",
    "        dicy = {}\n",
    "        for j in i:\n",
    "            dicy[j] = i[j]\n",
    "        new_graph.append(dicy)\n",
    "\n",
    "    new_in = []\n",
    "    for i in in_graph:\n",
    "        sety = set()\n",
    "        for j in i:\n",
    "            sety.add(j)\n",
    "        new_in.append(sety)\n",
    "\n",
    "    new_out = []\n",
    "    for i in out_graph:\n",
    "        sety = set()\n",
    "        for j in i:\n",
    "            sety.add(j)\n",
    "        new_out.append(sety)\n",
    "\n",
    "    scc_set = set()\n",
    "    for i in scc_2:\n",
    "        for j in i:\n",
    "            scc_set.add(j)\n",
    "    scc_dict = {}\n",
    "    for j in range(len(scc_2)):\n",
    "        for k in scc_2[j]:\n",
    "            scc_dict.update({k:j})\n",
    "    #new_graph = [i for i in graph_2]\n",
    "    #new_in = [i for i in in_graph]\n",
    "    #new_out = [i for i in out_graph]\n",
    "    count = 0\n",
    "    for i in scc_2:\n",
    "        count = count + 1\n",
    "        #if count%100 == 0:\n",
    "        #   print(count)\n",
    "        leny = int(np.sqrt(len(i)))\n",
    "        in_cycle_spec = set()\n",
    "        out_spec = set()\n",
    "        for p in i:\n",
    "            for k in in_graph[p]:\n",
    "                if k not in i:\n",
    "                    in_cycle_spec.add(p)\n",
    "            for k in out_graph[p]:\n",
    "                if k not in i:\n",
    "                    out_spec.add(k)\n",
    "\n",
    "        count_3 = 0\n",
    "\n",
    "        for x in in_cycle_spec:\n",
    "            new_out[x] = out_spec\n",
    "\n",
    "            data = cycle_simulator_2(i, x, graph_2,out_graph,in_graph, [leny+20,2*(leny+40)],1e-6) # faster parameters\n",
    "                                       #(, graph, out_graph, in_graph, iteration_set, cutoff, all_spec, out_spec)\n",
    "            # data = cycle_simulator_2(i, x, graph_2,out_graph,in_graph, [leny+200,2*(leny+400)],0) # starter slower parameters\n",
    "            new_graph[x] = data\n",
    "            #for p in out_data:\n",
    "\n",
    "            #    if p in new_graph[x]:\n",
    "\n",
    "            #        new_graph[x][p] += out_data[p]\n",
    "            #    else:\n",
    "            #        new_graph[x][p] = out_data[p]\n",
    "        for y in i:\n",
    "            if y not in in_cycle_spec:\n",
    "                for s in out_graph[y]:\n",
    "                    if y in new_in[s]:\n",
    "                        new_in[s].remove(y)\n",
    "                new_in[y] = set()\n",
    "                new_out[y] = set()\n",
    "                new_graph[y] = set()\n",
    "    #t9 = time.time()\n",
    "    #print(\"got to evaluate function before get yields\")\n",
    "    yi = get_yields(root, 1e-14, new_graph, new_in, new_out, back_set, spec_len) # getting yields (the test prod coefficients will be in the graph)\n",
    "    #print(\"finished getting yields\")\n",
    "    yi2 = {inputs['spc'][i]:yi[i] for i in range(spec_len)} # putting yields in dictionary\n",
    "    # print('Yields Calculated')\n",
    "    return yi2, new_graph, new_in #, out_graph_type, in_graph_type, r_g_clean\n",
    "    # new_graph[i] (same indexing as species list) is a dictionary {j:0.8, k:0.5, l:0.1} where j, k, and l are species that i goes to\n",
    "    # new_in[j] is a set with species {i}, lists incoming species\n",
    "    # compare species will be formatted as a list of species\n",
    "      # most species in reduced mechanism compare 1 to 1 with reference, but some compare to a group of species\n",
    "    # check if species are in full species list and if not then take out of category\n",
    "\n",
    "\n",
    "\n",
    "def get_yields(start, cutoff, graph, in_graph, out_graph, back_set, spec_len):\n",
    "    yields = {i:0 for i in range(spec_len)}\n",
    "    yields[start] = 1\n",
    "    search_list = {start:1}\n",
    "    while len(search_list)>0: # essentially a recursive function that works until every part of the graph is mapped\n",
    "        # print(len(search_list))\n",
    "        new_list = {}\n",
    "        for i in search_list:\n",
    "            for j in graph[i]:\n",
    "                val = search_list[i]*graph[i][j] # multiplying prior molecule by the coefficient of the molecule that it reacts into\n",
    "                yields[j] = yields[j] + val # species could be in multiple reactions, so adding each yield onto each other\n",
    "                if val>cutoff and j not in back_set:\n",
    "                    if j in new_list:\n",
    "                        new_list[j] = new_list[j] + val\n",
    "                    else:\n",
    "                        new_list[j] = val\n",
    "        search_list = {i:new_list[i] for i in new_list}\n",
    "\n",
    "        #print(random.choice(list(search_list.keys())))\n",
    "    return yields\n",
    "\n",
    "def strongly_connected_components(graph):\n",
    "    \"\"\"\n",
    "    Tarjan's Algorithm (named for its discoverer, Robert Tarjan) is a graph theory algorithm\n",
    "    for finding the strongly connected components of a graph.\n",
    "\n",
    "    Based on: http://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    index_counter = [0]\n",
    "    stack = []\n",
    "    lowlinks = {}\n",
    "    index = {}\n",
    "    result = []\n",
    "\n",
    "    def strongconnect(node):\n",
    "        # set the depth index for this node to the smallest unused index\n",
    "        index[node] = index_counter[0]\n",
    "        lowlinks[node] = index_counter[0]\n",
    "        index_counter[0] += 1\n",
    "        stack.append(node)\n",
    "\n",
    "        # Consider successors of `node`\n",
    "        try:\n",
    "            successors = graph[node]\n",
    "        except:\n",
    "            successors = []\n",
    "        for successor in successors:\n",
    "            if successor not in lowlinks:\n",
    "                # Successor has not yet been visited; recurse on it\n",
    "                strongconnect(successor)\n",
    "                lowlinks[node] = min(lowlinks[node],lowlinks[successor])\n",
    "            elif successor in stack:\n",
    "                # the successor is in the stack and hence in the current strongly connected component (SCC)\n",
    "                lowlinks[node] = min(lowlinks[node],index[successor])\n",
    "\n",
    "        # If `node` is a root node, pop the stack and generate an SCC\n",
    "        if lowlinks[node] == index[node]:\n",
    "            connected_component = []\n",
    "\n",
    "            while True:\n",
    "                successor = stack.pop()\n",
    "                connected_component.append(successor)\n",
    "                if successor == node: break\n",
    "            component = tuple(connected_component)\n",
    "            # storing the result\n",
    "            result.append(component)\n",
    "\n",
    "    for node in graph:\n",
    "        if node not in lowlinks:\n",
    "            strongconnect(node)\n",
    "\n",
    "    return result\n",
    "\n",
    "#get_prod_reac(eq_list)\n",
    "#takes eq list and separates into format reac_list2 = [[r1,r2],[r1],...], prod_list2 = [[r1,r2],[r1],...],\n",
    "#reac_coeff_list = [[1,1],[1],...], prod_coeff_list = [[1,1],[2],...] (as examples)\n",
    "def get_prod_reac(eq_list):\n",
    "    nums = ['0','1','2','3','4','5','6','7','8','9']\n",
    "    reac_list = [i[0].split(' = ')[0].split(' + ') for i in eq_list]\n",
    "    prod_list = [i[0].split(' = ')[1].split(' + ') for i in eq_list]\n",
    "    prod_list2 = deepcopy(prod_list)\n",
    "    prod_coeff_list = deepcopy(prod_list)\n",
    "    reac_list2 = deepcopy(reac_list)\n",
    "    reac_coeff_list = deepcopy(reac_list)\n",
    "    a=[]\n",
    "    for i in range(len(prod_list2)):\n",
    "        for j in range(len(prod_list2[i])):\n",
    "            if prod_list2[i][j][:1] in nums:\n",
    "                a=re.split('([a-zA-Z])',prod_list[i][j],1)\n",
    "                prod_list2[i][j]=a[1]+a[2]\n",
    "                prod_coeff_list[i][j]=a[0]\n",
    "            else:\n",
    "                prod_coeff_list[i][j]=1\n",
    "    b=[]\n",
    "    for i in range(len(reac_list2)):\n",
    "        for j in range(len(reac_list2[i])):\n",
    "            if reac_list2[i][j][:1] in nums:\n",
    "                b=re.split('([a-zA-Z])',reac_list[i][j],1)\n",
    "                reac_list2[i][j]=b[1]+b[2]\n",
    "                reac_coeff_list[i][j]=b[0]\n",
    "            else:\n",
    "                reac_coeff_list[i][j]=1\n",
    "    for i in range(len(prod_coeff_list)):\n",
    "        for j in range(len(prod_coeff_list[i])):\n",
    "            prod_coeff_list[i][j] = float(prod_coeff_list[i][j])\n",
    "    for i in range(len(reac_coeff_list)):\n",
    "        for j in range(len(reac_coeff_list[i])):\n",
    "            reac_coeff_list[i][j] = float(reac_coeff_list[i][j])\n",
    "    return reac_list2,reac_coeff_list,prod_list2,prod_coeff_list\n",
    "\n",
    "def rxn_index_convert(reac,prod, dic, background_spc, background_spc_n):\n",
    "    reac_len = len(reac)\n",
    "    reac_list = deepcopy(reac)\n",
    "    prod_list = deepcopy(prod)\n",
    "    reac_no_back = [[] for i in range(reac_len)]\n",
    "    prod_no_back = [[] for i in range(reac_len)]\n",
    "    rxn_reac = [[] for i in range(len(dic))]\n",
    "    rxn_prod = [[] for i in range(len(dic))]\n",
    "    for i in range(reac_len):\n",
    "        for j in range(len(reac_list[i])):\n",
    "            spec = reac[i][j]\n",
    "            if spec in background_spc_n:\n",
    "                ind = background_spc[background_spc_n.index(spec)]\n",
    "                reac_list[i][j] = ind\n",
    "                rxn_reac[ind].append(i)\n",
    "            else:\n",
    "                ind = dic.get(spec)\n",
    "                rxn_reac[ind].append(i)\n",
    "                reac_list[i][j] = ind\n",
    "                reac_no_back[i].append(ind)\n",
    "        for k in range(len(prod_list[i])):\n",
    "            spec = prod[i][k]\n",
    "            if spec in background_spc_n:\n",
    "                ind = background_spc[background_spc_n.index(spec)]\n",
    "                prod_list[i][k] = ind\n",
    "                rxn_prod[ind].append(i)\n",
    "            else:\n",
    "                ind = dic.get(spec)\n",
    "                prod_list[i][k] = ind\n",
    "                prod_no_back[i].append(ind)\n",
    "\n",
    "                rxn_prod[ind].append(i)\n",
    "    return reac_list, prod_list, reac_no_back, prod_no_back, rxn_reac, rxn_prod\n",
    "\n",
    "#pressure_to_m(P,T)\n",
    "#number density is molecules per cubic centimeter\n",
    "#it is n = p/kT, if p is in mbar, then that is equivalent to 100 pascals, so we multiple k by\n",
    "def pressure_to_m(P,T):\n",
    "    Na = 6.022e23; #molecules per mole\n",
    "    R = 8.314e4; # cm^3 mbar /K /mol\n",
    "    M = Na*P/(R*T)\n",
    "    return M\n",
    "\n",
    "#j_func(sza,I,m,n)\n",
    "#J = I * cos(SZA)^m * exp(-n * sec(SZA))\n",
    "def j_func(sza,I,m,n):\n",
    "    a = I*(np.cos(sza*(2*np.pi)/360)**m)\n",
    "    b = np.exp(-n/(np.cos(sza*(2*np.pi)/360)))\n",
    "    return a*b\n",
    "\n",
    "def get_k_list(rate_list, sza, temp):\n",
    "    TEMP = temp\n",
    "    k_list = []\n",
    "    for i in rate_list:\n",
    "        # print(i)\n",
    "        if 'J' in i:\n",
    "            if 'J(22)' in i or 'J22' in i:\n",
    "                I = 5.804e-6;\n",
    "                m = 1.092;\n",
    "                n = 0.377;\n",
    "                k_list.append(j_func(sza,I,m,n)*SUN)\n",
    "            elif 'J(34)' in i or 'J34' in i:\n",
    "                I = 1.537e-4;\n",
    "                m = 0.170;\n",
    "                n = 0.208;\n",
    "                k_list.append(j_func(sza,I,m,n)*SUN)\n",
    "            elif 'J(41)' in i or 'J41' in i:\n",
    "                I = 7.649e-6;\n",
    "                m = 0.682;\n",
    "                n = 0.279;\n",
    "                k_list.append(j_func(sza,I,m,n)*SUN)\n",
    "            elif 'J(31)' in i or 'J31' in i:\n",
    "                I = 6.845e-5\n",
    "                m = 0.130\n",
    "                n = 0.201\n",
    "                k_list.append(j_func(sza,I,m,n)*SUN)\n",
    "            elif 'J(32)' in i or 'J32' in i:\n",
    "                I = 1.032e-5\n",
    "                m = 0.130\n",
    "                n = 0.201\n",
    "                k_list.append(j_func(sza,I,m,n)*SUN)\n",
    "            elif 'J(33)' in i or 'J33' in i:\n",
    "                I = 3.802e-5\n",
    "                m = 0.644\n",
    "                n = 0.312\n",
    "                k_list.append(j_func(sza,I,m,n)*SUN)\n",
    "            elif 'J(11)' in i or 'J11' in i:\n",
    "                I = 4.642e-5\n",
    "                m = 0.762\n",
    "                n = 0.353\n",
    "                k_list.append(j_func(sza,I,m,n)*SUN)\n",
    "            elif 'J(12)' in i or 'J12' in i:\n",
    "                I = 6.853e-5\n",
    "                m = 0.477\n",
    "                n = 0.323\n",
    "                k_list.append(j_func(sza,I,m,n)*SUN)\n",
    "            elif 'J(15)' in i or 'J15' in i:\n",
    "                I = 2.792e-5\n",
    "                m = 0.805\n",
    "                n = 0.338\n",
    "                k_list.append(j_func(sza,I,m,n)*SUN)\n",
    "            elif 'J(51)' in i or 'J51' in i:\n",
    "                I = 1.588e-6;\n",
    "                m = 1.154;\n",
    "                n = 0.318;\n",
    "                k_list.append(j_func(sza,I,m,n)*SUN)\n",
    "        elif isinstance(i,str):\n",
    "            k_list.append(eval(i))\n",
    "        else:\n",
    "            k_list.append(i)\n",
    "    return k_list\n",
    "\n",
    "\"\"\"Code\"\"\"\n",
    "\n",
    "\n",
    "def arrhenius(A,n,Ea,T):\n",
    "    rate = A*(T**n)*math.exp(-Ea/T)\n",
    "    return rate\n",
    "def falloff(a1,a2,a3,f1,f2,f3,f4,f5,f6,f7,temp,m):\n",
    "    e1 = f1*((temp/300)**f2)*math.exp(-f3/temp)*m\n",
    "    r1 = e1/(a1*((temp/300)**a2))*math.exp(-a3/temp)\n",
    "    fr1 = 1/(1+((math.log10(r1))**2))\n",
    "    if f5 == 0 or f6 == 0:\n",
    "        lnrate = math.log(e1/(1+r1)) + fr1*math.log(f4)\n",
    "    else:\n",
    "        fr2 = (1-f4)*math.exp(-temp/f5) + f4*math.exp(-temp/f6) + math.exp(-f7/temp)\n",
    "        if f7== 0:\n",
    "            fr2 = fr2 -1\n",
    "        lnrate = math.log(e1/(1+r1)) + fr1*math.log(fr2)\n",
    "    return math.exp(lnrate)\n",
    "def get_rates(rate_numbers, rate_type,a_list_2,b_list_3,temp,press,M,sza):\n",
    "    rates = []\n",
    "    for i in range(len(rate_numbers)):\n",
    "        if rate_type[i] == 'WALL' or rate_type[i] == 'EXTRA':\n",
    "            rates.append(0)\n",
    "        elif rate_type[i] == 'HV':\n",
    "            ind = a_list_2.index(rate_numbers[i][1][0])\n",
    "            ind2 = b_list_3[ind].index(sza)\n",
    "            r = b_list_3[ind][ind2+1]*rate_numbers[i][1][1]\n",
    "            rates.append(r)\n",
    "        elif rate_type[i] == 'FALLOFF':\n",
    "            rates.append(falloff(rate_numbers[i][0][0],rate_numbers[i][0][1],rate_numbers[i][0][2],rate_numbers[i][1][0],rate_numbers[i][1][1],rate_numbers[i][1][2],rate_numbers[i][1][3],rate_numbers[i][1][4],rate_numbers[i][1][5],rate_numbers[i][1][6],temp,M))\n",
    "        elif rate_type[i] == 'NORMAL':\n",
    "            r = arrhenius(rate_numbers[i][0],rate_numbers[i][1],rate_numbers[i][2],temp)\n",
    "            rates.append(r)\n",
    "        else:\n",
    "            rates.append(0)\n",
    "    return rates\n",
    "\n",
    "\n",
    "def arrhenius(A,n,Ea,T):\n",
    "    rate = A*(T**n)*math.exp(-Ea/T)\n",
    "    return rate\n",
    "def falloff(a1,a2,a3,f1,f2,f3,f4,f5,f6,f7,temp,m):\n",
    "    e1 = f1*((temp/300)**f2)*math.exp(-f3/temp)*m\n",
    "    r1 = e1/(a1*((temp/300)**a2))*math.exp(-a3/temp)\n",
    "    fr1 = 1/(1+((math.log10(r1))**2))\n",
    "    if f5 == 0 or f6 == 0:\n",
    "        lnrate = math.log(e1/(1+r1)) + fr1*math.log(f4)\n",
    "    else:\n",
    "        fr2 = (1-f4)*math.exp(-temp/f5) + f4*math.exp(-temp/f6) + math.exp(-f7/temp)\n",
    "        if f7== 0:\n",
    "            fr2 = fr2 -1\n",
    "        lnrate = math.log(e1/(1+r1)) + fr1*math.log(fr2)\n",
    "    return math.exp(lnrate)\n",
    "def get_rates(rate_numbers, rate_type,a_list_2,b_list_3,temp,press,M,sza):\n",
    "    rates = []\n",
    "    for i in range(len(rate_numbers)):\n",
    "        if rate_type[i] == 'WALL' or rate_type[i] == 'EXTRA':\n",
    "            rates.append(0)\n",
    "        elif rate_type[i] == 'HV':\n",
    "            ind = a_list_2.index(rate_numbers[i][1][0])\n",
    "            ind2 = b_list_3[ind].index(sza)\n",
    "            r = b_list_3[ind][ind2+1]*rate_numbers[i][1][1]\n",
    "            rates.append(r)\n",
    "        elif rate_type[i] == 'FALLOFF':\n",
    "            rates.append(falloff(rate_numbers[i][0][0],rate_numbers[i][0][1],rate_numbers[i][0][2],rate_numbers[i][1][0],rate_numbers[i][1][1],rate_numbers[i][1][2],rate_numbers[i][1][3],rate_numbers[i][1][4],rate_numbers[i][1][5],rate_numbers[i][1][6],temp,M))\n",
    "        elif rate_type[i] == 'NORMAL':\n",
    "            r = arrhenius(rate_numbers[i][0],rate_numbers[i][1],rate_numbers[i][2],temp)\n",
    "            rates.append(r)\n",
    "        else:\n",
    "            rates.append(0)\n",
    "    return rates\n",
    "\n",
    "# does this produce an evaluation diffferent than the evaluation function a few cells above?\n",
    "def evaluate_old(coeff, inputs_test, inputs_ref):\n",
    "  # inputs required: compare_species_r, compare_species, reference_data, input_conditions\n",
    "  # inputs_test = {'spc':species_list_names_r, 'rxn':rxn_dict, 'bck':background_dict_2, 'scnd':scnd_dict, 'atm cond':atm_cond,'settings':settings}\n",
    "  inputs_test['rxn']['prod'] = prod_list_n_r\n",
    "  inputs_test['rxn']['prod_coeff'] = coeff # inputs_test['rxn']['prod_coeff'] = prod_coeff_list_r\n",
    "  reduction_data = [{} for i in range(len(input_conditions))]\n",
    "  for i in range(len(input_conditions)):\n",
    "      background_dict_2['OH'] = input_conditions[i]['OH']\n",
    "      background_dict_2['HO2'] = input_conditions[i]['HO2']\n",
    "      background_dict_2['NO'] = input_conditions[i]['NO']\n",
    "      background_dict_2['O3'] = input_conditions[i]['O3']\n",
    "      background_dict_2['NO3'] = input_conditions[i]['NO3']\n",
    "      background_dict_2['CH3OO'] = input_conditions[i]['CH3OO']\n",
    "      background_dict_2['CH3CO3'] = input_conditions[i]['CH3CO3']\n",
    "      background_dict_2['O2'] = 210000000\n",
    "      atm_cond['sza'] = input_conditions[i]['sza']\n",
    "      atm_cond['sun'] = input_conditions[i]['sun']\n",
    "      inputs_test['bck'] = background_dict_2\n",
    "      yields_test, new_graph, new_in = get_yields_from_inputs(inputs_test)\n",
    "      for j in compare_species_r:\n",
    "          if isinstance(compare_species_r[j], list):\n",
    "              cat_yield = 0\n",
    "              for k in compare_species[j]:\n",
    "                if k in species_list_names_r and k in yields_test:\n",
    "                    cat_yield+= yields_test[k]\n",
    "              reduction_data[i][j] = cat_yield\n",
    "          else:\n",
    "              reduction_data[i][j] = yields_test[compare_species_r[j]]\n",
    "  score_matrix = [{},{},{},{},{},{}]\n",
    "  score_list = []\n",
    "  for i in range(len(input_conditions)):\n",
    "      counter = 0\n",
    "      for j in compare_species:\n",
    "          ref_score = reference_data[i][j]\n",
    "          red_score = 0\n",
    "          spec_score = spec_factor[j]\n",
    "          red_score = reduction_data[i][j]\n",
    "          #(abs(ref_score)/spec_score)*\n",
    "          if spec_factor[j]!=-1:\n",
    "              score = (red_score - ref_score)/abs(abs(red_score)+abs(ref_score))\n",
    "              score_matrix[i][j] = score\n",
    "              score_list.append(abs(score))\n",
    "          else:\n",
    "              score = (red_score - ref_score)/abs(abs(red_score)+abs(ref_score))\n",
    "              score_matrix[i][j] = score\n",
    "              score_list.append(abs(score))\n",
    "          counter += 1\n",
    "  avg_score = np.mean(score_list)\n",
    "  return avg_score, score_matrix\n",
    "\n",
    "\n",
    "# inputs: conditions, background dict, test, compare_species, compare_species_r, species_list_names_r, reference_data, spec_factor,\n",
    "\n",
    "def evaluate(coeff, inputs):\n",
    "  # inputs required: compare_species_r, compare_species, reference_data, input_conditions\n",
    "  # inputs_test = {'spc':species_list_names_r, 'rxn':rxn_dict, 'bck':background_dict_2, 'scnd':scnd_dict, 'atm cond':atm_cond,'settings':settings}\n",
    "  inputs['test']['rxn']['prod'] = prod_list_n_r\n",
    "  inputs['test']['rxn']['prod_coeff'] = coeff # inputs_test['rxn']['prod_coeff'] = prod_coeff_list_r\n",
    "  inputs['test']['scc'] = inputs['scc']\n",
    "  reduction_data = [{} for i in range(len(inputs['conditions']))]\n",
    "  background_dict_2 = deepcopy(inputs['bck'])\n",
    "  for i in range(len(inputs['conditions'])):\n",
    "      background_dict_2['OH'] = inputs['conditions'][i]['OH']\n",
    "      background_dict_2['HO2'] = inputs['conditions'][i]['HO2']\n",
    "      background_dict_2['NO'] = inputs['conditions'][i]['NO']\n",
    "      background_dict_2['O3'] = inputs['conditions'][i]['O3']\n",
    "      background_dict_2['NO3'] = inputs['conditions'][i]['NO3']\n",
    "      background_dict_2['CH3OO'] = inputs['conditions'][i]['CH3OO']\n",
    "      background_dict_2['CH3CO3'] = inputs['conditions'][i]['CH3CO3']\n",
    "      background_dict_2['O2'] = 210000000\n",
    "      atm_cond['sza'] = inputs['conditions'][i]['sza']\n",
    "      atm_cond['sun'] = inputs['conditions'][i]['sun']\n",
    "      inputs['test']['bck'] = deepcopy(background_dict_2)\n",
    "      yields_test, new_graph, new_in = get_yields_from_inputs(inputs['test'])\n",
    "      for j in inputs['compare_species_r']:\n",
    "          if isinstance(inputs['compare_species_r'][j], list):\n",
    "              cat_yield = 0\n",
    "              for k in inputs['compare_species'][j]:\n",
    "                if k in inputs['spc'] and k in yields_test:\n",
    "                    cat_yield+= yields_test[k]\n",
    "              reduction_data[i][j] = cat_yield\n",
    "          else:\n",
    "              reduction_data[i][j] = yields_test[inputs['compare_species_r'][j]]\n",
    "  score_matrix = [{},{},{},{},{},{}]\n",
    "  score_list = []\n",
    "  for i in range(len(inputs['conditions'])):\n",
    "      counter = 0\n",
    "      for j in inputs['compare_species']:\n",
    "          ref_score = inputs['reference data'][i][j]\n",
    "          red_score = 0\n",
    "          spec_score = inputs['spec factor'][j]\n",
    "          red_score = reduction_data[i][j]\n",
    "          #(abs(ref_score)/spec_score)*\n",
    "          if spec_factor[j]!=-1:\n",
    "              score = (red_score - ref_score)/abs(abs(red_score)+abs(ref_score))\n",
    "              score_matrix[i][j] = score\n",
    "              score_list.append(abs(score))\n",
    "          else:\n",
    "              score = (red_score - ref_score)/abs(abs(red_score)+abs(ref_score))\n",
    "              score_matrix[i][j] = score\n",
    "              score_list.append(abs(score))\n",
    "          counter += 1\n",
    "  avg_score = np.mean(score_list)\n",
    "  return avg_score, score_matrix\n",
    "\n",
    "#avg_score, score_matrix = evaluate(prod_coeff_list_r)\n",
    "\n",
    "#print(\"Initial evalution:\", avg_score)\n",
    "#print(\"Initial score matrix:\", score_matrix)\n",
    "\n",
    "def test_get_yields_from_inputs(inputs, yCoeff, yCoeff2, cutoff):\n",
    "    t1 = time.time()\n",
    "    # create numerical species list\n",
    "    species_list = list(range(len(inputs['spc'])))\n",
    "    spec_len = len(species_list)\n",
    "    dic = {}\n",
    "    dic = {inputs['spc'][i]:i for i in range(spec_len)}\n",
    "    reac_len = len(inputs['rxn']['reac'])\n",
    "    #numerical background species list\n",
    "    background_spc = []\n",
    "    background_spc_n = []\n",
    "    for i in inputs['bck']:\n",
    "        if i in inputs['spc']:\n",
    "            background_spc.append(inputs['spc'].index(i))\n",
    "            background_spc_n.append(i)\n",
    "    back_set = set(background_spc)\n",
    "    # root node\n",
    "    root = inputs['spc'].index(inputs['settings']['root'])\n",
    "    t2 = time.time()\n",
    "    #print('Species list created. Root node = ' + str(root) + '.' + ' Step time = '+ str(t2-t1) + '.')\n",
    "    #change species to numerical values\n",
    "    reac_list, prod_list, reac_no_back, prod_no_back, rxns_reac, rxns_prod = rxn_index_convert(inputs['rxn']['reac'],inputs['rxn']['prod'],dic, background_spc, background_spc_n)\n",
    "    rxns_reac = [set(i) for i in rxns_reac]\n",
    "    prod_coeff_list = inputs['rxn']['prod_coeff'] # this is our prod coeff list\n",
    "    reac_coeff_list = inputs['rxn']['reac_coeff']\n",
    "    for i in background_spc:\n",
    "        rxns_reac[i] = set()\n",
    "    t3 = time.time()\n",
    "    #print('Reaction list reformatted.' + ' Step time = '+ str(t3-t2) + '.')\n",
    "    #get rates of rxn\n",
    "    p_fac = pressure_to_m(inputs['atm cond']['pres'],inputs['atm cond']['temp'])/1000000000\n",
    "    rate_list = []\n",
    "    TEMP = inputs['atm cond']['temp']\n",
    "    M = p_fac\n",
    "    k_list = get_k_list(inputs['rxn']['k'],inputs['atm cond']['sza'], TEMP)\n",
    "    for i in range(len(reac_list)):\n",
    "        if len(reac_list[i])==1:\n",
    "            rate_list.append(k_list[i])\n",
    "        else:\n",
    "            mark = True\n",
    "            for j in reac_list[i]:\n",
    "                if j in background_spc:\n",
    "                    mark = False\n",
    "                    conc = inputs['bck'][inputs['spc'][j]]\n",
    "            if mark:\n",
    "                conc = inputs['scnd'][inputs['rxn']['reac'][i][1]]\n",
    "            rate_list.append(k_list[i]*conc*p_fac)\n",
    "    t4 = time.time()\n",
    "    #print('Reaction rates calculated.' + ' Step time = '+ str(t4-t3) + '.')\n",
    "    # remove less important reactions of a species\n",
    "    #create prod_dict\n",
    "    rxn_dict = [{} for i in range(reac_len)]\n",
    "    prod_dict = []\n",
    "    for i in range(reac_len):\n",
    "        prods = {}\n",
    "        for j in reac_list[i]:\n",
    "            if j in back_set:\n",
    "                prods[j] = -1\n",
    "        for k in range(len(prod_list[i])):\n",
    "            spec = prod_list[i][k]\n",
    "            if spec in prods:\n",
    "                prods[spec] = prods[spec] + prod_coeff_list[i][k]\n",
    "            else:\n",
    "                prods[spec] = prod_coeff_list[i][k]\n",
    "        prod_dict.append(prods)\n",
    "        rxn_dict[i]['prod'] = prods\n",
    "        reacys = set()\n",
    "        for j in reac_list[i]:\n",
    "            reacys.add(j)\n",
    "        rxn_dict[i]['reac'] = reacys\n",
    "        rxn_dict[i]['k'] = k_list[i]\n",
    "        rxn_dict[i]['r'] = inputs['rxn']['rates'][i]\n",
    "    #create graph\n",
    "    graph = []\n",
    "    in_graph = [set() for i in range(spec_len)]\n",
    "    out_graph = [set() for i in range(spec_len)]\n",
    "    out_graph_type = [{} for i in range(spec_len)]\n",
    "    in_graph_type  = [{} for i in range(spec_len)]\n",
    "    for i in range(spec_len):\n",
    "        if i not in back_set:\n",
    "            edges = {}\n",
    "            rate_sum = 0\n",
    "            for j in rxns_reac[i]:\n",
    "                rate_sum = rate_sum + rate_list[j]\n",
    "            for j in rxns_reac[i]:\n",
    "                mult = rate_list[j]/max(1e-20,rate_sum)\n",
    "                if len(reac_list[j]) == 1:\n",
    "                    type_r = 'solo'\n",
    "                else:\n",
    "                    mark = True\n",
    "                    for p in reac_list[j]:\n",
    "                        if p in background_spc:\n",
    "                            type_r = p\n",
    "                            mark = False\n",
    "                    if mark:\n",
    "                        type_r = 'double'\n",
    "                for k in prod_dict[j]:\n",
    "                    if i in in_graph_type[k]:\n",
    "                        in_graph_type[k][i].add(type_r)\n",
    "                    else:\n",
    "                        in_graph_type[k][i] = set([type_r])\n",
    "                    if k in edges:\n",
    "                        edges[k] = edges[k] + prod_dict[j][k]*mult\n",
    "                        out_graph[i].add(k)\n",
    "                        in_graph[k].add(i)\n",
    "                        out_graph_type[i][k].add(type_r)\n",
    "\n",
    "                    else:\n",
    "                        edges[k] = prod_dict[j][k]*mult\n",
    "                        out_graph[i].add(k)\n",
    "                        in_graph[k].add(i)\n",
    "                        out_graph_type[i][k] = set([type_r])\n",
    "            graph.append(edges)\n",
    "        else:\n",
    "            graph.append({})\n",
    "    # strongly connected component identification, code source: http://www.logarithmic.net/pfh/blog/01208083168\n",
    "    test_graph = {}\n",
    "    for i in range(len(out_graph)):\n",
    "        test_graph[i] = list(out_graph[i])\n",
    "    scc = strongly_connected_components(test_graph)\n",
    "    scc_2 = []\n",
    "    for i in scc:\n",
    "        if len(i) >1:\n",
    "            scc_2.append(i)\n",
    "    scc_lens = [len(i) for i in scc_2]\n",
    "    t7 = time.time()\n",
    "\n",
    "    remove_specs = set()\n",
    "    protected = [inputs['spc'].index(p) for p in inputs['settings']['protected']]\n",
    "    for i in range(spec_len):\n",
    "        if i not in protected:\n",
    "            if len(in_graph[i])==0:\n",
    "                remove_specs.add(i)\n",
    "\n",
    "    old_list = set()\n",
    "    while len(remove_specs)>0:\n",
    "        new_specs = set()\n",
    "        for i in remove_specs:\n",
    "            old_list.add(i)\n",
    "            for j in out_graph[i]:\n",
    "                in_graph[j].discard(i)\n",
    "        for i in range(spec_len):\n",
    "            if i not in protected:\n",
    "                if i not in old_list:\n",
    "                    if len(in_graph[i])==0:\n",
    "                        new_specs.add(i)\n",
    "        remove_specs = {i for i in new_specs}\n",
    "    old_list.discard(root)\n",
    "    graph_2 = []\n",
    "    for i in graph:\n",
    "        new_edge = {}\n",
    "        for j in i:\n",
    "            if j not in old_list:\n",
    "                new_edge[j] = i[j]\n",
    "        graph_2.append(new_edge)\n",
    "\n",
    "\n",
    "    new_graph = []\n",
    "    for i in graph_2:\n",
    "        dicy = {}\n",
    "        for j in i:\n",
    "            dicy[j] = i[j]\n",
    "        new_graph.append(dicy)\n",
    "\n",
    "    new_in = []\n",
    "    for i in in_graph:\n",
    "        sety = set()\n",
    "        for j in i:\n",
    "            sety.add(j)\n",
    "        new_in.append(sety)\n",
    "\n",
    "    new_out = []\n",
    "    for i in out_graph:\n",
    "        sety = set()\n",
    "        for j in i:\n",
    "            sety.add(j)\n",
    "        new_out.append(sety)\n",
    "\n",
    "    scc_set = set()\n",
    "    for i in scc_2:\n",
    "        for j in i:\n",
    "            scc_set.add(j)\n",
    "    scc_dict = {}\n",
    "    for j in range(len(scc_2)):\n",
    "        for k in scc_2[j]:\n",
    "            scc_dict.update({k:j})\n",
    "    #new_graph = [i for i in graph_2]\n",
    "    #new_in = [i for i in in_graph]\n",
    "    #new_out = [i for i in out_graph]\n",
    "    count = 0\n",
    "    for i in scc_2:\n",
    "        count = count + 1\n",
    "        #if count%100 == 0:\n",
    "        #   print(count)\n",
    "        leny = int(np.sqrt(len(i)))\n",
    "        in_cycle_spec = set()\n",
    "        out_spec = set()\n",
    "        for p in i:\n",
    "            for k in in_graph[p]:\n",
    "                if k not in i:\n",
    "                    in_cycle_spec.add(p)\n",
    "            for k in out_graph[p]:\n",
    "                if k not in i:\n",
    "                    out_spec.add(k)\n",
    "\n",
    "        count_3 = 0\n",
    "\n",
    "        for x in in_cycle_spec:\n",
    "            new_out[x] = out_spec\n",
    "\n",
    "            data = cycle_simulator_2(i, x, graph_2,out_graph,in_graph, [leny+yCoeff,2*(leny+yCoeff2)],cutoff)\n",
    "            # data = cycle_simulator_2(i, x, graph_2,out_graph,in_graph, [leny+200,2*(leny+400)],0)\n",
    "            new_graph[x] = data\n",
    "        for y in i:\n",
    "            if y not in in_cycle_spec:\n",
    "                for s in out_graph[y]:\n",
    "                    if y in new_in[s]:\n",
    "                        new_in[s].remove(y)\n",
    "                new_in[y] = set()\n",
    "                new_out[y] = set()\n",
    "                new_graph[y] = set()\n",
    "    t9 = time.time()\n",
    "    yi = get_yields(root, 1e-14, new_graph, new_in, new_out, back_set, spec_len) # getting yields (the test prod coefficients will be in the graph)\n",
    "    yi2 = {inputs['spc'][i]:yi[i] for i in range(spec_len)} # putting yields in dictionary\n",
    "    # print('Yields Calculated')\n",
    "    return yi2 #, out_graph_type, in_graph_type, r_g_clean\n",
    "\n",
    "# does this produce an evaluation diffferent than the evaluation function a few cells above?\n",
    "def test_evaluate(coeff, yCoeff, yCoeff2, cutoff):\n",
    "  inputs_test['rxn']['prod'] = prod_list_n_r\n",
    "  inputs_test['rxn']['prod_coeff'] = coeff # inputs_test['rxn']['prod_coeff'] = prod_coeff_list_r\n",
    "  reduction_data = [{} for i in range(len(input_conditions))]\n",
    "  for i in range(len(input_conditions)):\n",
    "      background_dict_2['OH'] = input_conditions[i]['OH']\n",
    "      background_dict_2['HO2'] = input_conditions[i]['HO2']\n",
    "      background_dict_2['NO'] = input_conditions[i]['NO']\n",
    "      background_dict_2['O3'] = input_conditions[i]['O3']\n",
    "      background_dict_2['NO3'] = input_conditions[i]['NO3']\n",
    "      background_dict_2['CH3OO'] = input_conditions[i]['CH3OO']\n",
    "      background_dict_2['CH3CO3'] = input_conditions[i]['CH3CO3']\n",
    "      background_dict_2['O2'] = 210000000\n",
    "      atm_cond['sza'] = input_conditions[i]['sza']\n",
    "      atm_cond['sun'] = input_conditions[i]['sun']\n",
    "      inputs_test['bck'] = background_dict_2\n",
    "      yields_test = test_get_yields_from_inputs(inputs_test, yCoeff, yCoeff2, cutoff)\n",
    "      for j in compare_species:\n",
    "          reduction_data[i][j] = yields_test[j]\n",
    "  score_matrix = [{},{},{},{},{},{}]\n",
    "  score_list = []\n",
    "  for i in range(len(input_conditions)):\n",
    "      for j in compare_species:\n",
    "          if spec_factor[j]!=1:\n",
    "              score = (abs(reference_data[i][j])/spec_factor[j])*(reduction_data[i][j] - reference_data[i][j])/abs(abs(reduction_data[i][j])+abs(reference_data[i][j]))\n",
    "              score_matrix[i][j] = score\n",
    "              score_list.append(abs(score))\n",
    "          else:\n",
    "              score = (reduction_data[i][j] - reference_data[i][j])/abs(abs(reduction_data[i][j])+abs(reference_data[i][j]))\n",
    "              score_matrix[i][j] = score\n",
    "              score_list.append(abs(score))\n",
    "  avg_score = np.mean(score_list)\n",
    "  return avg_score, score_matrix\n",
    "\n",
    "def find_yield_change(ignore_coeffs, matrix_keys, inputs, new_prod_coeffs):\n",
    "  print(\"Starting sensitivity analysis\")\n",
    "  #print(ignore_coeffs)\n",
    "  avg_score, score_matrix = evaluate(new_prod_coeffs,inputs)\n",
    "  starter_score = avg_score\n",
    "  # the starter matrix that is commented out is the original starter matrix from the slower parameters, but since we are now analyzing the difference of the yields when each coefficient is changed, we are comparing it to the matrix made with the faster parameters\n",
    "  # starter_matrix = [{'ISOP1OH23O4OHt': -0.09787251925728065, 'ISOP1OH23O4OHc': -0.09558028413305211, 'ISOP1OH2OH34O': -0.015616054795262353, 'MVK': -0.00011169877812379283, 'HCHO': -0.05183290208181358, 'HAC': 0.19556194751656328, 'GLYC': -0.514966777088693, 'MGLY': 0.18847700857726427, 'MACR': -0.032902437776138736, 'OH': 0.19404496745100205, 'NO': 0.3174873026885275, 'NO2': -0.11845417702324008, 'O3': 0.1528228566728389, 'HO2': 0.11608801330600477, 'CH3OO': -0.007794277285444426, 'CH3CO3': -0.37894577272962077}, {'ISOP1OH23O4OHt': -0.16125046649836713, 'ISOP1OH23O4OHc': -0.1560599734025661, 'ISOP1OH2OH34O': -0.012129678874647946, 'MVK': 0.00043687481125419806, 'HCHO': -0.04547141562485739, 'HAC': 0.18576642096347573, 'GLYC': -0.38274666049091777, 'MGLY': 0.1744041433091504, 'MACR': -0.03679418730768008, 'OH': 0.12577936902946557, 'NO': 0.18958353702378541, 'NO2': -0.10213591740079915, 'O3': -0.558662938158745, 'HO2': 0.12201984645243, 'CH3OO': -0.29802286443210113, 'CH3CO3': -0.36716456631166233}, {'ISOP1OH23O4OHt': -0.09619082668875414, 'ISOP1OH23O4OHc': -0.09487844433047589, 'ISOP1OH2OH34O': -0.023072351981525192, 'MVK': 8.493505365820696e-05, 'HCHO': -0.045745520515016666, 'HAC': 0.16589678750795747, 'GLYC': -0.5016760439465852, 'MGLY': 0.18679759979838026, 'MACR': -0.012893719923402564, 'OH': 0.18708959609397952, 'NO': 0.19311661076059444, 'NO2': -0.1382948173311869, 'O3': 0.05128034156161733, 'HO2': 0.08253892887776249, 'CH3OO': -0.11878819329186503, 'CH3CO3': -0.3499942922636299}, {'ISOP1OH23O4OHt': -0.13281073405167346, 'ISOP1OH23O4OHc': -0.1287837391959723, 'ISOP1OH2OH34O': -0.012680997696579533, 'MVK': 0.0005600163269455355, 'HCHO': -0.05882275753271708, 'HAC': 0.17819555955141128, 'GLYC': -0.41857012340260313, 'MGLY': 0.19290368412444636, 'MACR': -0.024368687630766498, 'OH': 0.13369586053660287, 'NO': 0.21390567552327483, 'NO2': -0.13656810530869581, 'O3': 0.16506050695467156, 'HO2': 0.09867696918634818, 'CH3OO': -0.013246328711034837, 'CH3CO3': -0.41716433918163687}, {'ISOP1OH23O4OHt': -0.04571803994052719, 'ISOP1OH23O4OHc': -0.04513555434109117, 'ISOP1OH2OH34O': -0.011880375078875204, 'MVK': -0.007864826106213946, 'HCHO': -0.0805283648373068, 'HAC': 0.19870408232460904, 'GLYC': -0.5941527306781748, 'MGLY': 0.19042456456984375, 'MACR': -0.055088332330735866, 'OH': 0.1761138035611046, 'NO': 0.5176109566682872, 'NO2': -0.11868289155513966, 'O3': -0.40141790081752554, 'HO2': 0.0755153168596899, 'CH3OO': 0.050686045631993075, 'CH3CO3': -0.3880358408013048}, {'ISOP1OH23O4OHt': -0.07987689707129189, 'ISOP1OH23O4OHc': -0.07885919941119564, 'ISOP1OH2OH34O': -0.020526640123079507, 'MVK': 0.00047274644565867735, 'HCHO': -0.08753436946477353, 'HAC': 0.2090625149622428, 'GLYC': -0.5861835064366345, 'MGLY': 0.21859244252647558, 'MACR': -0.015436748250977211, 'OH': 0.1483209846021159, 'NO': 0.2382690077063841, 'NO2': -0.1893866691548442, 'O3': 0.16688874712004842, 'HO2': 0.0674360199113856, 'CH3OO': -0.010575745486166974, 'CH3CO3': -0.4778530972933386}]\n",
    "  starter_matrix = score_matrix\n",
    "  empty_matrix_0 = {n:0 for n in matrix_keys}\n",
    "  empty_matrix = [empty_matrix_0,empty_matrix_0,empty_matrix_0,empty_matrix_0,empty_matrix_0,empty_matrix_0]\n",
    "  matrix_len = len(inputs['conditions'])\n",
    "  tick = time.time()\n",
    "  avg_yield_change_list = [] # a list of how much the yield changed for each molecule from the initial yield\n",
    "  yield_change_list_3d = [] # a 3d dictionary of the yields for each molecule (not the change in yield, just the raw yield) in each condition for each species\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # for j in range(22):\n",
    "  county = 0\n",
    "  for j in range(len(new_prod_coeffs)):\n",
    "      for j in range(len(new_prod_coeffs[j])):\n",
    "          county+=1\n",
    "  print(county, ' coefficients')\n",
    "  for j in range(len(new_prod_coeffs)):\n",
    "    for k in range(len(new_prod_coeffs[j])):\n",
    "      #print('finished one')\n",
    "      if new_prod_coeffs[j][k] != 1.0 or ignore_coeffs != \"ones\":\n",
    "        print(\"analyzing\", prod_list_n_r[j][k], new_prod_coeffs[j][k])\n",
    "        new_matrix = deepcopy(empty_matrix)\n",
    "\n",
    "        # calculate the ammount to increase the coefficient by and add that to it\n",
    "        og_val = new_prod_coeffs[j][k]\n",
    "        increase = new_prod_coeffs[j][k]*1.05\n",
    "        new_prod_coeffs[j][k] = new_prod_coeffs[j][k] + increase\n",
    "\n",
    "        # evaluate the new set of coefficients\n",
    "        avg_score, score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "\n",
    "        # find the amount that the score changed with the new coefficients\n",
    "        yield_change = avg_score - starter_score\n",
    "\n",
    "        # update the molecule dictionaries with the score when the molecule is changed\n",
    "        molecule = prod_list_n_r[j][k]\n",
    "\n",
    "        # make 1d list\n",
    "        avg_yield_change_list.append(yield_change)\n",
    "\n",
    "        # make yield change matrix\n",
    "        for m in range(matrix_len):\n",
    "          for n in matrix_keys:\n",
    "            starter_val = starter_matrix[m][n]\n",
    "            test_val = score_matrix[m][n]\n",
    "            diff = test_val - starter_val\n",
    "            new_matrix[m][n] = diff\n",
    "        yield_change_list_3d.append(new_matrix)\n",
    "\n",
    "        # reduce the coefficient back to its original value\n",
    "        new_prod_coeffs[j][k] = og_val\n",
    "      else:\n",
    "        print(\"not analyzing\", prod_list_n_r[j][k], new_prod_coeffs[j][k])\n",
    "        avg_yield_change_list.append(0)\n",
    "        yield_change_list_3d.append(empty_matrix)\n",
    "  tock = time.time()\n",
    "  print(\"Sensitivity analysis complete\")\n",
    "  return yield_change_list_3d, avg_yield_change_list\n",
    "\n",
    "#yield_change_list_3d, avg_yield_change_list = find_yield_change(ignore_coeffs)\n",
    "#pickle_yield_change_list_3d = yield_change_list_3d\n",
    "\n",
    "\"\"\"### 4. Optimization Graph (Import 4 needed)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# create a copy of the sample coefficient list\n",
    "def copy_prod(prod_coeff_list_r):\n",
    "  list_copy = [row[:] for row in prod_coeff_list_r]\n",
    "  return list_copy\n",
    "\n",
    "# map for flattened 2d prod coeff list r list\n",
    "def make_map(prod_coeff_list_r):\n",
    "  map = []\n",
    "  counter = 0\n",
    "  for j in range(len(prod_coeff_list_r)):\n",
    "    for k in range(len(prod_coeff_list_r[j])):\n",
    "      map.append([j, k])\n",
    "\n",
    "  return map\n",
    "\n",
    "\"\"\"Graph\"\"\"\n",
    "\n",
    "def make_graph(pickle_yield_change_list_3d, matrix_keys):\n",
    "  max_coeff = 0\n",
    "\n",
    "  for i in range(len(pickle_yield_change_list_3d)):\n",
    "    for j in range(len(pickle_yield_change_list_3d[i])):\n",
    "      for key in matrix_keys:\n",
    "        if abs(pickle_yield_change_list_3d[i][j][key]) > max_coeff:\n",
    "          max_coeff = pickle_yield_change_list_3d[i][j][key]\n",
    "\n",
    "  # make importance matrix\n",
    "  threshold = max_coeff*0.005\n",
    "  # threshold = 2e-9\n",
    "  threshold_yield_change_list = deepcopy(pickle_yield_change_list_3d)\n",
    "  change_counter = 0\n",
    "  high_counter = 0\n",
    "  low_counter = 0\n",
    "  coeff_counter = 0\n",
    "  negligible_counter = 0\n",
    "\n",
    "  for i in range(len(pickle_yield_change_list_3d)):\n",
    "    mark = False\n",
    "    for j in range(len(pickle_yield_change_list_3d[i])):\n",
    "      for key in matrix_keys:\n",
    "        #print(key, pickle_yield_change_list_3d[i][j][key], threshold)\n",
    "        if abs(pickle_yield_change_list_3d[i][j][key]) > threshold:\n",
    "          mark = True\n",
    "          threshold_yield_change_list[i][j][key] = 1\n",
    "          high_counter += 1\n",
    "          change_counter += 1\n",
    "        else:\n",
    "          if abs(pickle_yield_change_list_3d[i][j][key]) > 1e-8:\n",
    "            threshold_yield_change_list[i][j][key] = 0\n",
    "            change_counter += 1\n",
    "          else:\n",
    "            threshold_yield_change_list[i][j][key] = 0\n",
    "            low_counter += 1\n",
    "\n",
    "    if mark:\n",
    "      coeff_counter += 1\n",
    "    else:\n",
    "      negligible_counter += 1\n",
    "\n",
    "  pct_threshold = (high_counter/change_counter)*100\n",
    "\n",
    "  # create bipartite graph\n",
    "  yield_change_graph = []\n",
    "  current_coeff_graph = []\n",
    "  for i in range(len(threshold_yield_change_list)):\n",
    "    current_coeff_graph = []\n",
    "    for j in range(len(threshold_yield_change_list[i])):\n",
    "      for key in matrix_keys:\n",
    "        current_coeff_graph.append(threshold_yield_change_list[i][j][key])\n",
    "    yield_change_graph.append(current_coeff_graph)\n",
    "\n",
    "  return yield_change_graph, matrix_keys\n",
    "\n",
    "def make_individual_groups(yield_change_graph):\n",
    "  start_threshold_list = [row[:] for row in yield_change_graph]\n",
    "\n",
    "  individual_coeffs = []\n",
    "  current_group = 0\n",
    "\n",
    "  # while True:\n",
    "  #   i = 0\n",
    "  first_coeff = 0\n",
    "  for i in range(len(start_threshold_list)):\n",
    "    mark = False\n",
    "    for j in range(len(start_threshold_list[i])):\n",
    "      if start_threshold_list[i][j] == 1:\n",
    "        mark = True\n",
    "          \n",
    "        break\n",
    "    if mark == False:\n",
    "      start_threshold_list[i] = []\n",
    "\n",
    "  for i in range(len(start_threshold_list)):\n",
    "    current_coeffs = []\n",
    "    if len(start_threshold_list[i]) > 0:\n",
    "      first_coeff = i\n",
    "      individual_coeffs.append([i])\n",
    "      # find coefficients to check\n",
    "      for j in range(len(start_threshold_list[i])):\n",
    "        if start_threshold_list[i][j] == 1:\n",
    "          current_coeffs.append(j)\n",
    "\n",
    "      start_threshold_list[i] = [] # set so it is not rechecked\n",
    "\n",
    "      for k in range(len(start_threshold_list)):\n",
    "        conflicts = 0\n",
    "        if len(start_threshold_list[k]) > 0:\n",
    "          for m in range(len(start_threshold_list[k])):\n",
    "            if start_threshold_list[k][m] != 0:\n",
    "              if m in current_coeffs:\n",
    "                conflicts = 1\n",
    "                break\n",
    "              else:\n",
    "                current_coeffs.append(m)\n",
    "          if conflicts == 0:\n",
    "            individual_coeffs[current_group].append(k)\n",
    "\n",
    "            start_threshold_list[k] = []\n",
    "      current_group += 1\n",
    "  return individual_coeffs\n",
    "\n",
    "def make_individual_map(individual_coeffs, yield_change_graph):\n",
    "  individual_coeff_map = [row[:] for row in individual_coeffs]\n",
    "  for i in range(len(individual_coeff_map)):\n",
    "    for j in range(len(individual_coeff_map[i])):\n",
    "      coeff = individual_coeff_map[i][j]\n",
    "      yield_changes = yield_change_graph[coeff]\n",
    "      current_changes = []\n",
    "      for k in range(len(yield_changes)):\n",
    "        if yield_changes[k] == 1:\n",
    "          current_changes.append(k)\n",
    "      individual_coeff_map[i][j] = current_changes\n",
    "\n",
    "  return individual_coeff_map\n",
    "\n",
    "\n",
    "#new_prod_coeffs = copy_prod()\n",
    "#map = make_map()\n",
    "#yield_change_graph, matrix_keys = make_graph()\n",
    "#individual_coeffs = make_individual_groups()\n",
    "#individual_coeff_map = make_individual_map()\n",
    "\n",
    "#new_prod_coeffs = copy_prod()\n",
    "#avg_score, score_matrix = evaluate(new_prod_coeffs)\n",
    "#print(\"Initial Score:\", avg_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F4ZzH5fqdm5q",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1765566469193,
     "user": {
      "displayName": "Forwood Cloud Wiser",
      "userId": "16746988716585544349"
     },
     "user_tz": 300
    },
    "id": "F4ZzH5fqdm5q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Qgg5MpcPBPGU",
   "metadata": {
    "id": "Qgg5MpcPBPGU"
   },
   "source": [
    "New Run Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcE284L0mcEN",
   "metadata": {
    "id": "bcE284L0mcEN"
   },
   "source": [
    "Imports:\n",
    "\n",
    "caltech_amore_isoprene_full_eqn.txt\n",
    "\n",
    "caltech_amore_isoprene_full_spc.txt\n",
    "\n",
    "optimizer.py\n",
    "\n",
    "reduced_mech.txt (rename reduced mech import to this)\n",
    "\n",
    "setup.py\n",
    "\n",
    "variables.py.\n",
    "\n",
    "Delete the blocks of text containing J22, J34, J41, and TROE2 in the mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GVGZAzCJSiD6",
   "metadata": {
    "id": "GVGZAzCJSiD6"
   },
   "source": [
    "Min coeffs and max coeffs set to 1 will be +-50% of original value, set to 0 will be no range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alXk-v7nU5dg",
   "metadata": {
    "id": "alXk-v7nU5dg"
   },
   "source": [
    "#Isop Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T8oeAonKUstb",
   "metadata": {
    "id": "T8oeAonKUstb"
   },
   "source": [
    "Isoprene Specific Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "YgivlDi7EzlG",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1765566470283,
     "user": {
      "displayName": "Forwood Cloud Wiser",
      "userId": "16746988716585544349"
     },
     "user_tz": 300
    },
    "id": "YgivlDi7EzlG"
   },
   "outputs": [],
   "source": [
    "input_conditions = [{},{},{},{},{},{}]\n",
    "\n",
    "#base\n",
    "input_conditions[0]['OH'] = 0.0002\n",
    "input_conditions[0]['HO2'] = 7.00E-03\n",
    "input_conditions[0]['NO'] = 0.08\n",
    "input_conditions[0]['O3'] = 20\n",
    "input_conditions[0]['NO3'] = 0.007\n",
    "input_conditions[0]['CH3OO'] = 0.001\n",
    "input_conditions[0]['CH3CO3'] = 0.001\n",
    "input_conditions[0]['O2'] = 210000000\n",
    "input_conditions[0]['sza'] = 0\n",
    "input_conditions[0]['sun'] = 1\n",
    "\n",
    "#low nox\n",
    "input_conditions[1]['OH'] = 0.0002\n",
    "input_conditions[1]['HO2'] = 7.00E-03\n",
    "input_conditions[1]['NO'] = 0.02\n",
    "input_conditions[1]['O3'] = 0\n",
    "input_conditions[1]['NO3'] = 0.000\n",
    "input_conditions[1]['CH3OO'] = 0.000\n",
    "input_conditions[1]['CH3CO3'] = 0.000\n",
    "input_conditions[1]['O2'] = 210000000\n",
    "input_conditions[1]['sza'] = 0\n",
    "input_conditions[1]['sun'] = 1\n",
    "\n",
    "#high nox\n",
    "input_conditions[2]['OH'] = 0.0002\n",
    "input_conditions[2]['HO2'] = 7.00E-03\n",
    "input_conditions[2]['NO'] = 0.2\n",
    "input_conditions[2]['O3'] = 0\n",
    "input_conditions[2]['NO3'] = 0.000\n",
    "input_conditions[2]['CH3OO'] = 0.000\n",
    "input_conditions[2]['CH3CO3'] = 0.000\n",
    "input_conditions[2]['O2'] = 210000000\n",
    "input_conditions[2]['sza'] = 0\n",
    "input_conditions[2]['sun'] = 1\n",
    "\n",
    "#high o3\n",
    "input_conditions[3]['OH'] = 0.0001\n",
    "input_conditions[3]['HO2'] = 7.00E-03\n",
    "input_conditions[3]['NO'] = 0.03\n",
    "input_conditions[3]['O3'] = 80\n",
    "input_conditions[3]['NO3'] = 0.000\n",
    "input_conditions[3]['CH3OO'] = 0.001\n",
    "input_conditions[3]['CH3CO3'] = 0.001\n",
    "input_conditions[3]['O2'] = 210000000\n",
    "input_conditions[3]['sza'] = 0\n",
    "input_conditions[3]['sun'] = 1\n",
    "\n",
    "#high no3 low hv\n",
    "input_conditions[4]['OH'] = 0.00005\n",
    "input_conditions[4]['HO2'] = 3.00E-03\n",
    "input_conditions[4]['NO'] = 0.08\n",
    "input_conditions[4]['O3'] = 0\n",
    "input_conditions[4]['NO3'] = 0.007\n",
    "input_conditions[4]['CH3OO'] = 0.001\n",
    "input_conditions[4]['CH3CO3'] = 0.001\n",
    "input_conditions[4]['O2'] = 210000000\n",
    "input_conditions[4]['sza'] = 90\n",
    "input_conditions[4]['sun'] = 0\n",
    "\n",
    "# low hv\n",
    "input_conditions[5]['OH'] = 0.00005\n",
    "input_conditions[5]['HO2'] = 3.00E-03\n",
    "input_conditions[5]['NO'] = 0.08\n",
    "input_conditions[5]['O3'] = 20\n",
    "input_conditions[5]['NO3'] = 0.000\n",
    "input_conditions[5]['CH3OO'] = 0.001\n",
    "input_conditions[5]['CH3CO3'] = 0.001\n",
    "input_conditions[5]['O2'] = 210000000\n",
    "input_conditions[5]['sza'] = 90\n",
    "input_conditions[5]['sun'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "M49DdmrcFw14",
   "metadata": {
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1765566470366,
     "user": {
      "displayName": "Forwood Cloud Wiser",
      "userId": "16746988716585544349"
     },
     "user_tz": 300
    },
    "id": "M49DdmrcFw14"
   },
   "outputs": [],
   "source": [
    "# reference mechanism inputs\n",
    "eq_list = read_eqns(full_eqn)\n",
    "species_list_names = read_spc(full_spc)\n",
    "# eq_list = read_eqns('./caltech_amore_isoprene_full_eqn.txt')\n",
    "# species_list_names = read_spc('./caltech_amore_isoprene_full_spc.txt')\n",
    "species_list_names.append('MGLYOX')\n",
    "species_list_names = list(set(species_list_names))\n",
    "species_list = list(range(len(species_list_names)))\n",
    "spec_len = len(species_list)\n",
    "rate_list = [i[1] for i in eq_list]\n",
    "rate_list_clean = [i[1] for i in eq_list]\n",
    "\n",
    "#background_rxns\n",
    "background_conc = [0.0002,\n",
    "0.015,\n",
    "0.5,\n",
    "0.001,\n",
    "0.007,\n",
    "20,\n",
    "0.0001,\n",
    "0.0001,\n",
    "0.0001,\n",
    "0.0001,\n",
    "0.0001,\n",
    "0.0001,\n",
    "0.0001,\n",
    "0.0001,\n",
    "0.0001,\n",
    "0.0001,\n",
    "  210000000,\n",
    "4.0727831285287275e-29,\n",
    "1e-5,\n",
    "1e-5,\n",
    "1e-5,\n",
    "1e-5,\n",
    "1e-5,\n",
    "4.0727831285287275e-29,\n",
    "4.0727831285287275e-29,\n",
    "4.0727831285287275e-29,\n",
    "400,\n",
    "9,\n",
    "210000000,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "0]\n",
    "\n",
    "#background_spc\n",
    "#a list of background species to be excluded from our reaction network for visual clarity (and also because they will mess with the drg reduction method)\n",
    "background_spc_0 = ['OH','NO','NO2','NO3','HO2','O3','CH3OO','CO2','CO','O2','N2O5','H2O','H2O2','H2Od','SO2',\n",
    "                  'H2S','HF','H2S','N2H4','HN3','HI','HBr',\n",
    "                  'HCl','HCN','H2Se','H2Te','NH2OH','HBrO','HClO','H3PO2','HPO3',\n",
    "                  'H2O3','OF2','O2F2','NOHSO4','COS','N2F4','N2O4','N2O3','HNO3',\n",
    "                  'HNO2','N2O','NF5','NI3','H2SO4','CS2','H2CO3','H2SO3','SO2Cl2','S4N4','H2SO5',\n",
    "                  'H2S2O7','S2F10','H3NO3S','Br2S','SF6','SF4', 'CH3CO3']\n",
    "background_spc = []\n",
    "background_spc_n = []\n",
    "for i in background_spc_0:\n",
    "    if i in species_list_names:\n",
    "        background_spc_n.append(i)\n",
    "        background_spc.append(species_list_names.index(i))\n",
    "\n",
    "#background concentrations\n",
    "background_dict = {}\n",
    "for i in range(len(background_spc_n)):\n",
    "    if i<len(background_conc):\n",
    "        background_dict[background_spc_n[i]] = background_conc[i]\n",
    "    else:\n",
    "        background_dict[background_spc_n[i]] = 0\n",
    "\n",
    "\n",
    "\n",
    "reac_list_n,reac_coeff_list,prod_list_n,prod_coeff_list = get_prod_reac(eq_list)\n",
    "reac_len = len(reac_list_n)\n",
    "\n",
    "\n",
    "# atmospheric inputs\n",
    "atm_cond = {}\n",
    "temp = 292\n",
    "pressure = 1000\n",
    "solar_zenith = 0\n",
    "solar_factor = 1\n",
    "atm_cond['temp'] = temp\n",
    "atm_cond['pres'] = pressure\n",
    "atm_cond['sza'] = solar_zenith\n",
    "atm_cond['sun'] = solar_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0498877e-3b4d-44ea-a71f-889dd84c5d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./AMORE_v2_isoprene_42_sp.txt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cY1CE6NNGEwI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1765566470382,
     "user": {
      "displayName": "Forwood Cloud Wiser",
      "userId": "16746988716585544349"
     },
     "user_tz": 300
    },
    "id": "cY1CE6NNGEwI",
    "outputId": "955155d1-f14a-4a31-b4c4-e19f88377e81"
   },
   "outputs": [],
   "source": [
    "CFACTOR = 2.5e+19;\n",
    "RO2 = 0.01*10e-9;\n",
    "SUN = 0.5;\n",
    "TEMP = 295;\n",
    "M = pressure_to_m(1000,295);\n",
    "sza = 0;\n",
    "\n",
    "background_dict_2 = deepcopy(background_dict)\n",
    "\n",
    "#reactions\n",
    "rxn_dict = {}\n",
    "rxn_dict['reac'] = reac_list_n\n",
    "rxn_dict['reac_coeff'] = reac_coeff_list\n",
    "rxn_dict['prod'] = prod_list_n\n",
    "rxn_dict['prod_coeff'] = prod_coeff_list\n",
    "rxn_dict['k'] = rate_list\n",
    "rxn_dict['rates'] = rate_list_clean\n",
    "\n",
    "#second reactants\n",
    "second_reactants = ['MVK3OOH4N',\n",
    "'ISOP1OH2N3CO4N',\n",
    "'MVK3N4OH4OH',\n",
    "'ISOP1OH2OH3N4N',\n",
    "'ISOP1OOH2OOH3CO4CO',\n",
    "'ISOP1N2OH3OH4N',\n",
    "'ISOP1N2OH3OOH4CO',\n",
    "'ISOP1N2OOH34O4OH',\n",
    "'MVK3OH3OOH',\n",
    "'ISOP1OH12O3OOH4N',\n",
    "'ISOP1OOH2OH3N4N',\n",
    "'ISOP1CO2OOH3OH4CO',\n",
    "'ISOP3N4N',\n",
    "'MVK3N4N',\n",
    "'ISOP1OH4CO4OH',\n",
    "'ISOP1OH23O4N',\n",
    "'ISOP1N2N3OH4OH',\n",
    "'ISOP1CO1OOH4OH',\n",
    "'HMML',\n",
    "'ISOP1OH4OOt',\n",
    "'MACR2OH3OOH',\n",
    "'ISOP1OOH2OOH3OH4N',\n",
    "'MVK3OH4OOH',\n",
    "'MACR3OH3OOH',\n",
    "'ISOP1CO23O4OOH',\n",
    "'ISOP1OOH2OH3OH4N',\n",
    "'ISOP12O3CO4N',\n",
    "'ISOP1OH4OOc',\n",
    "'ISOP1OH2OOH3N4N',\n",
    "'ISOP1CO1OH4N',\n",
    "'ISOP1CO2N3OH4N',\n",
    "'ISOP12O3OH4OOH',\n",
    "'ISOP1OOH2OH3OOH4OH',\n",
    "'ISOP1CO2N3OH4OH',\n",
    "'ISOP1OH2N3OOH4CO',\n",
    "'ISOP1N2OH3CO4OOH',\n",
    "'ISOP1CO2OOH34O4OH',\n",
    "'MACR4N',\n",
    "'ISOP1N2OH3OOH4OOH',\n",
    "'ISOP1OH2OH3OOH4N',\n",
    "'PYRAC',\n",
    "'ISOP1OOH2OH3CO4N',\n",
    "'ISOP1OOH2N3OH4N',\n",
    "'ISOP1OH2N3OOH4N',\n",
    "'ISOP1N2OH3N4OOH',\n",
    "'ISOP1OOH2OOH3N4CO',\n",
    "'HPETHNL',\n",
    "'ISOP1OH4CO4OOH',\n",
    "'ISOP1OH2OOH3OH4OO',\n",
    "'ISOP1OH2OH3N4CO',\n",
    "'ISOP1N2OH3OH4OOH',\n",
    "'ISOP1OH2OOH3CO4CO',\n",
    "'ETHLN',\n",
    "'ISOP1OH2OO',\n",
    "'ISOP1OH2OOH3OH4CO',\n",
    "'ISOP1N2N3OH4OOH',\n",
    "'ISOP1OO4OHc',\n",
    "'ISOP1OO4N',\n",
    "'ISOP1N4CO4OH',\n",
    "'ISOP1CO2OOH3N4OOH',\n",
    "'ISOP1N2OH3OOH4N',\n",
    "'ISOP1N2OH3OOH4OH',\n",
    "'ISOP1N2OOH3OOH4OH',\n",
    "'ISOP1OH12O3OH4OOH',\n",
    "'ISOP1N2N3CO4OH',\n",
    "'ISOP1CO2OH34O',\n",
    "'ISOP1OH4OH',\n",
    "'ISOP1CO2OH3OOH4OH',\n",
    "'ISOP1OH2N3OH4N',\n",
    "'ISOP1N4CO4OOH',\n",
    "'ISOP1OOH23O4CO',\n",
    "'PROPNN',\n",
    "'ISOP1N4N',\n",
    "'ISOP1CO2OOH3OOH4OOH',\n",
    "'ISOP1OH2N3OH4OOH',\n",
    "'ISOP1N2OOH3OH4N',\n",
    "'ISOP1OOH2OOH3OOH4CO',\n",
    "'ISOP1OH23O4CO',\n",
    "'ISOP1CO1OOH4N',\n",
    "'MACR2N3N',\n",
    "'ISOP1CO2OOH3N4OH',\n",
    "'ISOP1N2OH3N4OH',\n",
    "'ISOP1OH2OOH3N4CO',\n",
    "'ISOP1N2OO',\n",
    "'ISOP1N2OOH3N4OH',\n",
    "'MACR1OH',\n",
    "'ISOP1OOH2OH34O',\n",
    "'ISOP1OOH2OH34O4OH',\n",
    "'ISOP1N4O',\n",
    "'ISOP1N2OOH3OOH4CO',\n",
    "'ISOP1CO1OH4OH',\n",
    "'ISOP1OOH2OH3CO4CO',\n",
    "'ISOP1OH2OO3CO4OH',\n",
    "'ISOP1CO2N3OH4OOH',\n",
    "'ISOP1CO2OOH3OOH4N',\n",
    "'ISOP1N2N',\n",
    "'ISOP1OH2OOH3OOH4OH',\n",
    "'ISOP1OH2OOH3CO4N',\n",
    "'ISOP1OH2N3CO4OH',\n",
    "'ISOP1OH12O3OOH4CO',\n",
    "'MACR2OOH3N',\n",
    "'ISOP1OOH2OH3CO4OH',\n",
    "'ISOP1OOH2N3OOH4CO',\n",
    "'MACR2OH3N',\n",
    "'ISOP1CO2OOH3OH4N',\n",
    "'ISOP1OOH2OH3N4OH',\n",
    "'ISOP1CO2OH3OOH4CO',\n",
    "'MVK3OOH4CO4OOH',\n",
    "'HOCH2COCHO',\n",
    "'ISOP12O3OH4N',\n",
    "'MVK3OOH4OH4OH',\n",
    "'ISOP1OOH23O3OH4N',\n",
    "'ISOP1OH2OOH3OH4OOH',\n",
    "'ISOP1OO4OHt',\n",
    "'ISOP1N4OO',\n",
    "'ISOP1N2OH3OO4OH',\n",
    "'ISOP3OH4OH',\n",
    "'ISOP1N2OO3OH4OH',\n",
    "'ISOP1N2OOH3OH4CO',\n",
    "'ISOP1CO2OH3OOH4N',\n",
    "'ISOP1CO23O4OH',\n",
    "'ISOP1OH2N3OOH4OH',\n",
    "'ISOP1N2OOH3OH4OH',\n",
    "'ISOP1OH2OOH3OOH4CO',\n",
    "'MACR2N3OH',\n",
    "'ISOP1N2OOH3OH4OOH',\n",
    "'ISOP1CO2OOH3OOH4OH',\n",
    "'ISOP1CO3OH4OH',\n",
    "'ISOP1OH2OOH3OOH4N',\n",
    "'ISOP1CO4CO',\n",
    "'ISOP1N2OH3N4CO',\n",
    "'ISOP1N2OH34O',\n",
    "'ISOP1OH12O3OOH4OH',\n",
    "'ISOP12O3CO4OH',\n",
    "'ISOP1OOH2OH3OOH4N',\n",
    "'ISOP1OH3OH4CO',\n",
    "'HPAC',\n",
    "'ISO1OH12O',\n",
    "'ISOP3OO4N',\n",
    "'ISOP1OH2OO3OH4OOH',\n",
    "'ISOP1OH2OH3OOH4CO',\n",
    "'ISOP1N2N3OOH4OH',\n",
    "'ISOP3OO4OH',\n",
    "'MACR2OOH3OH',\n",
    "'ISOP1OOH2OH3N4CO',\n",
    "'ISOP1OH2OOH34O4OH',\n",
    "'ISOP1CO23O4N',\n",
    "'ISOP1CO2N3OOH4OH',\n",
    "'ISOP1OH2OH',\n",
    "'ISOP1CO2N3OOH4OOH',\n",
    "'ISOP1N23O4CO',\n",
    "'ISOP1CO2OOH3OH4OH',\n",
    "'ISOP1CO2OOH3OOH4CO',\n",
    "'MACR2OOH3CO3OOH',\n",
    "'ISOP1OH2OOH3N4OH',\n",
    "'ISOP1OH2N3N4OH',\n",
    "'ISOP12O3OH4CO',\n",
    "'ISOP1OH2OOH3CO4OH',\n",
    "'ISOP1N2OH3CO4N',\n",
    "'HCOOH',\n",
    "'ISOP1OH2OOH3OH4N']\n",
    "\n",
    "scnd_dict = {}\n",
    "for i in second_reactants:\n",
    "    scnd_dict[i] = 0\n",
    "\n",
    "# algorithm settings\n",
    "settings = {}\n",
    "settings['root'] = 'ISOP'\n",
    "settings['yield cutoff'] = 0.1\n",
    "settings['rate cutoff'] = 0\n",
    "settings['protected'] = ['ISOP','ISOP1OH23O4OHt','ISOP1OH23O4OHc', 'ISOP1OH2OH34O', 'MVK','HCHO', 'HAC','PAN','GLYC','PYRAC','MGLY','HCOOH','MACR','MPAN','GLYX']\n",
    "#settings['']\n",
    "settings['# of species desired'] = 200\n",
    "\n",
    "background_dict_2['OH'] = 0.0002\n",
    "background_dict_2['HO2'] = 7.00E-03\n",
    "background_dict_2['NO'] = 0.08\n",
    "background_dict_2['O3'] = 20\n",
    "background_dict_2['NO3'] = 0.007\n",
    "background_dict_2['CH3OO'] = 0.001\n",
    "background_dict_2['CH3CO3'] = 0.001\n",
    "background_dict_2['O2'] = 210000000\n",
    "atm_cond['sza'] = 0\n",
    "atm_cond['sun'] = 1\n",
    "\n",
    "\"\"\"Set original dictionary in same format as test dictionary\"\"\"\n",
    "\n",
    "inputs_ref = {'spc':species_list_names, 'rxn':rxn_dict, 'bck':background_dict_2, 'scnd':scnd_dict, 'atm cond':atm_cond,'settings':settings}\n",
    "\n",
    "\n",
    "# inputs required: compare_species_r, compare_species, reference_data, input_conditions\n",
    "# inputs_test = {'spc':species_list_names_r, 'rxn':rxn_dict, 'bck':background_dict_2, 'scnd':scnd_dict, 'atm cond':atm_cond,'settings':settings}\n",
    "\n",
    "# inputs: conditions, background dict, test, compare_species, compare_species_r, species_list_names_r, reference_data, spec_factor,\n",
    "\n",
    "\"\"\"PROD 1: Set new reaction dictionary with reduced mechanism and test coefficients\"\"\"\n",
    "\n",
    "# reduced_mechanism inputs\n",
    "\n",
    "\n",
    "reduced_mech_file = open(config['mech_file'], \"r\")\n",
    "#reduced_mech_file = open(mech_file, \"r\")\n",
    "red_mech = reduced_mech_file.read()\n",
    "\n",
    "species_list_names_r, reac_list_n_r,reac_coeff_list_r,prod_list_n_r,prod_coeff_list_r,rate_list_r, rates_eval_r, rates_2 = f0am_to_python(red_mech)\n",
    "\n",
    "\n",
    "species_list_names_r = list(set(species_list_names_r))\n",
    "species_list_r = list(range(len(species_list_names_r)))\n",
    "spec_len_r = len(species_list_r)\n",
    "\n",
    "\n",
    "#background_rxns\n",
    "background_conc = [0.0002,\n",
    "\n",
    "0.015,\n",
    "0.5,\n",
    "0.001,\n",
    "0.007,\n",
    "20,\n",
    "0.0001,\n",
    "0.0001,\n",
    "0.0001,\n",
    "0.0001,\n",
    "0.0001,\n",
    "0.0001,\n",
    "0.0001,\n",
    "0.0001,\n",
    "0.0001,\n",
    "0.0001,\n",
    "  210000000,\n",
    "4.0727831285287275e-29,\n",
    "1e-5,\n",
    "1e-5,\n",
    "1e-5,\n",
    "1e-5,\n",
    "1e-5,\n",
    "4.0727831285287275e-29,\n",
    "4.0727831285287275e-29,\n",
    "4.0727831285287275e-29,\n",
    "400,\n",
    "9,\n",
    "210000000,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "0]\n",
    "\n",
    "#background_spc\n",
    "#a list of background species to be excluded from our reaction network for visual clarity (and also because they will mess with the drg reduction method)\n",
    "background_spc_0 = ['OH','NO','NO2','NO3','HO2','O3','CH3OO','CO2','CO','O2','N2O5','H2O','H2O2','H2Od','SO2',\n",
    "                  'H2S','HF','H2S','N2H4','HN3','HI','HBr',\n",
    "                  'HCl','HCN','H2Se','H2Te','NH2OH','HBrO','HClO','H3PO2','HPO3',\n",
    "                  'H2O3','OF2','O2F2','NOHSO4','COS','N2F4','N2O4','N2O3','HNO3',\n",
    "                  'HNO2','N2O','NF5','NI3','H2SO4','CS2','H2CO3','H2SO3','SO2Cl2','S4N4','H2SO5',\n",
    "                  'H2S2O7','S2F10','H3NO3S','Br2S','SF6','SF4', 'CH3CO3']\n",
    "background_spc = []\n",
    "background_spc_n = []\n",
    "for i in background_spc_0:\n",
    "    if i in species_list_names_r:\n",
    "        background_spc_n.append(i)\n",
    "        background_spc.append(species_list_names_r.index(i))\n",
    "\n",
    "\n",
    "\n",
    "#background concentrations\n",
    "background_dict = {}\n",
    "for i in range(len(background_spc_n)):\n",
    "    if i<len(background_conc):\n",
    "        background_dict[background_spc_n[i]] = background_conc[i]\n",
    "    else:\n",
    "        background_dict[background_spc_n[i]] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reac_len = len(reac_list_n_r)\n",
    "\n",
    "\n",
    "# atmospheric inputs\n",
    "atm_cond = {}\n",
    "temp = 292\n",
    "pressure = 1000\n",
    "solar_zenith = 0\n",
    "solar_factor = 1\n",
    "atm_cond['temp'] = temp\n",
    "atm_cond['pres'] = pressure\n",
    "atm_cond['sza'] = solar_zenith\n",
    "atm_cond['sun'] = solar_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0629831a-f43c-449f-9fa3-dc65d3a25e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "XhBRw86iGRod",
   "metadata": {
    "executionInfo": {
     "elapsed": 666,
     "status": "ok",
     "timestamp": 1765566471049,
     "user": {
      "displayName": "Forwood Cloud Wiser",
      "userId": "16746988716585544349"
     },
     "user_tz": 300
    },
    "id": "XhBRw86iGRod"
   },
   "outputs": [],
   "source": [
    "CFACTOR = 2.5e+19;\n",
    "RO2 = 0.01*10e-9;\n",
    "SUN = 0.5;\n",
    "TEMP = 295;\n",
    "M = pressure_to_m(1000,295);\n",
    "sza = 0;\n",
    "\n",
    "background_dict_2 = deepcopy(background_dict)\n",
    "\n",
    "#reactions\n",
    "rxn_dict = {}\n",
    "rxn_dict['reac'] = reac_list_n_r\n",
    "rxn_dict['reac_coeff'] = reac_coeff_list_r\n",
    "rxn_dict['prod'] = prod_list_n_r\n",
    "rxn_dict['prod_coeff'] = prod_coeff_list_r #set new coefficients in reaction dictionary\n",
    "rxn_dict['k'] = rate_list_r\n",
    "rxn_dict['rates'] = rate_list_r\n",
    "\n",
    "#second reactants\n",
    "second_reactants = ['MVK3OOH4N',\n",
    "'ISOP1OH2N3CO4N',\n",
    "'MVK3N4OH4OH',\n",
    "'ISOP1OH2OH3N4N',\n",
    "'ISOP1OOH2OOH3CO4CO',\n",
    "'ISOP1N2OH3OH4N',\n",
    "'ISOP1N2OH3OOH4CO',\n",
    "'ISOP1N2OOH34O4OH',\n",
    "'MVK3OH3OOH',\n",
    "'ISOP1OH12O3OOH4N',\n",
    "'ISOP1OOH2OH3N4N',\n",
    "'ISOP1CO2OOH3OH4CO',\n",
    "'ISOP3N4N',\n",
    "'MVK3N4N',\n",
    "'ISOP1OH4CO4OH',\n",
    "'ISOP1OH23O4N',\n",
    "'ISOP1N2N3OH4OH',\n",
    "'ISOP1CO1OOH4OH',\n",
    "'HMML',\n",
    "'ISOP1OH4OOt',\n",
    "'MACR2OH3OOH',\n",
    "'ISOP1OOH2OOH3OH4N',\n",
    "'MVK3OH4OOH',\n",
    "'MACR3OH3OOH',\n",
    "'ISOP1CO23O4OOH',\n",
    "'ISOP1OOH2OH3OH4N',\n",
    "'ISOP12O3CO4N',\n",
    "'ISOP1OH4OOc',\n",
    "'ISOP1OH2OOH3N4N',\n",
    "'ISOP1CO1OH4N',\n",
    "'ISOP1CO2N3OH4N',\n",
    "'ISOP12O3OH4OOH',\n",
    "'ISOP1OOH2OH3OOH4OH',\n",
    "'ISOP1CO2N3OH4OH',\n",
    "'ISOP1OH2N3OOH4CO',\n",
    "'ISOP1N2OH3CO4OOH',\n",
    "'ISOP1CO2OOH34O4OH',\n",
    "'MACR4N',\n",
    "'ISOP1N2OH3OOH4OOH',\n",
    "'ISOP1OH2OH3OOH4N',\n",
    "'PYRAC',\n",
    "'ISOP1OOH2OH3CO4N',\n",
    "'ISOP1OOH2N3OH4N',\n",
    "'ISOP1OH2N3OOH4N',\n",
    "'ISOP1N2OH3N4OOH',\n",
    "'ISOP1OOH2OOH3N4CO',\n",
    "'HPETHNL',\n",
    "'ISOP1OH4CO4OOH',\n",
    "'ISOP1OH2OOH3OH4OO',\n",
    "'ISOP1OH2OH3N4CO',\n",
    "'ISOP1N2OH3OH4OOH',\n",
    "'ISOP1OH2OOH3CO4CO',\n",
    "'ETHLN',\n",
    "'ISOP1OH2OO',\n",
    "'ISOP1OH2OOH3OH4CO',\n",
    "'ISOP1N2N3OH4OOH',\n",
    "'ISOP1OO4OHc',\n",
    "'ISOP1OO4N',\n",
    "'ISOP1N4CO4OH',\n",
    "'ISOP1CO2OOH3N4OOH',\n",
    "'ISOP1N2OH3OOH4N',\n",
    "'ISOP1N2OH3OOH4OH',\n",
    "'ISOP1N2OOH3OOH4OH',\n",
    "'ISOP1OH12O3OH4OOH',\n",
    "'ISOP1N2N3CO4OH',\n",
    "'ISOP1CO2OH34O',\n",
    "'ISOP1OH4OH',\n",
    "'ISOP1CO2OH3OOH4OH',\n",
    "'ISOP1OH2N3OH4N',\n",
    "'ISOP1N4CO4OOH',\n",
    "'ISOP1OOH23O4CO',\n",
    "'PROPNN',\n",
    "'ISOP1N4N',\n",
    "'ISOP1CO2OOH3OOH4OOH',\n",
    "'ISOP1OH2N3OH4OOH',\n",
    "'ISOP1N2OOH3OH4N',\n",
    "'ISOP1OOH2OOH3OOH4CO',\n",
    "'ISOP1OH23O4CO',\n",
    "'ISOP1CO1OOH4N',\n",
    "'MACR2N3N',\n",
    "'ISOP1CO2OOH3N4OH',\n",
    "'ISOP1N2OH3N4OH',\n",
    "'ISOP1OH2OOH3N4CO',\n",
    "'ISOP1N2OO',\n",
    "'ISOP1N2OOH3N4OH',\n",
    "'MACR1OH',\n",
    "'ISOP1OOH2OH34O',\n",
    "'ISOP1OOH2OH34O4OH',\n",
    "'ISOP1N4O',\n",
    "'ISOP1N2OOH3OOH4CO',\n",
    "'ISOP1CO1OH4OH',\n",
    "'ISOP1OOH2OH3CO4CO',\n",
    "'ISOP1OH2OO3CO4OH',\n",
    "'ISOP1CO2N3OH4OOH',\n",
    "'ISOP1CO2OOH3OOH4N',\n",
    "'ISOP1N2N',\n",
    "'ISOP1OH2OOH3OOH4OH',\n",
    "'ISOP1OH2OOH3CO4N',\n",
    "'ISOP1OH2N3CO4OH',\n",
    "'ISOP1OH12O3OOH4CO',\n",
    "'MACR2OOH3N',\n",
    "'ISOP1OOH2OH3CO4OH',\n",
    "'ISOP1OOH2N3OOH4CO',\n",
    "'MACR2OH3N',\n",
    "'ISOP1CO2OOH3OH4N',\n",
    "'ISOP1OOH2OH3N4OH',\n",
    "'ISOP1CO2OH3OOH4CO',\n",
    "'MVK3OOH4CO4OOH',\n",
    "'HOCH2COCHO',\n",
    "'ISOP12O3OH4N',\n",
    "'MVK3OOH4OH4OH',\n",
    "'ISOP1OOH23O3OH4N',\n",
    "'ISOP1OH2OOH3OH4OOH',\n",
    "'ISOP1OO4OHt',\n",
    "'ISOP1N4OO',\n",
    "'ISOP1N2OH3OO4OH',\n",
    "'ISOP3OH4OH',\n",
    "'ISOP1N2OO3OH4OH',\n",
    "'ISOP1N2OOH3OH4CO',\n",
    "'ISOP1CO2OH3OOH4N',\n",
    "'ISOP1CO23O4OH',\n",
    "'ISOP1OH2N3OOH4OH',\n",
    "'ISOP1N2OOH3OH4OH',\n",
    "'ISOP1OH2OOH3OOH4CO',\n",
    "'MACR2N3OH',\n",
    "'ISOP1N2OOH3OH4OOH',\n",
    "'ISOP1CO2OOH3OOH4OH',\n",
    "'ISOP1CO3OH4OH',\n",
    "'ISOP1OH2OOH3OOH4N',\n",
    "'ISOP1CO4CO',\n",
    "'ISOP1N2OH3N4CO',\n",
    "'ISOP1N2OH34O',\n",
    "'ISOP1OH12O3OOH4OH',\n",
    "'ISOP12O3CO4OH',\n",
    "'ISOP1OOH2OH3OOH4N',\n",
    "'ISOP1OH3OH4CO',\n",
    "'HPAC',\n",
    "'ISO1OH12O',\n",
    "'ISOP3OO4N',\n",
    "'ISOP1OH2OO3OH4OOH',\n",
    "'ISOP1OH2OH3OOH4CO',\n",
    "'ISOP1N2N3OOH4OH',\n",
    "'ISOP3OO4OH',\n",
    "'MACR2OOH3OH',\n",
    "'ISOP1OOH2OH3N4CO',\n",
    "'ISOP1OH2OOH34O4OH',\n",
    "'ISOP1CO23O4N',\n",
    "'ISOP1CO2N3OOH4OH',\n",
    "'ISOP1OH2OH',\n",
    "'ISOP1CO2N3OOH4OOH',\n",
    "'ISOP1N23O4CO',\n",
    "'ISOP1CO2OOH3OH4OH',\n",
    "'ISOP1CO2OOH3OOH4CO',\n",
    "'MACR2OOH3CO3OOH',\n",
    "'ISOP1OH2OOH3N4OH',\n",
    "'ISOP1OH2N3N4OH',\n",
    "'ISOP12O3OH4CO',\n",
    "'ISOP1OH2OOH3CO4OH',\n",
    "'ISOP1N2OH3CO4N',\n",
    "'HCOOH',\n",
    "'ISOP1OH2OOH3OH4N']\n",
    "\n",
    "scnd_dict = {}\n",
    "for i in second_reactants:\n",
    "    scnd_dict[i] = 0\n",
    "\n",
    "\n",
    "# algorithm settings\n",
    "settings = {}\n",
    "settings['root'] = 'ISOP'\n",
    "settings['yield cutoff'] = 0.1\n",
    "settings['rate cutoff'] = 0\n",
    "settings['protected'] = ['ISOP','ISOP1OH23O4OHt','ISOP1OH23O4OHc', 'ISOP1OH2OH34O', 'MVK','HCHO', 'HAC','PAN','GLYC','PYRAC','MGLY','HCOOH','MACR','MPAN','GLYX']\n",
    "#settings['']\n",
    "settings['# of species desired'] = 200\n",
    "\n",
    "background_dict_2['OH'] = 0.0002\n",
    "background_dict_2['HO2'] = 7.00E-03\n",
    "background_dict_2['NO'] = 0.08\n",
    "background_dict_2['O3'] = 20\n",
    "background_dict_2['NO3'] = 0.007\n",
    "background_dict_2['CH3OO'] = 0.001\n",
    "background_dict_2['CH3CO3'] = 0.001\n",
    "background_dict_2['O2'] = 210000000\n",
    "\n",
    "\"\"\"PROD 2: rxn_dict includes the new prod coefficients, and it is put into a test dictionary\"\"\"\n",
    "\n",
    "\n",
    "# inputs required: compare_species_r, compare_species, reference_data, input_conditions\n",
    "# inputs_test = {'spc':species_list_names_r, 'rxn':rxn_dict, 'bck':background_dict_2, 'scnd':scnd_dict, 'atm cond':atm_cond,'settings':settings}\n",
    "\n",
    "# inputs: conditions, background dict, test, compare_species, compare_species_r, species_list_names_r, reference_data, spec_factor,\n",
    "\n",
    "\n",
    "inputs_test = {'spc':species_list_names_r, 'rxn':rxn_dict, 'bck':background_dict_2, 'scnd':scnd_dict, 'atm cond':atm_cond,'settings':settings} # dictionary with test variables\n",
    "\n",
    "#inputs = {'test': inputs_test, 'conditions':input_conditions, 'bck':background_dict_2,'compare species':compare_species,'compare_species_r':compare_species_r,'spc':species_list_names_r,'spec factor':spec_factor, 'reference data':reference_data}\n",
    "\n",
    "\n",
    "\n",
    "isop_nit_tetra = ['ISOPN','ISOP1OH2N3OH4OOH','ISOP1CO23O4N','ISOP1N2OH3OOH4N','ISOP1OOH2OH3CO4N','IHNPE','ISOP1N23O4OH','ISOP1N2OH3N4OH','ISOP1N253N4OH','ISOP1OH2N3N4OH','ISOP1CO2N3OH4N','ISOP1N2OOH3OOH4CO','ISOP1OOH2OH3OOH4N','ISOP1OH2N3OOH4CO','ISOP1OOH2OH3N4CO','ISOP1OH12O3OOH4N','ISOP1OOH2N3OOH4CO','ISOP1OOH23O3OH4N','ISOP1OOH2OH3OH4N','ISOP1N2OOH3OH4OH','ISOP1OH2OH3N4N','ISOP1OH2N3CO4N','ISOP1OH2N3OH4N','ISOP1OOH2OOH3N4CO','ISOP1CO2OH3OOH4N','ISOP12O3OH4N','ISOP1N23O4CO','ISOP1OH2N3OOH4N','ISOP1N2OH3OOH4OH','ISOP1N2OH3CO4N','ISOP1OOH2OH3N4N','ISOP1OH2OOH3OOH4N','ISOP1OH23O4N','ISOP1CO2N3OOH4OOH','ISOP1CO2N3OH4OH','ISOP1N2OH3OOH4OOH','ITHN','ISOP1N2N3OOH4OH','ISOP1N2OH3N4OOH','ISOP1OOH2OH3N4OH','ISOP1N2OOH34O4OH','ISOP1OH2OH3OOH4N','IDHCN','ISOP1OH2N3OOH4OH','ISOP1N2OOH3N4OH','ISOP1N2OH3N4CO','ISOP1N2OH3OH4N','ISOP1CO2OOH3OH4N','IDHDN','ISOP1N2N3OH4OH','ISOP1N2N3OH4OOH','IHNDP','ISOP1N2OOH3OH4CO','ICHNP','ISOP1OOH2N3OH4N','ISOP1CO2N3OOH4OH','ISOP1OH2N3CO4OH','IHPDN','ISOP1OOH2OOH3OH4N','ISOP1CO2OOH3N4OH','ISOP1N2OH34O','ISOP12O3CO4N','ISOP1CO2N3OH4OOH','ISOP1N2N3CO4OH','ISOP1CO2OOH3N4OOH','ISOP1OH2OOH3CO4N','ISOP1N2OH3OH4OOH','ICNE','ISOP1N2OH3CO4OOH','IDHPN','ICHDN','ISOP1CO2OOH3OOH4N','ISOP1OH2OOH3N4OH','ISOP1N2OOH3OH4OOH','ISOP1OH2OOH3OH4N','ISOP1OH2OOH3N4CO','ISOP1N2OOH3OH4N','ISOP1OH2OOH3N4N','ISOP1N2OH3OOH4CO','ISOP1OH2OH3N4CO','ITCN','IHNE','ISOP1N2OOH3OOH4OH']\n",
    "tetra = ['TETRA','ISOP1OH23O4CO','ISOP1OH12O3OOH4OH','ISOP1OH12O3OH4OOH','ISOP1CO2OH3OOH4CO','ISOP1OOH2OH3CO4OH','ISOP1CO2OOH34O4OH','ICHE','ISOP1CO2OOH3OOH4OOH','ISOP1OH12O3OOH4CO','ICPDH','ISOP1OH2OOH3CO4OH','ISOP1OOH2OH34O4OH','ISOP12O3OH4CO','ISOP1OH2OOH3CO4CO','ISOP1OOH2OH34O','ISOP1OOH2OH3CO4CO','IDCHP','ISOP1OOH23O4CO','ISOP1CO23O4OOH','ISOP1CO2OOH3OH4OH','ISOP1OH2OH3OOH4CO','ISOP12O3CO4OH','ICHDN','ISOP1CO2OH3OOH4OH','ISOP1CO2OOH3OOH4N','ISOP1OH2OOH3OH4OOH','ISOP1CO2OOH3OH4CO','ISOP1CO2OOH3OOH4OH','ISOP1OH2OOH3OOH4OH','ISOP1OOH2OOH3CO4CO','ISOP1OH2OOH3OOH4CO','IDHPE','ISOP1OOH2OOH3OOH4CO','ISOP1CO2OOH3OOH4CO','ISOP1OOH2OH3OOH4OH','IHPDN','IDHDP','ISOP12O3OH4OOH','ISOP1OH2OOH34O4OH','ISOP1CO23O4OH','ISOP1OH2OOH3OH4CO','ISOP1CO2OH34O']\n",
    "ihn_plus = ['IHN','ISOP1OOH4N','ISOP3N4N','IHNB','ICN','ISOP1N4OOH','ISOP1OH4Nc','ISOP3N4OH','ISOP1CO4N','ISOP3CO4N','IDN','ISOP1OH2N','ISOP1N4CO','ISOP3OH4N','ISOP1OH4N','ISOP1N2OOH','ISOP1N4N','ISOP1PAN4OH','ISOP1N4OH','INPD','ISOP1N2OH','ISOP1N2N','ISOP1N4OHt','ISOP1OH4PAN','ISOP1N4PAN','ISOP1PAN4N','ISOP1N4OHc','ISOP1OH4Nt','INPB','ISOP3OOH4N']\n",
    "int_yield = 0\n",
    "tetra_yield = 0\n",
    "ihn_plus_yield = 0\n",
    "\n",
    "compare_species = {'IEPOX':['ISOP1OH23O4OHt', 'ISOP1OH23O4OHc', 'ISOP1OH2OH34O'], 'MVK':'MVK', 'HCHO':'HCHO', 'HAC':'HAC', 'GLYC':'GLYC', 'MGLY':'MGLY', 'MACR':'MACR','OH':'OH','NO':'NO',    'NO2':'NO2',    'O3':'O3',     'HO2':'HO2','CH3OO':'CH3OO','CH3CO3':'CH3CO3', 'ISOPN':isop_nit_tetra, 'TETRA':tetra, 'IHN':ihn_plus}\n",
    "compare_species_r = {'IEPOX':['ISOP1OH23O4OHt', 'ISOP1OH23O4OHc', 'ISOP1OH2OH34O'], 'MVK':'MVK', 'HCHO':'HCHO', 'HAC':'HAC', 'GLYC':'GLYC', 'MGLY':'MGLY', 'MACR':'MACR','OH':'OH','NO':'NO',    'NO2':'NO2',    'O3':'O3',     'HO2':'HO2','CH3OO':'CH3OO','CH3CO3':'CH3CO3', 'ISOPN':isop_nit_tetra, 'TETRA':tetra, 'IHN':ihn_plus}\n",
    "\n",
    "\n",
    "species_list_names_2 = deepcopy(species_list_names)\n",
    "species_list_names_2.append('ISOPN')\n",
    "species_list_names_2.append('TETRA')\n",
    "species_list_names_2.append('IHN')\n",
    "dic = {species_list_names[i]:i for i in range(len(species_list_names))}\n",
    "dic_r = {species_list_names_r[i]:i for i in range(len(species_list_names_r))}\n",
    "#yi2, new_graph, new_in = get_yields_from_inputs(inputs_ref)\n",
    "#for index, i in enumerate(compare_species):\n",
    "#  compare_species[index] = [i]\n",
    "#  if i == 'isop_nit_tetra':\n",
    "#    caty = []\n",
    "#    for spec in isop_nit_tetra:\n",
    "#      if spec in full_spc:\n",
    "#        caty.append(spec)\n",
    "#    for i in caty:\n",
    "#            spec_yi = 0\n",
    "#            for x in new_in[i]:\n",
    "#                if x not in caty:\n",
    "#                    if i in new_graph[x]:\n",
    "#                        spec_yi += new_graph[x][i]*yi2[x]\n",
    "#            int_yield += spec_yi\n",
    "#    compare_species_r.append([caty])\n",
    "#  elif i == 'tetra':\n",
    "#    caty = []\n",
    "#    for spec in tetra:\n",
    "#      if spec in full_spc:\n",
    "#        caty.append(spec)\n",
    "#    for i in caty:\n",
    "#            spec_yi = 0\n",
    "#            for x in new_in[i]:\n",
    "#                if x not in caty:\n",
    "#                    if i in new_graph[x]:\n",
    "#                        spec_yi += new_graph[x][i]*yi2[x]\n",
    "#            tetra_yield += spec_yi\n",
    "#    compare_species_r.append([caty])\n",
    "#  elif i == 'ihn_plus_yield':\n",
    "#    caty = []\n",
    "#    for spec in ihn_plus_yield:\n",
    "#      if spec in full_spc:\n",
    "#        caty.append(spec)\n",
    "#    for i in caty:\n",
    "#            spec_yi = 0\n",
    "#            for x in new_in[i]:\n",
    "#                if x not in caty:\n",
    "#                    if i in new_graph[x]:\n",
    "#                        spec_yi += new_graph[x][i]*yi2[x]\n",
    "#            ihn_plus_yield += spec_yi\n",
    "#    compare_species_r.append([caty])\n",
    "#  else:\n",
    "#    compare_species_r.append([i])\n",
    "\n",
    "\n",
    "reference_data = [{} for i in range(len(input_conditions))]\n",
    "for i in range(len(input_conditions)):\n",
    "    background_dict_2['OH'] = input_conditions[i]['OH']\n",
    "    background_dict_2['HO2'] = input_conditions[i]['HO2']\n",
    "    background_dict_2['NO'] = input_conditions[i]['NO']\n",
    "    background_dict_2['O3'] = input_conditions[i]['O3']\n",
    "    background_dict_2['NO3'] = input_conditions[i]['NO3']\n",
    "    background_dict_2['CH3OO'] = input_conditions[i]['CH3OO']\n",
    "    background_dict_2['CH3CO3'] = input_conditions[i]['CH3CO3']\n",
    "    background_dict_2['O2'] = 210000000\n",
    "    atm_cond['sza'] = input_conditions[i]['sza']\n",
    "    atm_cond['sun'] = input_conditions[i]['sun']\n",
    "    inputs_ref['bck'] = background_dict_2\n",
    "    # 'spc', 'rxn', 'bck', 'scnd', 'atm cond', 'settings'\n",
    "    yields_ref, new_graph, new_in, scc = get_yields_from_inputs_full(inputs_ref)\n",
    "\n",
    "    for j in compare_species:\n",
    "      if isinstance(compare_species[j], list):\n",
    "          cat_yield = 0\n",
    "          for k in compare_species[j]:\n",
    "            if k in species_list_names:\n",
    "                cat_yield+= yields_ref[k]\n",
    "          reference_data[i][j] = cat_yield\n",
    "      else:\n",
    "        reference_data[i][j] = yields_ref[compare_species[j]]\n",
    "\n",
    "\n",
    "spec_factor = {}\n",
    "for k in compare_species:\n",
    "  if k not in background_spc_n:\n",
    "    yield_list = []\n",
    "    for i in range(len(reference_data)):\n",
    "        yield_list.append(abs(reference_data[i][k]))\n",
    "    spec_factor[k] = 1 #max(yield_list)\n",
    "  else:\n",
    "    spec_factor[k] = -1\n",
    "\n",
    "\"\"\"PROD 4: Update the reduced mechanism dictionary with the yields that are found from the prod coefficients\n",
    "\n",
    "There are two important dictionaries that go into this evaluation function: 1) The reference data dictionary which holds the data of the original mechanism holds the actual yields of its end species\n",
    "2) The reduction data dictionary holds the data for the reduced mechanism and the yields of its end species based on the prod coefficients that we are experimenting with\n",
    "\n",
    "We then look at species that we want to compare and assign a score based on the similarity of the reduction species yield and reference species yield\n",
    "\"\"\"\n",
    "\n",
    "#reduction_data = [{} for i in range(len(input_conditions))]\n",
    "#for i in range(len(input_conditions)):\n",
    "#    background_dict_2['OH'] = input_conditions[i]['OH']\n",
    "#    background_dict_2['HO2'] = input_conditions[i]['HO2']\n",
    "#    background_dict_2['NO'] = input_conditions[i]['NO']\n",
    "#    background_dict_2['O3'] = input_conditions[i]['O3']\n",
    "#    background_dict_2['NO3'] = input_conditions[i]['NO3']\n",
    "#    background_dict_2['CH3OO'] = input_conditions[i]['CH3OO']\n",
    "#    background_dict_2['CH3CO3'] = input_conditions[i]['CH3CO3']\n",
    "#    background_dict_2['O2'] = 210000000\n",
    "#    atm_cond['sza'] = input_conditions[i]['sza']\n",
    "#    atm_cond['sun'] = input_conditions[i]['sun']\n",
    "#    inputs_test['bck'] = background_dict_2\n",
    "#    yields_test, _, _ = get_yields_from_inputs(inputs_test) # get yields of species from new dictionary with test prod coefficients\n",
    "#    for k in compare_species:\n",
    "#      for j in k:\n",
    "\n",
    "\n",
    "prod_list_n_r\n",
    "# prod_coeff_list_r\n",
    "red_set = set(species_list_names_r)\n",
    "\n",
    "\n",
    "inputs = {'test': inputs_test, 'conditions':input_conditions, 'bck':background_dict_2,'compare_species':compare_species,'compare_species_r':compare_species_r,'spc':species_list_names_r,'spec factor':spec_factor, 'reference data':reference_data, 'scc':scc}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M-ZcyMyguau_",
   "metadata": {
    "id": "M-ZcyMyguau_"
   },
   "source": [
    "# Optimization Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7_IjA0GfqUFp",
   "metadata": {
    "executionInfo": {
     "elapsed": 173,
     "status": "ok",
     "timestamp": 1765566473675,
     "user": {
      "displayName": "Forwood Cloud Wiser",
      "userId": "16746988716585544349"
     },
     "user_tz": 300
    },
    "id": "7_IjA0GfqUFp"
   },
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "#import isoprene_rates as rate\n",
    "from math import exp as EXP\n",
    "from copy import deepcopy\n",
    "import sympy as sym\n",
    "#import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "#import graphviz\n",
    "#import pygraphviz as pgv\n",
    "#import to_precision\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Import modules\n",
    "import numpy as np\n",
    "\n",
    "# Import PySwarms\n",
    "#import pyswarms as ps\n",
    "#from pyswarms.utils.functions import single_obj as fx\n",
    "\n",
    "# this optimizer has min max for coeffs and conservartion constraints x+y+z\n",
    "#\n",
    "def anti_community_gd(iterations, inputs, learning_rate, beta1, beta2, individual_coeffs, evaluate, matrix_keys, individual_coeff_map, prod_list_n_r, min_coeffs, max_coeffs, conservation_constraints, map):\n",
    "  \"\"\"### 5. Gradient Descent with Adam\"\"\"\n",
    "  print()\n",
    "  print(\"Beginning anti-community clustering gradient descent\")\n",
    "\n",
    "  def adam(h, coeffs, grads, learning_rate, epsilon=1e-8):\n",
    "\n",
    "      curr_t = t[h]\n",
    "      curr_m = m[h]\n",
    "      curr_v = v[h]\n",
    "\n",
    "      curr_t += 1\n",
    "      new_coeffs = []\n",
    "\n",
    "\n",
    "      for i, (coeff, g) in enumerate(zip(coeffs, grads)):\n",
    "          curr_m[i] = beta1 * curr_m[i] + (1 - beta1) * g\n",
    "          curr_v[i] = beta2 * curr_v[i] + (1 - beta2) * (g ** 2)\n",
    "\n",
    "          m_hat = curr_m[i] / (1 - beta1 ** curr_t)\n",
    "          v_hat = curr_v[i] / (1 - beta2 ** curr_t)\n",
    "\n",
    "          coeff = coeff - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "          new_coeffs.append(coeff)\n",
    "\n",
    "      t[h] = curr_t\n",
    "      m[h] = curr_m\n",
    "      v[h] = curr_v\n",
    "\n",
    "      return new_coeffs\n",
    "\n",
    "  t = []\n",
    "  m = []\n",
    "  v = []\n",
    "\n",
    "  tick = time.time()\n",
    "  new_prod_coeffs = copy_prod(prod_coeff_list_r)\n",
    "  if min_coeffs == 1:\n",
    "    min_coeffs = []\n",
    "    for i in range(len(new_prod_coeffs)):\n",
    "      min_coeffs.append([])\n",
    "      for j in range(len(new_prod_coeffs[i])):\n",
    "        min_coeffs[i].append(new_prod_coeffs[i][j]/2)\n",
    "  if max_coeffs == 1:\n",
    "    max_coeffs = []\n",
    "    for i in range(len(new_prod_coeffs)):\n",
    "      max_coeffs.append([])\n",
    "      for j in range(len(new_prod_coeffs[i])):\n",
    "        max_coeffs[i].append(new_prod_coeffs[i][j]*1.5)\n",
    "  if min_coeffs == 0:\n",
    "    min_coeffs = []\n",
    "    for i in range(len(new_prod_coeffs)):\n",
    "      min_coeffs.append([])\n",
    "      for j in range(len(new_prod_coeffs[i])):\n",
    "        min_coeffs[i].append(-10000000000000000000000000000)\n",
    "  if max_coeffs == 0:\n",
    "    max_coeffs = []\n",
    "    for i in range(len(new_prod_coeffs)):\n",
    "      max_coeffs.append([])\n",
    "      for j in range(len(new_prod_coeffs[i])):\n",
    "        max_coeffs[i].append(10000000000000000000000000000)\n",
    "\n",
    "\n",
    "\n",
    "  history = []\n",
    "\n",
    "  values_to_change = individual_coeffs  # value 1260 was found to have the most effect, and value 1102 was found to have the most effect out of any value that shared coeff yield changes with value 1260\n",
    "  initial_score, initial_score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "\n",
    "  initial_flat_score_matrix = []\n",
    "  for i in range(len(initial_score_matrix)):\n",
    "    for key in matrix_keys:\n",
    "      initial_flat_score_matrix.append(initial_score_matrix[i][key])\n",
    "\n",
    "  # Initial coefficients (you can start with any values)\n",
    "  initial_coefficients = []\n",
    "  for i in range(len(individual_coeff_map)):\n",
    "      current_coeffs = []\n",
    "      for n in range(len(individual_coeff_map[i])):\n",
    "        j = map[values_to_change[i][n]][0]\n",
    "        k = map[values_to_change[i][n]][1]\n",
    "        current_coeffs.append(new_prod_coeffs[j][k])\n",
    "      initial_coefficients.append(current_coeffs)\n",
    "\n",
    "\n",
    "  # Set hyperparameters\n",
    "  # iterations = 3\n",
    "  # iterations = 1700\n",
    "  history = []\n",
    "  times = []\n",
    "  gradient_sum_squares = []\n",
    "  individual_coeff_score = []\n",
    "  # learning_rate = 0.015\n",
    "  learning_rates = []\n",
    "  perturbation = []\n",
    "\n",
    "  best_coeffs = []\n",
    "  for i in range(len(initial_coefficients)):\n",
    "    coeffs_list = []\n",
    "    for j in range(len(initial_coefficients[i])):\n",
    "      coeffs_list.append(initial_coefficients[i][j])\n",
    "    best_coeffs.append(coeffs_list)\n",
    "\n",
    "  best_score = initial_score\n",
    "\n",
    "  # set initial perturbations, learning rates, and find initial individual coefficient scores\n",
    "  for i in range(len(initial_coefficients)):\n",
    "    gradient_sum_squares.append([])\n",
    "    perturbation.append([])\n",
    "    learning_rates.append([])\n",
    "    individual_coeff_score.append([])\n",
    "    t.append(0)\n",
    "    m.append([])\n",
    "    v.append([])\n",
    "    for j in range(len(initial_coefficients[i])):\n",
    "      m[i].append(0)\n",
    "      v[i].append(0)\n",
    "      learning_rates[i].append(learning_rate)\n",
    "      # perturbation[i].append(initial_coefficients[i][j]*0.00000005)\n",
    "      perturbation[i].append(initial_coefficients[i][j]*0.000005)\n",
    "      gradient_sum_squares[i].append(0)\n",
    "      score = 0\n",
    "      for k in individual_coeff_map[i][j]:\n",
    "        score += abs(initial_flat_score_matrix[k])\n",
    "      individual_coeff_score[i].append(score)\n",
    "\n",
    "  reliers = []\n",
    "  for key in list(conservation_constraints.keys()):\n",
    "    # find indices of molecules for groups constraints in prod list\n",
    "    for i in range(len(conservation_constraints[key]['constraints'])):\n",
    "      for j in range(len(conservation_constraints[key]['constraints'][i][0])):\n",
    "        relieri = -1\n",
    "        molecule = conservation_constraints[key]['constraints'][i][0][j]\n",
    "        for k in range(len(conservation_constraints[key]['constraints'][i][2])):\n",
    "          if molecule == conservation_constraints[key]['constraints'][i][2][k]:\n",
    "            relieri = k\n",
    "        found = False\n",
    "        for k in range(len(prod_list_n_r[key-1])):\n",
    "          if molecule == prod_list_n_r[key-1][k]:\n",
    "            conservation_constraints[key]['constraints'][i][0][j] = [key-1, k]\n",
    "            found = True\n",
    "        if not found:\n",
    "          print(molecule, \"was not found in reaction\", key)\n",
    "        if relieri > -1:\n",
    "           conservation_constraints[key]['constraints'][i][2][relieri] = conservation_constraints[key]['constraints'][i][0][j]\n",
    "           reliers.append(conservation_constraints[key]['constraints'][i][0][j])\n",
    "    # find indices of molecules for ranges in prod list\n",
    "    for i in range(len(conservation_constraints[key]['prod_ranges'])):\n",
    "      molecule = conservation_constraints[key]['prod_ranges'][i][0]\n",
    "      lower = conservation_constraints[key]['prod_ranges'][i][1]\n",
    "      upper = conservation_constraints[key]['prod_ranges'][i][2]\n",
    "      found = False\n",
    "      for k in range(len(prod_list_n_r[key-1])):\n",
    "        if molecule == prod_list_n_r[key-1][k]:\n",
    "          conservation_constraints[key]['prod_ranges'][i][0] = [key-1, k]\n",
    "          found = True\n",
    "      molecule = conservation_constraints[key]['prod_ranges'][i][0]\n",
    "      if found and isinstance(lower, str):\n",
    "        if '%' in lower:\n",
    "          percentage = float(lower.strip('%'))/100\n",
    "          og_num = new_prod_coeffs[molecule[0]][molecule[1]]\n",
    "          conservation_constraints[key]['prod_ranges'][i][1] = percentage*og_num\n",
    "        if '%' in upper:\n",
    "          percentage = float(upper.strip('%'))/100\n",
    "          og_num = new_prod_coeffs[molecule[0]][molecule[1]]\n",
    "          conservation_constraints[key]['prod_ranges'][i][2] = percentage*og_num\n",
    "\n",
    "      if not found:\n",
    "        print(molecule, \"was not found in reaction\", key)\n",
    "\n",
    "  #print(conservation_constraints)\n",
    "\n",
    "  for x in range(iterations):\n",
    "      # for h in range(5):\n",
    "      # for h in [0]:\n",
    "      for h in range(len(individual_coeffs)):\n",
    "      # for h in [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]:\n",
    "        values_to_change = individual_coeffs[h]\n",
    "        gradient = np.zeros_like(initial_coefficients[h])\n",
    "\n",
    "        # Update prod coeffs list\n",
    "        for i in range(len(initial_coefficients[h])):\n",
    "            new_prod_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]] = initial_coefficients[h][i] + perturbation[h][i]\n",
    "\n",
    "        # calculate costs with plus perturbations\n",
    "        avg_score, score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "        flat_score_matrix = []\n",
    "        for i in range(len(score_matrix)):\n",
    "          for key in matrix_keys:\n",
    "            flat_score_matrix.append(score_matrix[i][key])\n",
    "\n",
    "        plus_scores = []\n",
    "        for i in range(len(initial_coefficients[h])):\n",
    "          matrix_values = individual_coeff_map[h][i]\n",
    "          total_score = 0\n",
    "          for j in matrix_values:\n",
    "            total_score += abs(flat_score_matrix[j])\n",
    "            # total_score += flat_score_matrix[j]**2\n",
    "\n",
    "          total_score = (total_score/len(matrix_values))\n",
    "          plus_scores.append(total_score)\n",
    "\n",
    "\n",
    "        plus_costs = np.multiply(plus_scores, plus_scores) # square costs\n",
    "\n",
    "        for i in range(len(initial_coefficients[h])):\n",
    "            new_prod_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]] = initial_coefficients[h][i] - perturbation[h][i]\n",
    "\n",
    "        # calculate costs with minus perturbations\n",
    "        avg_score, score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "        flat_score_matrix = []\n",
    "        for i in range(len(score_matrix)):\n",
    "          for key in matrix_keys:\n",
    "            flat_score_matrix.append(score_matrix[i][key])\n",
    "\n",
    "        minus_scores = []\n",
    "        for i in range(len(initial_coefficients[h])):\n",
    "          matrix_values = individual_coeff_map[h][i]\n",
    "          total_score = 0\n",
    "          for j in matrix_values:\n",
    "            total_score += abs(flat_score_matrix[j])\n",
    "\n",
    "          total_score = (total_score/len(matrix_values))\n",
    "          minus_scores.append(total_score)\n",
    "\n",
    "\n",
    "        minus_costs = np.multiply(minus_scores, minus_scores) # square costs\n",
    "\n",
    "        gradients = []\n",
    "        for i in range(len(plus_costs)):\n",
    "          gradient = (plus_costs[i]-minus_costs[i])/perturbation[h][i]\n",
    "          gradients.append(gradient)\n",
    "\n",
    "        new_coeffs = adam(h, initial_coefficients[h], gradients, learning_rate=learning_rate)\n",
    "\n",
    "        # Update coefficients and adapt perturbations and learning rates\n",
    "        for i in range(len(initial_coefficients[h])):\n",
    "          score = 0\n",
    "          for k in individual_coeff_map[h][i]:\n",
    "            score += abs(flat_score_matrix[k])\n",
    "\n",
    "          individual_coeff_score[h][i] = score\n",
    "          initial_coefficients[h][i] = new_coeffs[i]\n",
    "\n",
    "          boundary_found = False\n",
    "          for key in list(conservation_constraints.keys()):\n",
    "            for group in conservation_constraints[key]['prod_ranges']:\n",
    "              molecule = group[0]\n",
    "              if molecule == [map[values_to_change[i]][0],map[values_to_change[i]][1]]:\n",
    "                boundary_found = True\n",
    "                if initial_coefficients[h][i]<group[1]:\n",
    "                  initial_coefficients[h][i]=group[1]\n",
    "                elif initial_coefficients[h][i]>group[2]:\n",
    "                  initial_coefficients[h][i]=group[2]\n",
    "\n",
    "          if not boundary_found:\n",
    "            if initial_coefficients[h][i] < 0:\n",
    "              initial_coefficients[h][i] = 0\n",
    "            if initial_coefficients[h][i]<min_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]]:\n",
    "              initial_coefficients[h][i]=min_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]]\n",
    "            if initial_coefficients[h][i]>max_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]]:\n",
    "              initial_coefficients[h][i]=max_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]]\n",
    "\n",
    "          new_prod_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]] = initial_coefficients[h][i]\n",
    "\n",
    "          for key in list(conservation_constraints.keys()):\n",
    "            for group in conservation_constraints[key]['constraints']:\n",
    "              relier = group[2][0]\n",
    "              if relier[0] == [map[values_to_change[i]][0],map[values_to_change[i]][1]]:\n",
    "                other_sum = 0\n",
    "                for coeff in group[0]:\n",
    "                  if coeff != relier:\n",
    "                    other_sum += new_prod_coeffs[coeff[0]][coeff[1]]\n",
    "                relier_val = group[1]-other_sum\n",
    "                new_prod_coeffs[relier[0]][relier[1]] = relier_val\n",
    "                initial_coefficients[h][i] = relier_val\n",
    "\n",
    "      # set all coefficients to their updated values\n",
    "      for h in range(len(initial_coefficients)):\n",
    "        for i in range(len(initial_coefficients[h])):\n",
    "            new_prod_coeffs[map[individual_coeffs[h][i]][0]][map[individual_coeffs[h][i]][1]] = initial_coefficients[h][i]\n",
    "\n",
    "      # check for the ones that need to be summed up correctly\n",
    "      # update coefficients that needed to be a specific value in order to satisfy sums\n",
    "\n",
    "      for key in list(conservation_constraints.keys()):\n",
    "        for group in conservation_constraints[key]['constraints']:\n",
    "          # change [0] later to loop through all reliers\n",
    "          relier = group[2][0]\n",
    "          if relier != 0:\n",
    "            other_sum = 0\n",
    "            for coeff in group[0]:\n",
    "              if coeff != relier:\n",
    "                other_sum += new_prod_coeffs[coeff[0]][coeff[1]]\n",
    "            relier_val = group[1]-other_sum\n",
    "            new_prod_coeffs[relier[0]][relier[1]] = relier_val\n",
    "          #print(\"group\", group, \"other_sum\", other_sum, \"relier_val\", relier_val)\n",
    "\n",
    "      avg_score, score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "\n",
    "      history.append(avg_score)\n",
    "      times.append(time.time()-tick)\n",
    "\n",
    "      print((x+1)*100/iterations, \"percent completed: The average score has been optimized by\", ((initial_score - avg_score) / initial_score) * 100, \"percent\")\n",
    "\n",
    "      if x>30:\n",
    "        if abs((((initial_score - history[x-30]) / initial_score) * 100) - (((initial_score - avg_score) / initial_score) * 100)) < 0.5:\n",
    "          all_bad = True\n",
    "          for i in range(1, 15):\n",
    "            if abs((((initial_score - history[x-i]) / initial_score) * 100) - (((initial_score - avg_score) / initial_score) * 100)) > 0.2:\n",
    "              all_bad = False\n",
    "          if all_bad:\n",
    "            print(\"not enough progress to continue--stopping optimization\")\n",
    "            break\n",
    "\n",
    "      if avg_score < best_score:\n",
    "        best_score = avg_score\n",
    "        best_coeffs = []\n",
    "        for i in range(len(initial_coefficients)):\n",
    "          coeffs_list = []\n",
    "          for j in range(len(initial_coefficients[i])):\n",
    "            coeffs_list.append(initial_coefficients[i][j])\n",
    "          best_coeffs.append(coeffs_list)\n",
    "\n",
    "  for i in range(len(best_coeffs)):\n",
    "    for j in range(len(best_coeffs[i])):\n",
    "      new_prod_coeffs[map[individual_coeffs[i][j]][0]][map[individual_coeffs[i][j]][1]] = best_coeffs[i][j]\n",
    "\n",
    "\n",
    "  final_score, final_score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "  tock = time.time()\n",
    "\n",
    "  print(final_score_matrix)\n",
    "  print(\"\\nOptimized Coefficients:\", best_coeffs)\n",
    "  print(\"Final Score:\", final_score)\n",
    "  print(\"Optimized by\", ((initial_score - final_score) / initial_score) * 100, \"percent\")\n",
    "  print(\"Times array\", times)\n",
    "  print(\"Scores array\", history)\n",
    "  print(\"\\nThe gradient descent\", tock-tick, \"seconds\")\n",
    "\n",
    "  plt.plot(times, history)\n",
    "  return best_coeffs\n",
    "\n",
    "def undirected_graph_sort(iterations, learning_rate, beta1, beta2, individual_coeffs, evaluate, matrix_keys, individual_coeff_map, copy_prod, min_coeffs, max_coeffs, conservation_constraints):\n",
    "  print()\n",
    "  print(\"Beginning undirected graph sort gradient descent\")\n",
    "  \"\"\"### 6. Undirected Graph Sort\"\"\"\n",
    "\n",
    "  # MAKE TOPOLOGICAL ORIENTATION OF GRAPH (reverse of yield_change_graph)\n",
    "  topo_graph = []\n",
    "  for j in range(len(yield_change_graph[0])):\n",
    "    current_graph = []\n",
    "    for row in yield_change_graph:\n",
    "      current_graph.append(row[j])\n",
    "    topo_graph.append(current_graph)\n",
    "\n",
    "  # SORT GRAPH\n",
    "  topo_sort_graph = []\n",
    "  topo_coeffs2 = []\n",
    "  topo_effected_yields = []\n",
    "  rows_map = []\n",
    "  topo_graph_dict = {}\n",
    "\n",
    "  for i in range(len(topo_graph)):\n",
    "    rows_map.append(i)\n",
    "    topo_graph_dict[i] = topo_graph[i]\n",
    "\n",
    "\n",
    "  rows_delete = []\n",
    "  dict_keys = list(topo_graph_dict.keys())\n",
    "  for i in dict_keys:\n",
    "    counter = 0\n",
    "    for j in range(len(topo_graph[i])):\n",
    "      counter += topo_graph[i][j]\n",
    "    if counter == 0:\n",
    "      rows_delete.append(i)\n",
    "\n",
    "  for i in rows_delete:\n",
    "    del topo_graph_dict[i]\n",
    "\n",
    "\n",
    "  # iterate sorting until there are no rows left\n",
    "  while len(topo_graph_dict) > 0:\n",
    "    counter_obj = {}\n",
    "    rows_delete = []\n",
    "    least = 1000\n",
    "    greatest = 0\n",
    "    dict_keys = list(topo_graph_dict.keys())\n",
    "\n",
    "    # check how many connections all rows have\n",
    "    for i in dict_keys:\n",
    "      counter = 0\n",
    "      for j in range(len(topo_graph_dict[i])):\n",
    "        counter += topo_graph[i][j]\n",
    "      # check if this row has the least amount of connections\n",
    "      if counter < least and counter > 0:\n",
    "        least = counter\n",
    "      # find greatest number of connections just for informational purposes\n",
    "      if counter > greatest:\n",
    "        greatest = counter\n",
    "      # update counting object with the number of connection this row has\n",
    "      if counter in counter_obj:\n",
    "        new_list = counter_obj[counter]\n",
    "        counter_obj[counter].append(i)\n",
    "      else:\n",
    "        counter_obj[counter] = [i]\n",
    "\n",
    "\n",
    "    # get all rows with the least amount of connections, and get all effecting coefficients in each of those rows\n",
    "    least_rows = counter_obj[least]\n",
    "    curr_topo_coeffs = []\n",
    "    curr_effected_yields = []\n",
    "    curr_topo_coeffs22 = []\n",
    "    for i in least_rows:\n",
    "      curr_topo_coeffs2 = []\n",
    "      for j in range(len(topo_graph_dict[i])):\n",
    "        if topo_graph_dict[i][j] == 1 and j not in curr_topo_coeffs:\n",
    "          curr_topo_coeffs.append(j)\n",
    "          curr_topo_coeffs2.append(j)\n",
    "      curr_effected_yields.append(i)\n",
    "      curr_topo_coeffs22.append(curr_topo_coeffs2)\n",
    "\n",
    "    topo_sort_graph.append(curr_topo_coeffs)\n",
    "    topo_effected_yields.append(curr_effected_yields)\n",
    "    topo_coeffs2.append(curr_topo_coeffs22)\n",
    "\n",
    "    # remove all connections with the current coefficients that we just got\n",
    "    dict_keys = list(topo_graph_dict.keys())\n",
    "    for i in dict_keys:\n",
    "      for j in curr_topo_coeffs:\n",
    "        topo_graph[i][j] = 0\n",
    "\n",
    "    for i in dict_keys:\n",
    "      counter = 0\n",
    "      for j in range(len(topo_graph[i])):\n",
    "        counter += topo_graph[i][j]\n",
    "      if counter == 0:\n",
    "        rows_delete.append(i)\n",
    "\n",
    "    for i in rows_delete:\n",
    "      del topo_graph_dict[i]\n",
    "\n",
    "  counter = 0\n",
    "  for row in topo_sort_graph:\n",
    "    for num in row:\n",
    "      counter += 1\n",
    "\n",
    "  topo_map = [row[:] for row in topo_sort_graph]\n",
    "  for i in range(len(topo_map)):\n",
    "    for j in range(len(topo_map[i])):\n",
    "      coeff = topo_map[i][j]\n",
    "      yield_changes = yield_change_graph[coeff]\n",
    "      current_changes = []\n",
    "      for k in range(len(yield_changes)):\n",
    "        if yield_changes[k] == 1:\n",
    "          current_changes.append(k)\n",
    "      topo_map[i][j] = current_changes\n",
    "\n",
    "  topo_map2 = [row[:] for row in [row2[:] for row2 in [row3[:] for row3 in topo_coeffs2]]]\n",
    "  topo_map2 = []\n",
    "  for i in range(len(topo_coeffs2)):\n",
    "    curr_topo_map = []\n",
    "    for j in range(len(topo_coeffs2[i])):\n",
    "      curr_topo_map2 = []\n",
    "      for k in range(len(topo_coeffs2[i][j])):\n",
    "        coeff = topo_coeffs2[i][j][k]\n",
    "        yield_changes = yield_change_graph[coeff]\n",
    "        current_changes = []\n",
    "        for l in range(len(yield_changes)):\n",
    "          if yield_changes[l] == 1:\n",
    "            current_changes.append(l)\n",
    "        curr_topo_map2.append(current_changes)\n",
    "      curr_topo_map.append(curr_topo_map2)\n",
    "    topo_map2.append(curr_topo_map)\n",
    "\n",
    "  tick = time.time()\n",
    "  new_prod_coeffs = copy_prod(prod_coeff_list_r)\n",
    "  if min_coeffs == 1:\n",
    "    min_coeffs = []\n",
    "    for i in range(len(new_prod_coeffs)):\n",
    "      min_coeffs.append([])\n",
    "      for j in range(len(new_prod_coeffs[i])):\n",
    "        min_coeffs[i].append(new_prod_coeffs[i][j]/2)\n",
    "  if max_coeffs == 1:\n",
    "    max_coeffs = []\n",
    "    for i in range(len(new_prod_coeffs)):\n",
    "      max_coeffs.append([])\n",
    "      for j in range(len(new_prod_coeffs[i])):\n",
    "        max_coeffs[i].append(new_prod_coeffs[i][j]*1.5)\n",
    "  if min_coeffs == 0:\n",
    "    min_coeffs = []\n",
    "    for i in range(len(new_prod_coeffs)):\n",
    "      min_coeffs.append([])\n",
    "      for j in range(len(new_prod_coeffs[i])):\n",
    "        min_coeffs[i].append(-10000000000000000000000000000)\n",
    "  if max_coeffs == 0:\n",
    "    max_coeffs = []\n",
    "    for i in range(len(new_prod_coeffs)):\n",
    "      max_coeffs.append([])\n",
    "      for j in range(len(new_prod_coeffs[i])):\n",
    "        max_coeffs[i].append(10000000000000000000000000000)\n",
    "  history = []\n",
    "\n",
    "  values_to_change = topo_sort_graph  # value 1260 was found to have the most effect, and value 1102 was found to have the most effect out of any value that shared coeff yield changes with value 1260\n",
    "  initial_score, initial_score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "\n",
    "  initial_flat_score_matrix = []\n",
    "  for i in range(len(initial_score_matrix)):\n",
    "    for key in matrix_keys:\n",
    "      initial_flat_score_matrix.append(initial_score_matrix[i][key])\n",
    "\n",
    "  # Initial coefficients (you can start with any values)\n",
    "  initial_coefficients = []\n",
    "  for i in range(len(topo_map)):\n",
    "      current_coeffs = []\n",
    "      for n in range(len(topo_map[i])):\n",
    "        j = map[values_to_change[i][n]][0]\n",
    "        k = map[values_to_change[i][n]][1]\n",
    "        current_coeffs.append(new_prod_coeffs[j][k])\n",
    "      initial_coefficients.append(current_coeffs)\n",
    "\n",
    "  print(\"Initial Coefficients\", initial_coefficients)\n",
    "  print(\"Initial Score\", initial_score)\n",
    "\n",
    "  # Set hyperparameters\n",
    "  max_iterations = iterations\n",
    "  best_score = initial_score\n",
    "  best_coeffs = initial_coefficients\n",
    "  # iterations = 150\n",
    "  history = []\n",
    "  times = []\n",
    "  history_raw = []\n",
    "  perturbation = []\n",
    "  initial_learning_rate = learning_rate\n",
    "  learning_rate = []\n",
    "  gradient_sum_squares = []\n",
    "  individual_coeff_score = []\n",
    "\n",
    "  # set initial perturbations, learning rates, and find initial individual coefficient scores\n",
    "  for i in range(len(initial_coefficients)):\n",
    "    perturbation.append([])\n",
    "    learning_rate.append([])\n",
    "    gradient_sum_squares.append([])\n",
    "    individual_coeff_score.append([])\n",
    "    for j in range(len(initial_coefficients[i])):\n",
    "      learning_rate[i].append(initial_learning_rate)\n",
    "      # learning_rate[i].append(0.02)\n",
    "      gradient_sum_squares[i].append(0)\n",
    "      perturbation[i].append(initial_coefficients[i][j]*0.000005)\n",
    "      score = 0\n",
    "      for k in topo_map[i][j]:\n",
    "        score += abs(initial_flat_score_matrix[k])\n",
    "      individual_coeff_score[i].append(score)\n",
    "\n",
    "  # for h in range(5):\n",
    "  # for h in [0]:\n",
    "  for h in range(len(topo_sort_graph)):\n",
    "      past_scores = []\n",
    "      for x in range(max_iterations):\n",
    "        gradients = []\n",
    "        # for h in [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]:\n",
    "        values_to_change = topo_sort_graph[h]\n",
    "        gradient = np.zeros_like(initial_coefficients[h])\n",
    "\n",
    "        # create a copy of the list and then update each coefficient one at a time, checking each gradient\n",
    "        list_copy = [row[:] for row in new_prod_coeffs]\n",
    "        for i in range(len(initial_coefficients[h])):\n",
    "            new_prod_coeffs = [row[:] for row in list_copy]\n",
    "            new_prod_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]] = initial_coefficients[h][i] + perturbation[h][i]\n",
    "\n",
    "            # calculate costs with plus perturbations\n",
    "            avg_score, score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "            flat_score_matrix = []\n",
    "            for j in range(len(score_matrix)):\n",
    "              for key in matrix_keys:\n",
    "                flat_score_matrix.append(score_matrix[j][key])\n",
    "\n",
    "            matrix_values = topo_map[h][i]\n",
    "            total_score = 0\n",
    "            for j in matrix_values:\n",
    "              total_score += abs(flat_score_matrix[j])\n",
    "\n",
    "            plus_score = (total_score/len(matrix_values))\n",
    "\n",
    "\n",
    "            plus_cost = plus_score*plus_score # square costs\n",
    "\n",
    "\n",
    "            new_prod_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]] = initial_coefficients[h][i] - perturbation[h][i]\n",
    "\n",
    "            # calculate costs with minus perturbations\n",
    "            avg_score, score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "            flat_score_matrix = []\n",
    "            for j in range(len(score_matrix)):\n",
    "              for key in matrix_keys:\n",
    "                flat_score_matrix.append(score_matrix[j][key])\n",
    "\n",
    "            history.append(avg_score)\n",
    "\n",
    "            matrix_values = topo_map[h][i]\n",
    "            total_score = 0\n",
    "            for j in matrix_values:\n",
    "              total_score += abs(flat_score_matrix[j])\n",
    "\n",
    "            minus_score = (total_score/len(matrix_values))\n",
    "\n",
    "            minus_cost = minus_score*minus_score # square costs\n",
    "\n",
    "            gradients.append((plus_cost-minus_cost)/perturbation[h][i])\n",
    "\n",
    "        # Update coefficients and adapt perturbations and learning rates\n",
    "        for i in range(len(initial_coefficients[h])):\n",
    "          score = 0\n",
    "          for k in topo_map[h][i]:\n",
    "            score += abs(flat_score_matrix[k])\n",
    "\n",
    "          gradient_sum_square = gradient_sum_squares[h][i] + gradients[i]**2\n",
    "          gradient_sum_squares[h][i] = gradient_sum_square\n",
    "          new_learning_rate = learning_rate[h][i]/np.sqrt(gradient_sum_square + 1e-8)\n",
    "\n",
    "\n",
    "          if score <= individual_coeff_score[h][i]:\n",
    "            # learning_rate[h][i] = learning_rate[h][i]*1.09\n",
    "            learning_rate[h][i] = learning_rate[h][i]*1.05\n",
    "            # perturbation[h][i] = perturbation[h][i]*1.06\n",
    "          else:\n",
    "            learning_rate[h][i] = learning_rate[h][i]*0.8\n",
    "            perturbation[h][i] = perturbation[h][i]*0.8\n",
    "\n",
    "          individual_coeff_score[h][i] = score\n",
    "\n",
    "          gradient = (new_learning_rate * gradients[i])\n",
    "          initial_coefficients[h][i] -= gradient\n",
    "\n",
    "          if initial_coefficients[h][i] < 0:\n",
    "            initial_coefficients[h][i] = 0\n",
    "          if initial_coefficients[h][i]<min_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]]:\n",
    "            initial_coefficients[h][i]=min_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]]\n",
    "          if initial_coefficients[h][i]>max_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]]:\n",
    "            initial_coefficients[h][i]=max_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]]\n",
    "\n",
    "          new_prod_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]] = initial_coefficients[h][i]\n",
    "\n",
    "          for group in conservation_constraints:\n",
    "            relier = group[2]\n",
    "            if relier == [map[values_to_change[i]][0],map[values_to_change[i]][1]] and relier != 0:\n",
    "              other_sum = 0\n",
    "              for coeff in group[0]:\n",
    "                if coeff != relier:\n",
    "                  other_sum += new_prod_coeffs[coeff[0]][coeff[1]]\n",
    "              relier_val = group[1]-other_sum\n",
    "              new_prod_coeffs[relier[0]][relier[1]] = relier_val\n",
    "              initial_coefficients[h][i] = relier_val\n",
    "\n",
    "        times.append(time.time()-tick)\n",
    "        history_raw.append(avg_score)\n",
    "\n",
    "        print((h+x*0.001+1)*100/(len(topo_sort_graph)), \"percent completed: The average score has been optimized by\", ((initial_score - avg_score) / initial_score) * 100, \"percent\")\n",
    "        past_scores.append(((initial_score - avg_score) / initial_score) * 100)\n",
    "        if avg_score < best_score:\n",
    "          best_score = avg_score\n",
    "          best_coeffs = initial_coefficients\n",
    "        if x>20:\n",
    "          if past_scores[x-20] > (((initial_score - avg_score) / initial_score) * 100)-0.1:\n",
    "            initial_coefficients = best_coeffs\n",
    "            break\n",
    "\n",
    "  final_score, final_score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "  tock = time.time()\n",
    "\n",
    "  print(final_score_matrix)\n",
    "  print(\"\\nOptimized Coefficients:\", initial_coefficients)\n",
    "  print(\"Final Score:\", final_score)\n",
    "  print(\"Optimized by\", ((initial_score - final_score) / initial_score) * 100, \"percent\")\n",
    "  print(\"Times array\", times)\n",
    "  print(\"Scores array\", history_raw)\n",
    "  print(\"\\nThe gradient descent\", tock-tick, \"seconds\")\n",
    "\n",
    "  return initial_coefficients, topo_sort_graph\n",
    "\n",
    "def random_ugs(iterations, learning_rate, beta1, beta2, individual_coeffs, evaluate, matrix_keys, individual_coeff_map, copy_prod, min_coeffs, max_coeffs, conservation_constraints):\n",
    "  print()\n",
    "  print(\"Beginning random undirected graph sort gradient descent\")\n",
    "\n",
    "  # MAKE TOPOLOGICAL ORIENTATION OF GRAPH (reverse of yield_change_graph)\n",
    "  topo_graph = []\n",
    "  for j in range(len(yield_change_graph[0])):\n",
    "    current_graph = []\n",
    "    for row in yield_change_graph:\n",
    "      current_graph.append(row[j])\n",
    "    topo_graph.append(current_graph)\n",
    "\n",
    "  # SORT GRAPH\n",
    "  topo_sort_graph = []\n",
    "  topo_coeffs2 = []\n",
    "  topo_effected_yields = []\n",
    "  rows_map = []\n",
    "  topo_graph_dict = {}\n",
    "\n",
    "  for i in range(len(topo_graph)):\n",
    "    rows_map.append(i)\n",
    "    topo_graph_dict[i] = topo_graph[i]\n",
    "\n",
    "\n",
    "  rows_delete = []\n",
    "  dict_keys = list(topo_graph_dict.keys())\n",
    "  for i in dict_keys:\n",
    "    counter = 0\n",
    "    for j in range(len(topo_graph[i])):\n",
    "      counter += topo_graph[i][j]\n",
    "    if counter == 0:\n",
    "      rows_delete.append(i)\n",
    "\n",
    "  for i in rows_delete:\n",
    "    del topo_graph_dict[i]\n",
    "\n",
    "\n",
    "  # iterate sorting until there are no rows left\n",
    "  while len(topo_graph_dict) > 0:\n",
    "    counter_obj = {}\n",
    "    rows_delete = []\n",
    "    least = 1000\n",
    "    greatest = 0\n",
    "    dict_keys = list(topo_graph_dict.keys())\n",
    "\n",
    "    # check how many connections all rows have\n",
    "    for i in dict_keys:\n",
    "      counter = 0\n",
    "      for j in range(len(topo_graph_dict[i])):\n",
    "        counter += topo_graph[i][j]\n",
    "      # check if this row has the least amount of connections\n",
    "      if counter < least and counter > 0:\n",
    "        least = counter\n",
    "      # find greatest number of connections just for informational purposes\n",
    "      if counter > greatest:\n",
    "        greatest = counter\n",
    "      # update counting object with the number of connection this row has\n",
    "      if counter in counter_obj:\n",
    "        new_list = counter_obj[counter]\n",
    "        counter_obj[counter].append(i)\n",
    "      else:\n",
    "        counter_obj[counter] = [i]\n",
    "\n",
    "\n",
    "    # get all rows with the least amount of connections, and get all effecting coefficients in each of those rows\n",
    "    least_rows = counter_obj[least]\n",
    "    curr_topo_coeffs = []\n",
    "    curr_effected_yields = []\n",
    "    curr_topo_coeffs22 = []\n",
    "    for i in least_rows:\n",
    "      curr_topo_coeffs2 = []\n",
    "      for j in range(len(topo_graph_dict[i])):\n",
    "        if topo_graph_dict[i][j] == 1 and j not in curr_topo_coeffs:\n",
    "          curr_topo_coeffs.append(j)\n",
    "          curr_topo_coeffs2.append(j)\n",
    "      curr_effected_yields.append(i)\n",
    "      curr_topo_coeffs22.append(curr_topo_coeffs2)\n",
    "\n",
    "    topo_sort_graph.append(curr_topo_coeffs)\n",
    "    topo_effected_yields.append(curr_effected_yields)\n",
    "    topo_coeffs2.append(curr_topo_coeffs22)\n",
    "\n",
    "    # remove all connections with the current coefficients that we just got\n",
    "    dict_keys = list(topo_graph_dict.keys())\n",
    "    for i in dict_keys:\n",
    "      for j in curr_topo_coeffs:\n",
    "        topo_graph[i][j] = 0\n",
    "\n",
    "    for i in dict_keys:\n",
    "      counter = 0\n",
    "      for j in range(len(topo_graph[i])):\n",
    "        counter += topo_graph[i][j]\n",
    "      if counter == 0:\n",
    "        rows_delete.append(i)\n",
    "\n",
    "    for i in rows_delete:\n",
    "      del topo_graph_dict[i]\n",
    "\n",
    "  counter = 0\n",
    "  for row in topo_sort_graph:\n",
    "    for num in row:\n",
    "      counter += 1\n",
    "\n",
    "  topo_map = [row[:] for row in topo_sort_graph]\n",
    "  for i in range(len(topo_map)):\n",
    "    for j in range(len(topo_map[i])):\n",
    "      coeff = topo_map[i][j]\n",
    "      yield_changes = yield_change_graph[coeff]\n",
    "      current_changes = []\n",
    "      for k in range(len(yield_changes)):\n",
    "        if yield_changes[k] == 1:\n",
    "          current_changes.append(k)\n",
    "      topo_map[i][j] = current_changes\n",
    "\n",
    "  topo_map2 = [row[:] for row in [row2[:] for row2 in [row3[:] for row3 in topo_coeffs2]]]\n",
    "  topo_map2 = []\n",
    "  for i in range(len(topo_coeffs2)):\n",
    "    curr_topo_map = []\n",
    "    for j in range(len(topo_coeffs2[i])):\n",
    "      curr_topo_map2 = []\n",
    "      for k in range(len(topo_coeffs2[i][j])):\n",
    "        coeff = topo_coeffs2[i][j][k]\n",
    "        yield_changes = yield_change_graph[coeff]\n",
    "        current_changes = []\n",
    "        for l in range(len(yield_changes)):\n",
    "          if yield_changes[l] == 1:\n",
    "            current_changes.append(l)\n",
    "        curr_topo_map2.append(current_changes)\n",
    "      curr_topo_map.append(curr_topo_map2)\n",
    "    topo_map2.append(curr_topo_map)\n",
    "\n",
    "  coeffs_list = []\n",
    "\n",
    "  for i in range(len(topo_sort_graph)):\n",
    "    for j in range(len(topo_sort_graph[i])):\n",
    "      coeffs_list.append(topo_sort_graph[i][j])\n",
    "\n",
    "  print(len(coeffs_list), coeffs_list)\n",
    "  random.shuffle(coeffs_list)\n",
    "  print(len(coeffs_list), coeffs_list)\n",
    "\n",
    "  random_list = []\n",
    "\n",
    "  current_list = []\n",
    "  for i in range(len(coeffs_list)):\n",
    "    current_list.append(coeffs_list[i])\n",
    "    if i%4 == 0 and i != 0:\n",
    "      random_list.append(current_list)\n",
    "      current_list = []\n",
    "\n",
    "  print(random_list)\n",
    "\n",
    "  # make map\n",
    "  topo_sort_graph = random_list\n",
    "  topo_map = [row[:] for row in topo_sort_graph]\n",
    "  for i in range(len(topo_map)):\n",
    "    for j in range(len(topo_map[i])):\n",
    "      coeff = topo_map[i][j]\n",
    "      yield_changes = yield_change_graph[coeff]\n",
    "      current_changes = []\n",
    "      for k in range(len(yield_changes)):\n",
    "        if yield_changes[k] == 1:\n",
    "          current_changes.append(k)\n",
    "      topo_map[i][j] = current_changes\n",
    "\n",
    "  print(topo_map)\n",
    "\n",
    "  tick = time.time()\n",
    "  new_prod_coeffs = copy_prod(prod_coeff_list_r)\n",
    "  if min_coeffs == 1:\n",
    "    min_coeffs = []\n",
    "    for i in range(len(new_prod_coeffs)):\n",
    "      min_coeffs.append([])\n",
    "      for j in range(len(new_prod_coeffs[i])):\n",
    "        min_coeffs[i].append(new_prod_coeffs[i][j]/2)\n",
    "  if max_coeffs == 1:\n",
    "    max_coeffs = []\n",
    "    for i in range(len(new_prod_coeffs)):\n",
    "      max_coeffs.append([])\n",
    "      for j in range(len(new_prod_coeffs[i])):\n",
    "        max_coeffs[i].append(new_prod_coeffs[i][j]*1.5)\n",
    "  if min_coeffs == 0:\n",
    "    min_coeffs = []\n",
    "    for i in range(len(new_prod_coeffs)):\n",
    "      min_coeffs.append([])\n",
    "      for j in range(len(new_prod_coeffs[i])):\n",
    "        min_coeffs[i].append(-10000000000000000000000000000)\n",
    "  if max_coeffs == 0:\n",
    "    max_coeffs = []\n",
    "    for i in range(len(new_prod_coeffs)):\n",
    "      max_coeffs.append([])\n",
    "      for j in range(len(new_prod_coeffs[i])):\n",
    "        max_coeffs[i].append(10000000000000000000000000000)\n",
    "  history = []\n",
    "\n",
    "  values_to_change = topo_sort_graph  # value 1260 was found to have the most effect, and value 1102 was found to have the most effect out of any value that shared coeff yield changes with value 1260\n",
    "  initial_score, initial_score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "\n",
    "  initial_flat_score_matrix = []\n",
    "  for i in range(len(initial_score_matrix)):\n",
    "    for key in matrix_keys:\n",
    "      initial_flat_score_matrix.append(initial_score_matrix[i][key])\n",
    "\n",
    "  # Initial coefficients (you can start with any values)\n",
    "  initial_coefficients = []\n",
    "  for i in range(len(topo_map)):\n",
    "      current_coeffs = []\n",
    "      for n in range(len(topo_map[i])):\n",
    "        j = map[values_to_change[i][n]][0]\n",
    "        k = map[values_to_change[i][n]][1]\n",
    "        current_coeffs.append(new_prod_coeffs[j][k])\n",
    "      initial_coefficients.append(current_coeffs)\n",
    "\n",
    "  print(\"Initial Coefficients\", initial_coefficients)\n",
    "  print(\"Initial Score\", initial_score)\n",
    "\n",
    "  # Set hyperparameters\n",
    "  max_iterations = iterations\n",
    "  best_score = initial_score\n",
    "  best_coeffs = initial_coefficients\n",
    "  # iterations = 150\n",
    "  history = []\n",
    "  times = []\n",
    "  history_raw = []\n",
    "  perturbation = []\n",
    "  initial_learning_rate = learning_rate\n",
    "  learning_rate = []\n",
    "  gradient_sum_squares = []\n",
    "  individual_coeff_score = []\n",
    "\n",
    "  # set initial perturbations, learning rates, and find initial individual coefficient scores\n",
    "  for i in range(len(initial_coefficients)):\n",
    "    perturbation.append([])\n",
    "    learning_rate.append([])\n",
    "    gradient_sum_squares.append([])\n",
    "    individual_coeff_score.append([])\n",
    "    for j in range(len(initial_coefficients[i])):\n",
    "      learning_rate[i].append(initial_learning_rate)\n",
    "      # learning_rate[i].append(0.02)\n",
    "      gradient_sum_squares[i].append(0)\n",
    "      perturbation[i].append(initial_coefficients[i][j]*0.000005)\n",
    "      score = 0\n",
    "      for k in topo_map[i][j]:\n",
    "        score += abs(initial_flat_score_matrix[k])\n",
    "      individual_coeff_score[i].append(score)\n",
    "\n",
    "  # for h in range(5):\n",
    "  # for h in [0]:\n",
    "  for h in range(len(topo_sort_graph)):\n",
    "      past_scores = []\n",
    "      for x in range(max_iterations):\n",
    "        gradients = []\n",
    "        # for h in [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]:\n",
    "        values_to_change = topo_sort_graph[h]\n",
    "        gradient = np.zeros_like(initial_coefficients[h])\n",
    "\n",
    "        # create a copy of the list and then update each coefficient one at a time, checking each gradient\n",
    "        list_copy = [row[:] for row in new_prod_coeffs]\n",
    "        for i in range(len(initial_coefficients[h])):\n",
    "            new_prod_coeffs = [row[:] for row in list_copy]\n",
    "            new_prod_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]] = initial_coefficients[h][i] + perturbation[h][i]\n",
    "\n",
    "            # calculate costs with plus perturbations\n",
    "            avg_score, score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "            flat_score_matrix = []\n",
    "            for j in range(len(score_matrix)):\n",
    "              for key in matrix_keys:\n",
    "                flat_score_matrix.append(score_matrix[j][key])\n",
    "\n",
    "            matrix_values = topo_map[h][i]\n",
    "            total_score = 0\n",
    "            for j in matrix_values:\n",
    "              total_score += abs(flat_score_matrix[j])\n",
    "\n",
    "            plus_score = (total_score/len(matrix_values))\n",
    "\n",
    "\n",
    "            plus_cost = plus_score*plus_score # square costs\n",
    "\n",
    "\n",
    "            new_prod_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]] = initial_coefficients[h][i] - perturbation[h][i]\n",
    "\n",
    "            # calculate costs with minus perturbations\n",
    "            avg_score, score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "            flat_score_matrix = []\n",
    "            for j in range(len(score_matrix)):\n",
    "              for key in matrix_keys:\n",
    "                flat_score_matrix.append(score_matrix[j][key])\n",
    "\n",
    "            history.append(avg_score)\n",
    "\n",
    "            matrix_values = topo_map[h][i]\n",
    "            total_score = 0\n",
    "            for j in matrix_values:\n",
    "              total_score += abs(flat_score_matrix[j])\n",
    "\n",
    "            minus_score = (total_score/len(matrix_values))\n",
    "\n",
    "            minus_cost = minus_score*minus_score # square costs\n",
    "\n",
    "            gradients.append((plus_cost-minus_cost)/perturbation[h][i])\n",
    "\n",
    "        # Update coefficients and adapt perturbations and learning rates\n",
    "        for i in range(len(initial_coefficients[h])):\n",
    "          score = 0\n",
    "          for k in topo_map[h][i]:\n",
    "            score += abs(flat_score_matrix[k])\n",
    "\n",
    "          gradient_sum_square = gradient_sum_squares[h][i] + gradients[i]**2\n",
    "          gradient_sum_squares[h][i] = gradient_sum_square\n",
    "          new_learning_rate = learning_rate[h][i]/np.sqrt(gradient_sum_square + 1e-8)\n",
    "\n",
    "\n",
    "          if score <= individual_coeff_score[h][i]:\n",
    "            # learning_rate[h][i] = learning_rate[h][i]*1.09\n",
    "            learning_rate[h][i] = learning_rate[h][i]*1.05\n",
    "            # perturbation[h][i] = perturbation[h][i]*1.06\n",
    "          else:\n",
    "            learning_rate[h][i] = learning_rate[h][i]*0.8\n",
    "            perturbation[h][i] = perturbation[h][i]*0.8\n",
    "\n",
    "          individual_coeff_score[h][i] = score\n",
    "\n",
    "          gradient = (new_learning_rate * gradients[i])\n",
    "          initial_coefficients[h][i] -= gradient\n",
    "\n",
    "          if initial_coefficients[h][i] < 0:\n",
    "            initial_coefficients[h][i] = 0\n",
    "          if initial_coefficients[h][i]<min_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]]:\n",
    "            initial_coefficients[h][i]=min_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]]\n",
    "          if initial_coefficients[h][i]>max_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]]:\n",
    "            initial_coefficients[h][i]=max_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]]\n",
    "\n",
    "          new_prod_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]] = initial_coefficients[h][i]\n",
    "\n",
    "          for group in conservation_constraints:\n",
    "            relier = group[2]\n",
    "            if relier == [map[values_to_change[i]][0],map[values_to_change[i]][1]] and relier != 0:\n",
    "              other_sum = 0\n",
    "              for coeff in group[0]:\n",
    "                if coeff != relier:\n",
    "                  other_sum += new_prod_coeffs[coeff[0]][coeff[1]]\n",
    "              relier_val = group[1]-other_sum\n",
    "              new_prod_coeffs[relier[0]][relier[1]] = relier_val\n",
    "              initial_coefficients[h][i] = relier_val\n",
    "\n",
    "        times.append(time.time()-tick)\n",
    "        history_raw.append(avg_score)\n",
    "\n",
    "        print((h+x*0.001+1)*100/(len(topo_sort_graph)), \"percent completed: The average score has been optimized by\", ((initial_score - avg_score) / initial_score) * 100, \"percent\")\n",
    "        past_scores.append(((initial_score - avg_score) / initial_score) * 100)\n",
    "        if avg_score < best_score:\n",
    "          best_score = avg_score\n",
    "          best_coeffs = initial_coefficients\n",
    "        if x>20:\n",
    "          if past_scores[x-20] > (((initial_score - avg_score) / initial_score) * 100)-0.1:\n",
    "            initial_coefficients = best_coeffs\n",
    "            break\n",
    "\n",
    "  final_score, final_score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "  tock = time.time()\n",
    "\n",
    "  print(final_score_matrix)\n",
    "  print(\"\\nOptimized Coefficients:\", initial_coefficients)\n",
    "  print(\"Final Score:\", final_score)\n",
    "  print(\"Optimized by\", ((initial_score - final_score) / initial_score) * 100, \"percent\")\n",
    "  print(\"Times array\", times)\n",
    "  print(\"Scores array\", history_raw)\n",
    "  print(\"\\nThe gradient descent\", tock-tick, \"seconds\")\n",
    "\n",
    "  return initial_coefficients, topo_sort_graph\n",
    "\n",
    "\n",
    "def standard_gd(iterations, learning_rate, beta1, beta2, individual_coeffs, evaluate, matrix_keys, individual_coeff_map, copy_prod, min_coeffs, max_coeffs, conservation_constraints):\n",
    "  print()\n",
    "  print(\"Beginning standard gradient descent\")\n",
    "  \"\"\"### 7. Standard GD (Avg Score)\"\"\"\n",
    "\n",
    "  def adam(h, coeffs, grads, learning_rate, epsilon=1e-8,):\n",
    "\n",
    "      curr_t = t[h]\n",
    "      curr_m = m[h]\n",
    "      curr_v = v[h]\n",
    "\n",
    "      curr_t += 1\n",
    "      new_coeffs = []\n",
    "\n",
    "\n",
    "      for i, (coeff, g) in enumerate(zip(coeffs, grads)):\n",
    "          curr_m[i] = beta1 * curr_m[i] + (1 - beta1) * g\n",
    "          curr_v[i] = beta2 * curr_v[i] + (1 - beta2) * (g ** 2)\n",
    "\n",
    "          m_hat = curr_m[i] / (1 - beta1 ** curr_t)\n",
    "          v_hat = curr_v[i] / (1 - beta2 ** curr_t)\n",
    "\n",
    "          coeff = coeff - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "          new_coeffs.append(coeff)\n",
    "\n",
    "      t[h] = curr_t\n",
    "      m[h] = curr_m\n",
    "      v[h] = curr_v\n",
    "\n",
    "      return new_coeffs\n",
    "\n",
    "  import numpy as np\n",
    "\n",
    "  t = []\n",
    "  m = []\n",
    "  v = []\n",
    "\n",
    "  tick = time.time()\n",
    "  new_prod_coeffs = copy_prod(prod_coeff_list_r)\n",
    "  if min_coeffs == 1:\n",
    "    min_coeffs = []\n",
    "    for i in range(len(new_prod_coeffs)):\n",
    "      min_coeffs.append([])\n",
    "      for j in range(len(new_prod_coeffs[i])):\n",
    "        min_coeffs[i].append(new_prod_coeffs[i][j]/2)\n",
    "  if max_coeffs == 1:\n",
    "    max_coeffs = []\n",
    "    for i in range(len(new_prod_coeffs)):\n",
    "      max_coeffs.append([])\n",
    "      for j in range(len(new_prod_coeffs[i])):\n",
    "        max_coeffs[i].append(new_prod_coeffs[i][j]*1.5)\n",
    "  if min_coeffs == 0:\n",
    "    min_coeffs = []\n",
    "    for i in range(len(new_prod_coeffs)):\n",
    "      min_coeffs.append([])\n",
    "      for j in range(len(new_prod_coeffs[i])):\n",
    "        min_coeffs[i].append(-10000000000000000000000000000)\n",
    "  if max_coeffs == 0:\n",
    "    max_coeffs = []\n",
    "    for i in range(len(new_prod_coeffs)):\n",
    "      max_coeffs.append([])\n",
    "      for j in range(len(new_prod_coeffs[i])):\n",
    "        max_coeffs[i].append(10000000000000000000000000000)\n",
    "  history = []\n",
    "\n",
    "  values_to_change = individual_coeffs  # value 1260 was found to have the most effect, and value 1102 was found to have the most effect out of any value that shared coeff yield changes with value 1260\n",
    "  initial_score, initial_score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "\n",
    "  initial_flat_score_matrix = []\n",
    "  for i in range(len(initial_score_matrix)):\n",
    "    for key in matrix_keys:\n",
    "      initial_flat_score_matrix.append(initial_score_matrix[i][key])\n",
    "\n",
    "  # Initial coefficients (you can start with any values)\n",
    "  initial_coefficients = []\n",
    "  for i in range(len(individual_coeff_map)):\n",
    "      current_coeffs = []\n",
    "      for n in range(len(individual_coeff_map[i])):\n",
    "        j = map[values_to_change[i][n]][0]\n",
    "        k = map[values_to_change[i][n]][1]\n",
    "        current_coeffs.append(new_prod_coeffs[j][k])\n",
    "      initial_coefficients.append(current_coeffs)\n",
    "\n",
    "  print(\"Initial Coefficients\", initial_coefficients)\n",
    "  print(\"Initial Score\", initial_score)\n",
    "\n",
    "  # Set hyperparameters\n",
    "  # iterations = 1000\n",
    "  # iterations = 2000\n",
    "  times = []\n",
    "  history = []\n",
    "  gradient_sum_squares = []\n",
    "  individual_coeff_score = []\n",
    "  # learning_rate = 0.015\n",
    "  best_score = initial_score\n",
    "  best_coeffs = []\n",
    "  for i in range(len(initial_coefficients)):\n",
    "    coeffs_list = []\n",
    "    for j in range(len(initial_coefficients[i])):\n",
    "      coeffs_list.append(initial_coefficients[i][j])\n",
    "    best_coeffs.append(coeffs_list)\n",
    "  learning_rates = []\n",
    "  perturbation = []\n",
    "\n",
    "  # set initial perturbations, learning rates, and find initial individual coefficient scores\n",
    "  for i in range(len(initial_coefficients)):\n",
    "    gradient_sum_squares.append([])\n",
    "    perturbation.append([])\n",
    "    learning_rates.append([])\n",
    "    individual_coeff_score.append([])\n",
    "    t.append(0)\n",
    "    m.append([])\n",
    "    v.append([])\n",
    "    for j in range(len(initial_coefficients[i])):\n",
    "      m[i].append(0)\n",
    "      v[i].append(0)\n",
    "      learning_rates[i].append(learning_rate)\n",
    "      # perturbation[i].append(initial_coefficients[i][j]*0.00000005)\n",
    "      perturbation[i].append(initial_coefficients[i][j]*0.000005)\n",
    "      gradient_sum_squares[i].append(0)\n",
    "      score = 0\n",
    "      for k in individual_coeff_map[i][j]:\n",
    "        score += abs(initial_flat_score_matrix[k])\n",
    "      individual_coeff_score[i].append(score)\n",
    "\n",
    "  for x in range(iterations):\n",
    "      for h in range(len(individual_coeffs)):\n",
    "        values_to_change = individual_coeffs[h]\n",
    "        gradients = []\n",
    "\n",
    "        # Update prod coeffs list\n",
    "        for i in range(len(initial_coefficients[h])):\n",
    "            new_prod_coeffs = copy_prod(prod_coeff_list_r)\n",
    "            for j in range(len(initial_coefficients)):\n",
    "              for k in range(len(initial_coefficients[j])):\n",
    "                new_prod_coeffs[map[individual_coeffs[j][k]][0]][map[individual_coeffs[j][k]][1]] = initial_coefficients[j][k]\n",
    "\n",
    "            new_prod_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]] = initial_coefficients[h][i] + perturbation[h][i]\n",
    "\n",
    "            # calculate costs with plus perturbations\n",
    "            avg_score, score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "            plus_cost = avg_score*avg_score\n",
    "\n",
    "            new_prod_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]] = initial_coefficients[h][i] - perturbation[h][i]\n",
    "\n",
    "            # calculate costs with minus perturbations\n",
    "            avg_score, score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "            minus_cost = avg_score*avg_score\n",
    "\n",
    "            gradient = (plus_cost-minus_cost)/perturbation[h][i]\n",
    "            gradients.append(gradient)\n",
    "\n",
    "        new_coeffs = adam(h, initial_coefficients[h], gradients, learning_rate=learning_rate)\n",
    "\n",
    "        # Update coefficients and adapt perturbations and learning rates\n",
    "        for i in range(len(initial_coefficients[h])):\n",
    "          initial_coefficients[h][i] = new_coeffs[i]\n",
    "\n",
    "          if initial_coefficients[h][i] < 0:\n",
    "            initial_coefficients[h][i] = 0\n",
    "          if initial_coefficients[h][i]<min_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]]:\n",
    "            initial_coefficients[h][i]=min_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]]\n",
    "          if initial_coefficients[h][i]>max_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]]:\n",
    "            initial_coefficients[h][i]=max_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]]\n",
    "\n",
    "          new_prod_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]] = initial_coefficients[h][i]\n",
    "\n",
    "          for group in conservation_constraints:\n",
    "            relier = group[2]\n",
    "            if relier == [map[values_to_change[i]][0],map[values_to_change[i]][1]] and relier != 0:\n",
    "              other_sum = 0\n",
    "              for coeff in group[0]:\n",
    "                if coeff != relier:\n",
    "                  other_sum += new_prod_coeffs[coeff[0]][coeff[1]]\n",
    "              relier_val = group[1]-other_sum\n",
    "              new_prod_coeffs[relier[0]][relier[1]] = relier_val\n",
    "              initial_coefficients[h][i] = relier_val\n",
    "\n",
    "      history.append(avg_score)\n",
    "      times.append(time.time()-tick)\n",
    "\n",
    "      if avg_score < best_score:\n",
    "        best_score = avg_score\n",
    "        best_coeffs = []\n",
    "        for i in range(len(initial_coefficients)):\n",
    "          coeffs_list = []\n",
    "          for j in range(len(initial_coefficients[i])):\n",
    "            coeffs_list.append(initial_coefficients[i][j])\n",
    "          best_coeffs.append(coeffs_list)\n",
    "\n",
    "      if x>20:\n",
    "        if (((initial_score - history[x-20]) / initial_score) * 100) > (((initial_score - avg_score) / initial_score) * 100)-0.05:\n",
    "          all_bad = True\n",
    "          for i in range(1, 10):\n",
    "            if (((initial_score - history[x-i]) / initial_score) * 100) < (((initial_score - avg_score) / initial_score) * 100)-0.05:\n",
    "              all_bad = False\n",
    "          if all_bad:\n",
    "            break\n",
    "\n",
    "      print((x+1)*100/iterations, \"percent completed: The average score has been optimized by\", ((initial_score - avg_score) / initial_score) * 100, \"percent\")\n",
    "\n",
    "  for i in range(len(best_coeffs)):\n",
    "    for j in range(len(best_coeffs[i])):\n",
    "      new_prod_coeffs[map[individual_coeffs[i][j]][0]][map[individual_coeffs[i][j]][1]] = best_coeffs[i][j]\n",
    "\n",
    "\n",
    "  final_score, final_score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "  tock = time.time()\n",
    "\n",
    "  print(final_score_matrix)\n",
    "  print(\"\\nOptimized Coefficients:\", best_coeffs)\n",
    "  print(\"Final Score:\", final_score)\n",
    "  print(\"Optimized by\", ((initial_score - final_score) / initial_score) * 100, \"percent\")\n",
    "  print(\"Times array\", times)\n",
    "  print(\"Scores array\", history)\n",
    "  print(\"\\nThe gradient descent\", tock-tick, \"seconds\")\n",
    "\n",
    "  return best_coeffs\n",
    "\n",
    "def pso(iterations, learning_rate, beta1, beta2, individual_coeffs, evaluate, matrix_keys, individual_coeff_map, copy_prod, min_coeffs, max_coeffs, conservation_constraints):\n",
    "  # BOUND SET\n",
    "\n",
    "  new_prod_coeffs = copy_prod(prod_coeff_list_r)\n",
    "  avg_score, score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "  initial_score = avg_score\n",
    "\n",
    "  dimensions = 0\n",
    "  for h in range(len(individual_coeffs)):\n",
    "      for i in range(len(individual_coeffs[h])):\n",
    "        dimensions += 1\n",
    "\n",
    "  def pso_func(coeffs_input):\n",
    "    global initial_score\n",
    "    avg_score, score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "    iter_best = avg_score\n",
    "    avg_scores = []\n",
    "    for j in range(len(coeffs_input)):\n",
    "      counter = 0\n",
    "      for h in range(len(individual_coeffs)):\n",
    "        values_to_change = individual_coeffs[h]\n",
    "        for i in range(len(individual_coeffs[h])):\n",
    "            new_prod_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]] = coeffs_input[j][counter]\n",
    "            counter += 1\n",
    "\n",
    "      avg_score, score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "      avg_scores.append(avg_score)\n",
    "      # avg_scores.append(abs(avg_score))\n",
    "\n",
    "    avg_scores_np = np.array(avg_scores)\n",
    "    iter_best = np.min(avg_scores_np)\n",
    "    #print(\"\\nBest score of the iteration\", iter_best, \", \", (initial_score-iter_best)*100/initial_score, \"percent better\")\n",
    "    print(\"score\", iter_best)\n",
    "    output = np.sum(avg_scores_np)\n",
    "\n",
    "    print(avg_scores_np)\n",
    "\n",
    "    # return avg_scores_np.sum(axis=1)\n",
    "    return avg_scores_np\n",
    "\n",
    "  # Set-up hyperparameters\n",
    "  options = {'c1': 0.5, 'c2': 0.3, 'w':0.9}\n",
    "  particles = 100\n",
    "\n",
    "  lower_bounds = np.full(dimensions, 0)\n",
    "  upper_bounds = np.full(dimensions, 1.5)\n",
    "  bounds = (lower_bounds, upper_bounds)\n",
    "\n",
    "  init_pos = np.random.uniform(low=0, high=1, size=(particles, dimensions))\n",
    "  init_pos = np.clip(init_pos, lower_bounds, upper_bounds) # make sure all coeffs are within the bounds\n",
    "\n",
    "  # Call instance of PSO\n",
    "  optimizer = ps.single.GlobalBestPSO(n_particles=particles, dimensions=dimensions, options=options, bounds=bounds, init_pos=init_pos)\n",
    "  # optimizer = ps.single.GlobalBestPSO(n_particles=particles, dimensions=137, options=options, bounds=bounds, init_pos=init_pos)\n",
    "\n",
    "  # Perform optimization\n",
    "  cost, pos = optimizer.optimize(pso_func, iters=iterations)\n",
    "\n",
    "  new_prod_coeffs = copy_prod(prod_coeff_list_r)\n",
    "  for j in range(len(list(pos))):\n",
    "    counter = 0\n",
    "    for h in range(len(individual_coeffs)):\n",
    "      values_to_change = individual_coeffs[h]\n",
    "      for i in range(len(individual_coeffs[h])):\n",
    "          new_prod_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]] = list(pos)[j]\n",
    "          counter += 1\n",
    "\n",
    "  return new_prod_coeffs\n",
    "\n",
    "def f0am_file(prod_list_n_r, mech_name, copy_prod, optimized_coeffs, individual_coeffs, map, method, topo_graph):\n",
    "  \"\"\"### Create f0am File\n",
    "\n",
    "  Make prod coeff list with our coeffs\n",
    "  \"\"\"\n",
    "\n",
    "  # set our coefficients\n",
    "  # initial_coefficients = [[6.103286364346059, 7.029856289430873, 0, 0, 0, 4.84191943494451, 0.9243617644534453, 0, 6.215205162275304, 0, 3.2940156755667998], [4.9941901033123886, 1.6358405251378647, 0, 7.981480206991661, 0, 0, 0, 9.93422210054361, 0, 0, 0], [4.8678811645250555, 0.2794876746399177, 0, 0, 0.8945465832793635, 1.1496558833327433, 0, 2.377607297559912], [5.189446674226551, 0, 2.4016590162231948, 0.07360323417099703, 0.146161588770096], [1.0228326131146226, 2.3299096334191596, 0.016119283889303553], [0.6880988979545873, 2.225240560477585], [0, 0, 1.7631716793215342, 6.764312004269307, 0, 8.315105397537797, 0, 5.190408618767493], [0, 0, 0.025905877777998697, 0, 1.6489566374977533, 0, 8.628484580774584], [0, 0, 0, 0, 0.07356279822465539], [1.1893794138233957, 1.9214338911235849, 0, 0.008258281923444442, 0], [0, 0, 0], [0, 0], [0, 0], [5.329225998376063], [0], [0.04633996004069245, 0.26347402292873445, 0], [0.18440494523311868, 0, 0], [5.62931716990747], [0], [0.06757405700073058, 0, 0, 0], [0, 0.30179232276842777, 0], [0, 0.7556905385369188], [0, 1.0011438732608509], [0, 1.105630349804568], [0, 0.726947568829931, 0, 0], [0, 0, 0], [1.4109207327966249, 0], [1.873808830919395, 0.008044541177947397, 0, 0], [0.9343317109580763, 0, 0], [1.929896321066118, 0.07762237115414321, 0, 0], [0.12455366376232976, 0, 0.5969599258574305], [0.5701543103677081], [0.1963617159225069], [0, 0.23588822931317957, 0], [0, 2.0905066917126236]]\n",
    "  if method == \"undirectedgraph\" or method == \"random\":\n",
    "    new_prod_coeffs = copy_prod(prod_coeff_list_r)\n",
    "    for h in range(len(topo_graph)):\n",
    "        values_to_change = topo_graph[h]\n",
    "        for i in range(len(optimized_coeffs[h])):\n",
    "            new_prod_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]] = optimized_coeffs[h][i]\n",
    "  elif method == \"pso\":\n",
    "    new_prod_coeffs = optimized_coeffs\n",
    "  else:\n",
    "    new_prod_coeffs = copy_prod(prod_coeff_list_r)\n",
    "    for h in range(len(individual_coeffs)):\n",
    "        values_to_change = individual_coeffs[h]\n",
    "        gradient = np.zeros_like(optimized_coeffs[h])\n",
    "        for i in range(len(optimized_coeffs[h])):\n",
    "            new_prod_coeffs[map[values_to_change[i]][0]][map[values_to_change[i]][1]] = optimized_coeffs[h][i]\n",
    "\n",
    "\n",
    "  # reactions is a list and each reaction is the\n",
    "  class Mechanism:\n",
    "      def __init__(self, species, reactions):\n",
    "          self.species = species\n",
    "          self.reactions = reactions\n",
    "\n",
    "  class Reaction:\n",
    "      def __init__(self, reactants, prod_dict, rate_law, eval_rate_law, rate, rate_string = '', multiplier = 1):\n",
    "          self.reactants = reactants\n",
    "          self.prod_dict = prod_dict\n",
    "          self.rate_law = rate_law\n",
    "          self.eval_rate_law = eval_rate_law\n",
    "          self.rate = rate\n",
    "          self.rate_string = rate_string\n",
    "          self.multiplier = multiplier\n",
    "\n",
    "  reactions = []\n",
    "  for i in range(len(prod_list_n_r)):\n",
    "    prod_dict = {prod_list_n_r[i][j]: new_prod_coeffs[i][j] for j in range(len(prod_list_n_r[i]))}\n",
    "    rxn = Reaction(reac_list_n_r[i], prod_dict, rates_2[i], 1, 1, 1, 1)\n",
    "    reactions.append(rxn)\n",
    "\n",
    "  optimized_mech = Mechanism(species_list_names_r, reactions)\n",
    "\n",
    "  #create_f0am_file(network,reaction_list,species_list,name)\n",
    "  def create_f0am_file_no_rate_change(mech,name):\n",
    "      spec_2_add = \"SpeciesToAdd = {'ISOPN'; \"\n",
    "      count = 0\n",
    "      for i in mech.species:\n",
    "          count+=1\n",
    "          spec_2_add = spec_2_add + \"'\" + i +\"'\"+ ';'\n",
    "      spec_2_add = spec_2_add[:-1]\n",
    "      spec_2_add = spec_2_add  + \"};\"\n",
    "\n",
    "      eq_str = ''\n",
    "      for i in range(len(mech.reactions)):\n",
    "          r_string = ''\n",
    "          for j in mech.reactions[i].reactants:\n",
    "              r_string = r_string + str(j) + ' + '\n",
    "          r_string = r_string[:-2] + '= '\n",
    "          for j in mech.reactions[i].prod_dict:\n",
    "              r_string = r_string + str(j) + ' + '\n",
    "          r_string = r_string[:-3]\n",
    "          reac_str = ''\n",
    "          reac_str = reac_str + \"\\ni=i+1;\\nRnames{i} = '\" + r_string + \"';\\nk(:,i) = \"+ str(mech.reactions[i].rate_law)+ '*'+str(mech.reactions[i].multiplier) + ';\\n'\n",
    "          counter = 1\n",
    "          for j in mech.reactions[i].reactants:\n",
    "              reac_str = reac_str + 'Gstr{i,'+str(counter)+\"} = '\"+str(j)+\"'; \"\n",
    "              counter = counter + 1\n",
    "          reac_str = reac_str +'\\n'\n",
    "          for k in mech.reactions[i].reactants:\n",
    "              reac_str = reac_str + 'f'+ str(k) +'(i)'+'='+'f'+ str(k) +'(i)'+'-1' + '; '\n",
    "          for k in mech.reactions[i].prod_dict:\n",
    "              reac_str = reac_str + 'f'+ str(k)+'(i)'+'='+'f'+ str(k)+'(i)'+'+'+str(mech.reactions[i].prod_dict[k]) + '; '\n",
    "\n",
    "          reac_str = reac_str +'\\n'\n",
    "          reac_str = reac_str +'\\n'\n",
    "          eq_str = eq_str + reac_str\n",
    "\n",
    "      full = spec_2_add + '\\n'+'RO2ToAdd = {};'+'\\n'+'AddSpecies'+'\\n'+ eq_str\n",
    "      f0am_file = open(\"f0am_\"+name+\".m\",\"w+\")\n",
    "      f0am_file.write(full)\n",
    "\n",
    "  create_f0am_file_no_rate_change(optimized_mech, mech_name)\n",
    "\n",
    "def AMORE_Optimization(iterations=100, learning_rate=0.015, input_conditions=1, method=\"anticommunity\", individual_params=1, lower_limit=0, upper_limit=1000, beta1=0.9, beta2=0.9, mech_name=\"f0am_optimized_mechanism\"):\n",
    "  # setup.config = {'mech_file': mech_file, 'full_eqn': full_eqn, 'full_spc': full_spc, 'input_conditions': input_conditions, 'individual_params': 1}\n",
    "  # initialize({'mech_file': mech_file, 'full_eqn': full_eqn, 'full_spc': full_spc, 'input_conditions': input_conditions, 'individual_params': 1})\n",
    "  # lower_limit=0, upper_limit=1000, alpha=0.9, beta=0.9, learning_rate, 'method': method\n",
    "  new_prod_coeffs = copy_prod(inputs['test']['rxn']['prod_coeff'])\n",
    "  new_prod_coeffs_2 = copy_prod(inputs['test']['rxn']['prod_coeff'])\n",
    "  map = make_map(inputs['test']['rxn']['prod_coeff'])\n",
    "  avg_score, score_matrix = evaluate(new_prod_coeffs, inputs)\n",
    "  optimized_coeffs = [[]]\n",
    "  valid_method = True\n",
    "  matrix_keys = []\n",
    "  for k in inputs['compare_species']:\n",
    "    matrix_keys.append(k)\n",
    "  yield_change_list_3d, avg_yield_change_list = find_yield_change(ignore_coeffs, matrix_keys, inputs, new_prod_coeffs)\n",
    "  yield_change_graph, matrix_keys = make_graph(yield_change_list_3d, matrix_keys)\n",
    "  individual_coeffs = make_individual_groups(yield_change_graph)\n",
    "  individual_coeff_map = make_individual_map(individual_coeffs, yield_change_graph)\n",
    "  if method == \"anticommunity\":\n",
    "    optimized_coeffs = anti_community_gd(iterations, inputs, learning_rate, beta1, beta2, individual_coeffs, evaluate, matrix_keys, individual_coeff_map, inputs['test']['rxn']['prod_coeff'], 0, 0, conservation, map)\n",
    "  elif method == \"undirectedgraph\":\n",
    "    optimized_coeffs = undirected_graph_sort(iterations, learning_rate, beta1, beta2, individual_coeffs, evaluate, matrix_keys, individual_coeff_map, copy_prod)\n",
    "  elif method == \"standard\":\n",
    "    optimized_coeffs = standard_gd(iterations, learning_rate, beta1, beta2, individual_coeffs, evaluate, matrix_keys, individual_coeff_map, copy_prod)\n",
    "  else:\n",
    "    print(\"Please choose a valid optimization method: anticommunity, undirectedgraph, or standard\")\n",
    "    valid_method = False\n",
    "\n",
    "  if valid_method:\n",
    "    f0am_file(prod_list_n_r, mech_name, copy_prod, optimized_coeffs, individual_coeffs, map, 'acc',[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W2llxp8gdioP",
   "metadata": {
    "id": "W2llxp8gdioP"
   },
   "source": [
    "#Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22083670-f47c-4e33-adf8-0e2317eb7e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1NgguBxeBQjS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NgguBxeBQjS",
    "outputId": "68d7d659-9c49-4dc8-a4c8-18fcf5564a8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sensitivity analysis\n",
      "391  coefficients\n",
      "analyzing ISOP1OHc 0.31500000000000006\n",
      "analyzing ISOP1OHt 0.31500000000000006\n",
      "analyzing ISOP4OHc 0.25899999999999995\n",
      "analyzing ISOP4OHt 0.11099999999999997\n",
      "analyzing NO2 0.8581153172379743\n",
      "analyzing MVK 0.8393459139850188\n",
      "analyzing HO2 0.8595220151043509\n",
      "analyzing HCHO 0.8406812668986197\n",
      "analyzing IHN 0.14248800323599\n",
      "analyzing CO 0.023099061829346523\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 25\u001b[0m\n\u001b[0;32m     17\u001b[0m conservation \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 0.08881285082438445\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# conservation = {1: {'constraints': [[[\"ISOP1OHc\", \"ISOP1OHt\", \"ISOP4OHc\", \"ISOP4OHt\"], 1]], 'prod_ranges': []},\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#                 2: {'constraints': [[[\"NO2\", \"ISOP1OH2N\"], 1]], 'prod_ranges': [[\"MVK\", \"50%\", \"150%\"], [\"HO2\", \"50%\", \"150%\"], [\"HCHO\", \"50%\", \"150%\"], [\"CO\", \"50%\", \"150%\"], [\"MVK3CO4OH\", \"50%\", \"150%\"], [\"CH3CO3\", \"50%\", \"150%\"], [\"OH\", \"50%\", \"150%\"], [\"CO2\", \"50%\", \"150%\"]]},\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#                 3: {'constraints': [[[\"ISOP1OH2N\", \"NO2\"], 1]], 'prod_ranges': [[\"MACR\", \"50%\", \"150%\"], [\"HO2\", \"50%\", \"150%\"], [\"HCHO\", \"50%\", \"150%\"], [\"CO\", \"50%\", \"150%\"], [\"MVK3CO4OH\", \"50%\", \"150%\"], [\"GLYX\", \"50%\", \"150%\"], [\"HAC\", \"50%\", \"150%\"], [\"OH\", \"50%\", \"150%\"], [\"MGLY\", \"50%\", \"150%\"], [\"CH3CO3\", \"50%\", \"150%\"], [\"CO2\", \"50%\", \"150%\"]]},\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#                 4: {'constraints': [[[\"NO2\", \"IHN\"], 1]], 'prod_ranges': [[\"HO2\", \"50%\", \"150%\"], [\"CO\", \"50%\", \"150%\"], [\"MVK3CO4OH\", \"50%\", \"150%\"], [\"GLYX\", \"50%\", \"150%\"], [\"HAC\", \"50%\", \"150%\"], [\"OH\", \"50%\", \"150%\"], [\"MGLY\", \"50%\", \"150%\"], [\"CH3CO3\", \"50%\", \"150%\"], [\"CO2\", \"50%\", \"150%\"]]}}\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m AMORE_Optimization(iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0015\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manticommunity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# AMORE_Optimization(iterations=100, learning_rate=0.0015, min_coeffs=0, max_coeffs=0, conservation_constraints=conservation)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# AMORE_Optimization(iterations=400, min_coeffs=aa0, max_coeffs=0, conservation_constraints=conservation)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# AMORE_Optimization(iterations=15)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m files\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf0am_optimized_mechanism.m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[29], line 1473\u001b[0m, in \u001b[0;36mAMORE_Optimization\u001b[1;34m(iterations, learning_rate, input_conditions, method, individual_params, lower_limit, upper_limit, beta1, beta2, mech_name)\u001b[0m\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompare_species\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m   1472\u001b[0m   matrix_keys\u001b[38;5;241m.\u001b[39mappend(k)\n\u001b[1;32m-> 1473\u001b[0m yield_change_list_3d, avg_yield_change_list \u001b[38;5;241m=\u001b[39m find_yield_change(ignore_coeffs, matrix_keys, inputs, new_prod_coeffs)\n\u001b[0;32m   1474\u001b[0m yield_change_graph, matrix_keys \u001b[38;5;241m=\u001b[39m make_graph(yield_change_list_3d, matrix_keys)\n\u001b[0;32m   1475\u001b[0m individual_coeffs \u001b[38;5;241m=\u001b[39m make_individual_groups(yield_change_graph)\n",
      "Cell \u001b[1;32mIn[10], line 1656\u001b[0m, in \u001b[0;36mfind_yield_change\u001b[1;34m(ignore_coeffs, matrix_keys, inputs, new_prod_coeffs)\u001b[0m\n\u001b[0;32m   1653\u001b[0m new_prod_coeffs[j][k] \u001b[38;5;241m=\u001b[39m new_prod_coeffs[j][k] \u001b[38;5;241m+\u001b[39m increase\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;66;03m# evaluate the new set of coefficients\u001b[39;00m\n\u001b[1;32m-> 1656\u001b[0m avg_score, score_matrix \u001b[38;5;241m=\u001b[39m evaluate(new_prod_coeffs, inputs)\n\u001b[0;32m   1658\u001b[0m \u001b[38;5;66;03m# find the amount that the score changed with the new coefficients\u001b[39;00m\n\u001b[0;32m   1659\u001b[0m yield_change \u001b[38;5;241m=\u001b[39m avg_score \u001b[38;5;241m-\u001b[39m starter_score\n",
      "Cell \u001b[1;32mIn[10], line 1320\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(coeff, inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m atm_cond[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msun\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconditions\u001b[39m\u001b[38;5;124m'\u001b[39m][i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msun\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m   1319\u001b[0m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbck\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m deepcopy(background_dict_2)\n\u001b[1;32m-> 1320\u001b[0m yields_test, new_graph, new_in \u001b[38;5;241m=\u001b[39m get_yields_from_inputs(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompare_species_r\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompare_species_r\u001b[39m\u001b[38;5;124m'\u001b[39m][j], \u001b[38;5;28mlist\u001b[39m):\n",
      "Cell \u001b[1;32mIn[10], line 748\u001b[0m, in \u001b[0;36mget_yields_from_inputs\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m    746\u001b[0m TEMP \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124matm cond\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    747\u001b[0m M \u001b[38;5;241m=\u001b[39m p_fac\n\u001b[1;32m--> 748\u001b[0m k_list \u001b[38;5;241m=\u001b[39m get_k_list(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrxn\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m],inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124matm cond\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msza\u001b[39m\u001b[38;5;124m'\u001b[39m], TEMP)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(reac_list)):\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(reac_list[i])\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[1;32mIn[10], line 1169\u001b[0m, in \u001b[0;36mget_k_list\u001b[1;34m(rate_list, sza, temp)\u001b[0m\n\u001b[0;32m   1167\u001b[0m         k_list\u001b[38;5;241m.\u001b[39mappend(j_func(sza,I,m,n)\u001b[38;5;241m*\u001b[39mSUN)\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i,\u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1169\u001b[0m     k_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28meval\u001b[39m(i))\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1171\u001b[0m     k_list\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from optimizer import *\n",
    "#from google.colab import files\n",
    "# !pip install pyswarms\n",
    "\n",
    "# import numpy as np\n",
    "# import pyswarms as ps\n",
    "# from pyswarms.utils.functions import single_obj as fx\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# %%time\n",
    "\n",
    "# {'r1': {'constraints': [[[a,b,c],1]], 'prod_ranges': [[a,0,1],[b,0.5,0.8],[c,0.2,0.8]]}}\n",
    "# prod ranges are raw value of each species\n",
    "\n",
    "conservation = {}\n",
    "\n",
    "# 0.08881285082438445\n",
    "\n",
    "# conservation = {1: {'constraints': [[[\"ISOP1OHc\", \"ISOP1OHt\", \"ISOP4OHc\", \"ISOP4OHt\"], 1]], 'prod_ranges': []},\n",
    "#                 2: {'constraints': [[[\"NO2\", \"ISOP1OH2N\"], 1]], 'prod_ranges': [[\"MVK\", \"50%\", \"150%\"], [\"HO2\", \"50%\", \"150%\"], [\"HCHO\", \"50%\", \"150%\"], [\"CO\", \"50%\", \"150%\"], [\"MVK3CO4OH\", \"50%\", \"150%\"], [\"CH3CO3\", \"50%\", \"150%\"], [\"OH\", \"50%\", \"150%\"], [\"CO2\", \"50%\", \"150%\"]]},\n",
    "#                 3: {'constraints': [[[\"ISOP1OH2N\", \"NO2\"], 1]], 'prod_ranges': [[\"MACR\", \"50%\", \"150%\"], [\"HO2\", \"50%\", \"150%\"], [\"HCHO\", \"50%\", \"150%\"], [\"CO\", \"50%\", \"150%\"], [\"MVK3CO4OH\", \"50%\", \"150%\"], [\"GLYX\", \"50%\", \"150%\"], [\"HAC\", \"50%\", \"150%\"], [\"OH\", \"50%\", \"150%\"], [\"MGLY\", \"50%\", \"150%\"], [\"CH3CO3\", \"50%\", \"150%\"], [\"CO2\", \"50%\", \"150%\"]]},\n",
    "#                 4: {'constraints': [[[\"NO2\", \"IHN\"], 1]], 'prod_ranges': [[\"HO2\", \"50%\", \"150%\"], [\"CO\", \"50%\", \"150%\"], [\"MVK3CO4OH\", \"50%\", \"150%\"], [\"GLYX\", \"50%\", \"150%\"], [\"HAC\", \"50%\", \"150%\"], [\"OH\", \"50%\", \"150%\"], [\"MGLY\", \"50%\", \"150%\"], [\"CH3CO3\", \"50%\", \"150%\"], [\"CO2\", \"50%\", \"150%\"]]}}\n",
    "AMORE_Optimization(iterations=100, learning_rate=0.0015, method=\"anticommunity\")\n",
    "# AMORE_Optimization(iterations=100, learning_rate=0.0015, min_coeffs=0, max_coeffs=0, conservation_constraints=conservation)\n",
    "# AMORE_Optimization(iterations=400, min_coeffs=aa0, max_coeffs=0, conservation_constraints=conservation)\n",
    "# AMORE_Optimization(iterations=15)\n",
    "\n",
    "files.download(\"f0am_optimized_mechanism.m\")\n",
    "\n",
    "# All parameters\n",
    "# AMORE_Optimization(iterations=400, learning_rate=0.015, input_conditions=1, method=\"anticommunity\", individual_params=1, lower_limit=0, upper_limit=1000, beta1=0.9, beta2=0.9, mech_name=\"f0am_optimized_mechanism\", min_coeffs=1, max_coeffs=1, conservation_constraints=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eYoB0vWh1H-3",
   "metadata": {
    "id": "eYoB0vWh1H-3"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"f0am_optimized_mechanism.m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21638b02-c7bd-410b-a9ea-4dabd8a42f69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "alXk-v7nU5dg",
    "M-ZcyMyguau_"
   ],
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1AEWwIcPVjjAgoza8g0WFf84EwIwbR5fV",
     "timestamp": 1756926426981
    },
    {
     "file_id": "1pK5E3EBTRdfMN5U3WSNP4riNwPRx_tFX",
     "timestamp": 1746475311617
    },
    {
     "file_id": "1zfkjY_zIlUEaDTHk4DV181MzV8mVmr7F",
     "timestamp": 1724705685629
    },
    {
     "file_id": "1wb6rB0UTzV6Z8-EW2z2CKsunE7wo5lb_",
     "timestamp": 1719960775599
    },
    {
     "file_id": "13Vj92KsfnaH44eB6nyHZ1joWsbX4je95",
     "timestamp": 1719645131399
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
